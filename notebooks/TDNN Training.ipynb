{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDmhP_idRXT4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# CNN Multilabeling through AlexNet\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "\n",
    "El objetivo de este notebook es realizar predicciones multilabel sobre patrones geométricos mediante AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import math\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from utils import KunischMetrics\n",
    "from utils import KunischPruner\n",
    "from utils import DataExplorer\n",
    "from utils import KunischPlotter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuración de dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando device: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# 0 es 3090, 1 y 2 son 2080\n",
    "CUDA_ID = 0\n",
    "\n",
    "device = torch.device(f'cuda:{CUDA_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuración del experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Flags para los datos sintéticos\n",
    "# Cada flag está asociada a una o más funciones de data augmentation.\n",
    "# Los datos deben existir previamente \n",
    "# (se generan a partir del notebook split and augmentation)\n",
    "DS_FLAGS = []\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'gausblur': [gausblur]\n",
    "              # 'msblur': [msblur]\n",
    "              # 'mtnblur': [mtnblur]\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "            \n",
    "# Las flags crop, randaug, elastic y gausblur \n",
    "# se pueden aplicar más de una vez c/u. \n",
    "# (si no están en DS_FLAGS, serán ignoradas).\n",
    "CROP_TIMES = 1\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1\n",
    "GAUSBLUR_TIMES = 1\n",
    "\n",
    "# Usar pesos en el entrenamiento\n",
    "use_pos_weights = True\n",
    "pos_weights_factor = 1\n",
    "\n",
    "# Cantidad de etiquetas en estudio y batch_size\n",
    "NUM_LABELS = 26\n",
    "BATCH_SIZE = 124\n",
    "\n",
    "# Threshold de asignación en entrenamiento, validación y test\n",
    "TH_TRAIN = 0.75\n",
    "TH_VAL = 0.75\n",
    "TH_TEST = 0.75\n",
    "\n",
    "# Hiperparámetros del entrenamiento\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "w = 0.01 # weight decay\n",
    "\n",
    "# Early Stopping\n",
    "patience = 10\n",
    "    \n",
    "# Guardar  resultados?\n",
    "SAVE = True\n",
    "\n",
    "# Cantidad de folds\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Nombre del experimento: 26L_weighted_1\n",
      "--Pattern set encontrado en ../patterns/base/0\n",
      "--Labels set encontrado en ../labels/base/0\n",
      "\n",
      "Los resultados se guardarán en: ../outputs/alexnet/base/26L_weighted_1/0\n",
      "Los modelos se guardarán en: ../models/alexnet/base/0\n",
      "Fold  1\n",
      "Nombre del experimento: 26L_weighted_1\n",
      "--Pattern set encontrado en ../patterns/base/1\n",
      "--Labels set encontrado en ../labels/base/1\n",
      "\n",
      "Los resultados se guardarán en: ../outputs/alexnet/base/26L_weighted_1/1\n",
      "Los modelos se guardarán en: ../models/alexnet/base/1\n",
      "Fold  2\n",
      "Nombre del experimento: 26L_weighted_1\n",
      "--Pattern set encontrado en ../patterns/base/2\n",
      "--Labels set encontrado en ../labels/base/2\n",
      "\n",
      "Los resultados se guardarán en: ../outputs/alexnet/base/26L_weighted_1/2\n",
      "Los modelos se guardarán en: ../models/alexnet/base/2\n",
      "Fold  3\n",
      "Nombre del experimento: 26L_weighted_1\n",
      "--Pattern set encontrado en ../patterns/base/3\n",
      "--Labels set encontrado en ../labels/base/3\n",
      "\n",
      "Los resultados se guardarán en: ../outputs/alexnet/base/26L_weighted_1/3\n",
      "Los modelos se guardarán en: ../models/alexnet/base/3\n"
     ]
    }
   ],
   "source": [
    "# Esta celda construye la variable data_flags, que lee DS_FLAGS de \n",
    "# la celda anterior y mapea su contenido a distintas rutas de \n",
    "# patrones, etiquetas y outputs\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "         'gausblur': GAUSBLUR_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic', 'gausblur']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "# Revisión de los folds y creación de diccionario con paths\n",
    "Kfolds = {}\n",
    "\n",
    "for i in range(0, K):\n",
    "    print(\"Fold \", i)\n",
    "    patterns_dir = os.path.join(root_dir, 'patterns', data_flags, str(i))\n",
    "    labels_dir = os.path.join(root_dir, 'labels', data_flags, str(i))\n",
    "\n",
    "    if not (os.path.isdir(patterns_dir) and os.path.isdir(labels_dir)):\n",
    "        print(patterns_dir)\n",
    "        print(labels_dir)\n",
    "        raise FileNotFoundError(\"\"\"\n",
    "        No existen directorios de datos para el conjunto de flags seleccionado. \n",
    "        Verifique que el dataset exista y, de lo contrario, llame a Split and Augmentation.\n",
    "        \"\"\")\n",
    "        \n",
    "    exp_name = f\"{NUM_LABELS}L\"\n",
    "    weights_str = str(pos_weights_factor)\n",
    "    weights_str = weights_str.replace('.','_')\n",
    "    exp_name += f'_weighted_{weights_str}' if use_pos_weights else ''\n",
    "    print(f\"Nombre del experimento: {exp_name}\")\n",
    "     \n",
    "    output_dir = os.path.join(root_dir, \"outputs\", \"alexnet\", data_flags, exp_name, str(i))\n",
    "    model_dir = os.path.join(root_dir, \"models\", \"alexnet\", data_flags, str(i))\n",
    "    model_path = os.path.join(model_dir, exp_name + '.pth')\n",
    "\n",
    "    \n",
    "    Kfolds[i] = {\n",
    "        'patterns_dir': patterns_dir,\n",
    "        'labels_dir': labels_dir,\n",
    "        'output_dir': output_dir,\n",
    "        'model_path': model_path\n",
    "    }\n",
    "    \n",
    "    print(\"--Pattern set encontrado en {}\".format(patterns_dir))\n",
    "    print(\"--Labels set encontrado en {}\".format(labels_dir))\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "    if SAVE:\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        os.makedirs(model_dir, exist_ok = True)\n",
    "        print(f\"Los resultados se guardarán en: {output_dir}\")\n",
    "        print(f\"Los modelos se guardarán en: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_positive_weights(labels, n_samples, factor=1):     \n",
    "    total = n_samples #labels.values.sum()\n",
    "    weights = [0.] * len(labels)\n",
    "    for i, label in enumerate(labels):\n",
    "      weights[i] = total/(factor * labels[i])\n",
    "    return weights\n",
    "\n",
    "# images_dir=os.path.join(root_dir, 'patterns', data_flags, 'train'),\n",
    "# labels_file=os.path.join(root_dir, 'labels', data_flags, 'augmented_train_df.json'),\n",
    "class KunischDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, images_dir, labels_file, transform, top_labels):\n",
    "    self.pruner = KunischPruner(len(top_labels))\n",
    "    self.pruner.set_top_labels(top_labels)\n",
    "    labels = pd.read_json(labels_file, orient='index')\n",
    "    self.labels_frame = self.pruner.filter_df(labels)\n",
    "    self.num_labels = len(top_labels)\n",
    "    self.images_dir = images_dir\n",
    "    self.labels_file = labels_file\n",
    "    self.transform = transform\n",
    "    self.flags = data_flags\n",
    "    self.top_labels = top_labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels_frame)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_id = self.labels_frame.iloc[idx].name + '.png'\n",
    "    img_name = None\n",
    "    for chapter in os.listdir(self.images_dir):\n",
    "      if img_id in os.listdir(os.path.join(self.images_dir, chapter)):\n",
    "        img_name = os.path.join(self.images_dir, chapter, img_id)\n",
    "        break\n",
    "    if img_name is None:\n",
    "      raise Exception(f'No se encontró la imagen para {img_id}')\n",
    "    image = Image.open(img_name)\n",
    "    image = image.convert('RGB')\n",
    "    image = self.transform(image)\n",
    "    labels = self.labels_frame.iloc[idx].values\n",
    "    labels = np.array(labels)\n",
    "    labels = torch.from_numpy(labels.astype('int'))\n",
    "    #print(img_id, img_name, self.labels_frame.iloc[idx], self.labels_frame.iloc[idx].values, labels)\n",
    "    sample = {'image': image, 'labels': labels, 'paths': img_name}\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Define the function for training, validation, and test\n",
    "def alex_train(epoch, num_epochs, train_losses, learning_rate, w, real_num_labels):\n",
    "  alex_net.train()\n",
    "  train_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, real_num_labels), dtype=int)\n",
    "  labels_total = np.empty((1, real_num_labels), dtype=int)\n",
    "    \n",
    "  for i, sample_batched in enumerate(kunischTrainLoader, 1):\n",
    "      inputs = sample_batched['image'].to(device)\n",
    "      labels = sample_batched['labels'].to(device)\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = alex_net(inputs)\n",
    "      loss = criterion(outputs.float(), labels.float())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      train_loss += loss.item()\n",
    "      pred = (torch.sigmoid(outputs).data > TH_TRAIN).int()\n",
    "      # print(pred)\n",
    "      labels = labels.int()\n",
    "      # print(labels)\n",
    "      preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "      labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "    \n",
    "      TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "      TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "      FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "      FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "      #print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP, TN, FP, FN))\n",
    "  \n",
    "\n",
    "  TP = TP.cpu().numpy()\n",
    "  TN = TN.cpu().numpy()\n",
    "  FP = FP.cpu().numpy()\n",
    "  FN = FN.cpu().numpy()\n",
    "\n",
    "  accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "  precision = TP / (TP + FP)\n",
    "  recall = TP / (TP + FN)\n",
    "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "  f2_score = (1+2**2) * (precision * recall) / ((2**2*precision) + recall)\n",
    "  train_loss = train_loss / len(kunischTrainLoader.dataset) * BATCH_SIZE\n",
    "  train_losses.append([epoch, learning_rate, w, train_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score,\n",
    "                      f2_score])\n",
    "\n",
    "  # print statistics\n",
    "  print('Train Trial [{}/{}], LR: {:.4g}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, F2 score: {:.4f}'\n",
    "        .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, train_loss, accuracy, f1_score, f2_score))\n",
    "  return f2_score\n",
    "\n",
    "\n",
    "def alex_valid(epoch, num_epochs, valid_losses, learning_rate, w, real_num_labels):\n",
    "  # Have our model in evaluation mode\n",
    "  alex_net.eval()\n",
    "  # Set losses and Correct labels to zero\n",
    "  valid_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, real_num_labels), dtype=int)\n",
    "  labels_total = np.empty((1, real_num_labels), dtype=int)\n",
    "  with torch.no_grad():\n",
    "      for i, sample_batched in enumerate(kunischValidationLoader, 1):\n",
    "          inputs = sample_batched['image'].to(device)\n",
    "          labels = sample_batched['labels'].to(device)\n",
    "          outputs = alex_net(inputs)\n",
    "          loss = criterion(outputs.float(), labels.float())\n",
    "          valid_loss += loss.item()\n",
    "          pred = (torch.sigmoid(outputs).data > TH_VAL).int()\n",
    "          labels = labels.int()\n",
    "          preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "          labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "        \n",
    "          TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "          TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "          FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "          FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "          # print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )\n",
    "\n",
    "      TP = TP.cpu().numpy()\n",
    "      TN = TN.cpu().numpy()\n",
    "      FP = FP.cpu().numpy()\n",
    "      FN = FN.cpu().numpy()\n",
    "      accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "      precision = TP / (TP + FP)\n",
    "      recall = TP / (TP + FN)\n",
    "      f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "      f2_score = (1+2**2) * (precision * recall) / ((2**2*precision) + recall)\n",
    "      \n",
    "      scheduler.step(f2_score)\n",
    "\n",
    "      valid_loss = valid_loss / len(kunischValidationLoader.dataset) * BATCH_SIZE  # 1024 is the batch size\n",
    "      valid_losses.append(\n",
    "          [epoch, learning_rate, w, valid_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score,\n",
    "          f2_score])\n",
    "      # print statistics\n",
    "      print('Valid Trial [{}/{}], LR: {}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, F2 score: {:.4f}'\n",
    "            .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, valid_loss, accuracy, f1_score, f2_score))\n",
    "      return f2_score\n",
    "\n",
    "\n",
    "    \n",
    "def alex_test(epoch, num_epochs, pred_array, test_losses, learning_rate, w, real_num_labels, show_images=1):\n",
    "  # Have our model in evaluation mode\n",
    "  alex_net.eval()\n",
    "  # Set losses and Correct labels to zero\n",
    "  test_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, real_num_labels), dtype=int)\n",
    "  labels_total = np.empty((1, real_num_labels), dtype=int)\n",
    "    \n",
    "  with torch.no_grad():\n",
    "      for i,sample_batched in enumerate(kunischTestLoader, 1):\n",
    "          print(\"CURRENT BATCH SIZE: \", BATCH_SIZE)\n",
    "          inputs = sample_batched['image'].to(device)\n",
    "          labels = sample_batched['labels'].to(device)\n",
    "          paths = sample_batched['paths']\n",
    "          outputs = alex_net(inputs)\n",
    "          \n",
    "          loss = criterion(outputs.float(), labels.float())\n",
    "          test_loss += loss.item()\n",
    "          pred = (torch.sigmoid(outputs).data > TH_TEST).int()\n",
    "          # print(pred)\n",
    "          labels = labels.int()\n",
    "          # print(labels)\n",
    "          pred_array.append([paths, test_loss, labels, pred])\n",
    "          preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "          labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "          \n",
    "          for j in range(0, min(BATCH_SIZE, show_images)): # j itera sobre ejemplos\n",
    "              print(f\"Mostrando imagen {j} del batch {i}\")\n",
    "              img = np.transpose(sample_batched['image'][j]) # imagen j \n",
    "              plt.imshow(img, interpolation='nearest')\n",
    "              plt.show()\n",
    "              labels_correctos = \"\"\n",
    "              labels_predichos = \"\"\n",
    "              for k in range(0, len(pred[j])):\n",
    "                labels_correctos += (kunischTestSet.labels_frame.columns.values[k]+' ') if labels[j].cpu().detach()[k] else \"\"\n",
    "                labels_predichos += (kunischTestSet.labels_frame.columns.values[k]+' ') if pred[j].cpu().detach()[k] else \"\"\n",
    "              print(\"Labels correctos:\")\n",
    "              #print(labels[j].cpu().detach().numpy())\n",
    "              print(labels_correctos)\n",
    "              print(\"Labels predichos:\")\n",
    "              #print(pred[j].cpu().detach().numpy())\n",
    "              print(labels_predichos)\n",
    "              print(\"\\n\")\n",
    "            \n",
    "          TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "          TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "          FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "          FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "          # print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )\n",
    "\n",
    "      TP = TP.cpu().numpy()\n",
    "      TN = TN.cpu().numpy()\n",
    "      FP = FP.cpu().numpy()\n",
    "      FN = FN.cpu().numpy()\n",
    "      accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "      precision = TP / (TP + FP)\n",
    "      recall = TP / (TP + FN)\n",
    "      f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "      f2_score = (1+2**2) * (precision * recall) / ((2**2*precision) + recall)\n",
    "\n",
    "      test_loss = test_loss / len(kunischTestLoader.dataset) * 1024  # 1024 is the batch size\n",
    "      test_losses.append([epoch, learning_rate, w, test_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score, \n",
    "                          f2_score])\n",
    "      # print statistics\n",
    "      print('Test Trial [{}/{}], LR: {}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, F2 score: {:.4f}'\n",
    "            .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, test_loss, accuracy, f1_score, f2_score))\n",
    "      return f2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando top_labels previamente generados para 26 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mavergar/anaconda3/envs/Kunisch_10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mavergar/anaconda3/envs/Kunisch_10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Trial [0/200], LR: 0.001, W: 0.01, Avg Loss: 1.0789, Accuracy: 0.8606, F1 score: 0.4570, F2 score: 0.4404\n",
      "Valid Trial [0/200], LR: 0.001, W: 0.01, Avg Loss: 1.7891, Accuracy: 0.8542, F1 score: 0.5185, F2 score: 0.5594\n",
      "\n",
      "Train Trial [1/200], LR: 0.001, W: 0.01, Avg Loss: 0.8426, Accuracy: 0.8885, F1 score: 0.5986, F2 score: 0.6052\n",
      "Valid Trial [1/200], LR: 0.001, W: 0.01, Avg Loss: 1.5935, Accuracy: 0.8900, F1 score: 0.5672, F2 score: 0.5519\n",
      "\n",
      "Train Trial [2/200], LR: 0.001, W: 0.01, Avg Loss: 0.7926, Accuracy: 0.8919, F1 score: 0.6159, F2 score: 0.6274\n",
      "Valid Trial [2/200], LR: 0.001, W: 0.01, Avg Loss: 1.4155, Accuracy: 0.8782, F1 score: 0.5855, F2 score: 0.6207\n",
      "\n",
      "Train Trial [3/200], LR: 0.001, W: 0.01, Avg Loss: 0.7745, Accuracy: 0.8946, F1 score: 0.6242, F2 score: 0.6345\n",
      "Valid Trial [3/200], LR: 0.001, W: 0.01, Avg Loss: 1.5604, Accuracy: 0.8873, F1 score: 0.5981, F2 score: 0.6171\n",
      "\n",
      "Train Trial [4/200], LR: 0.001, W: 0.01, Avg Loss: 0.7365, Accuracy: 0.8977, F1 score: 0.6377, F2 score: 0.6509\n",
      "Valid Trial [4/200], LR: 0.001, W: 0.01, Avg Loss: 1.6093, Accuracy: 0.8622, F1 score: 0.5521, F2 score: 0.6009\n",
      "\n",
      "Train Trial [5/200], LR: 0.001, W: 0.01, Avg Loss: 0.7203, Accuracy: 0.8996, F1 score: 0.6464, F2 score: 0.6618\n",
      "Valid Trial [5/200], LR: 0.001, W: 0.01, Avg Loss: 1.3697, Accuracy: 0.8825, F1 score: 0.5956, F2 score: 0.6274\n",
      "\n",
      "Train Trial [6/200], LR: 0.001, W: 0.01, Avg Loss: 0.7597, Accuracy: 0.8935, F1 score: 0.6225, F2 score: 0.6351\n",
      "Valid Trial [6/200], LR: 0.001, W: 0.01, Avg Loss: 1.4113, Accuracy: 0.8862, F1 score: 0.6077, F2 score: 0.6395\n",
      "\n",
      "Train Trial [7/200], LR: 0.001, W: 0.01, Avg Loss: 0.7012, Accuracy: 0.9005, F1 score: 0.6533, F2 score: 0.6734\n",
      "Valid Trial [7/200], LR: 0.00075, W: 0.01, Avg Loss: 1.5632, Accuracy: 0.8873, F1 score: 0.6041, F2 score: 0.6289\n",
      "\n",
      "Train Trial [8/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6434, Accuracy: 0.9071, F1 score: 0.6757, F2 score: 0.6957\n",
      "Valid Trial [8/200], LR: 0.00075, W: 0.01, Avg Loss: 1.5671, Accuracy: 0.9006, F1 score: 0.6235, F2 score: 0.6205\n",
      "\n",
      "Train Trial [9/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6437, Accuracy: 0.9065, F1 score: 0.6727, F2 score: 0.6913\n",
      "Valid Trial [9/200], LR: 0.00075, W: 0.01, Avg Loss: 1.4727, Accuracy: 0.8910, F1 score: 0.6250, F2 score: 0.6584\n",
      "\n",
      "Train Trial [10/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6398, Accuracy: 0.9066, F1 score: 0.6738, F2 score: 0.6932\n",
      "Valid Trial [10/200], LR: 0.00075, W: 0.01, Avg Loss: 1.4706, Accuracy: 0.9038, F1 score: 0.6552, F2 score: 0.6738\n",
      "\n",
      "Train Trial [11/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6006, Accuracy: 0.9116, F1 score: 0.6911, F2 score: 0.7111\n",
      "Valid Trial [11/200], LR: 0.00075, W: 0.01, Avg Loss: 1.4283, Accuracy: 0.8889, F1 score: 0.6204, F2 score: 0.6564\n",
      "\n",
      "Train Trial [12/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6003, Accuracy: 0.9125, F1 score: 0.6953, F2 score: 0.7167\n",
      "Valid Trial [12/200], LR: 0.00075, W: 0.01, Avg Loss: 1.5949, Accuracy: 0.8803, F1 score: 0.5758, F2 score: 0.5961\n",
      "\n",
      "Train Trial [13/200], LR: 0.00075, W: 0.01, Avg Loss: 0.5929, Accuracy: 0.9135, F1 score: 0.6975, F2 score: 0.7173\n",
      "Valid Trial [13/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.4612, Accuracy: 0.8777, F1 score: 0.6017, F2 score: 0.6543\n",
      "\n",
      "Train Trial [14/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5421, Accuracy: 0.9198, F1 score: 0.7205, F2 score: 0.7426\n",
      "Valid Trial [14/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.5287, Accuracy: 0.8905, F1 score: 0.6306, F2 score: 0.6720\n",
      "\n",
      "Train Trial [15/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5466, Accuracy: 0.9184, F1 score: 0.7149, F2 score: 0.7357\n",
      "Valid Trial [15/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.5694, Accuracy: 0.9001, F1 score: 0.6383, F2 score: 0.6527\n",
      "\n",
      "Train Trial [16/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5286, Accuracy: 0.9217, F1 score: 0.7258, F2 score: 0.7455\n",
      "Valid Trial [16/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.8054, Accuracy: 0.8969, F1 score: 0.6267, F2 score: 0.6408\n",
      "\n",
      "Train Trial [17/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5265, Accuracy: 0.9229, F1 score: 0.7298, F2 score: 0.7493\n",
      "Valid Trial [17/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.6183, Accuracy: 0.8841, F1 score: 0.6252, F2 score: 0.6825\n",
      "\n",
      "Train Trial [18/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5023, Accuracy: 0.9247, F1 score: 0.7367, F2 score: 0.7576\n",
      "Valid Trial [18/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.7155, Accuracy: 0.8953, F1 score: 0.6343, F2 score: 0.6625\n",
      "\n",
      "Train Trial [19/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.4867, Accuracy: 0.9274, F1 score: 0.7454, F2 score: 0.7649\n",
      "Valid Trial [19/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 1.6254, Accuracy: 0.8905, F1 score: 0.6266, F2 score: 0.6636\n",
      "\n",
      "Train Trial [20/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.4598, Accuracy: 0.9313, F1 score: 0.7594, F2 score: 0.7799\n",
      "Valid Trial [20/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 2.0962, Accuracy: 0.9092, F1 score: 0.6614, F2 score: 0.6645\n",
      "\n",
      "Train Trial [21/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.4529, Accuracy: 0.9315, F1 score: 0.7605, F2 score: 0.7822\n",
      "Valid Trial [21/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 1.9054, Accuracy: 0.8990, F1 score: 0.6316, F2 score: 0.6429\n",
      "\n",
      "Train Trial [22/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.4503, Accuracy: 0.9317, F1 score: 0.7603, F2 score: 0.7806\n",
      "Valid Trial [22/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 1.8835, Accuracy: 0.9044, F1 score: 0.6413, F2 score: 0.6421\n",
      "\n",
      "Train Trial [23/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.4355, Accuracy: 0.9338, F1 score: 0.7680, F2 score: 0.7885\n",
      "Valid Trial [23/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 2.0188, Accuracy: 0.9022, F1 score: 0.6362, F2 score: 0.6400\n",
      "\n",
      "Train Trial [24/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.4516, Accuracy: 0.9330, F1 score: 0.7636, F2 score: 0.7810\n",
      "Valid Trial [24/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 1.7404, Accuracy: 0.8969, F1 score: 0.6393, F2 score: 0.6669\n",
      "\n",
      "Train Trial [25/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.4340, Accuracy: 0.9351, F1 score: 0.7717, F2 score: 0.7910\n",
      "Valid Trial [25/200], LR: 0.00031640625000000006, W: 0.01, Avg Loss: 1.8059, Accuracy: 0.8862, F1 score: 0.5958, F2 score: 0.6162\n",
      "\n",
      "Train Trial [26/200], LR: 0.0003164, W: 0.01, Avg Loss: 0.4153, Accuracy: 0.9376, F1 score: 0.7800, F2 score: 0.7983\n",
      "Valid Trial [26/200], LR: 0.00031640625000000006, W: 0.01, Avg Loss: 1.8054, Accuracy: 0.8964, F1 score: 0.6367, F2 score: 0.6635\n",
      "\n",
      "Train Trial [27/200], LR: 0.0003164, W: 0.01, Avg Loss: 0.4111, Accuracy: 0.9382, F1 score: 0.7822, F2 score: 0.8010\n",
      "Valid Trial [27/200], LR: 0.00031640625000000006, W: 0.01, Avg Loss: 1.9287, Accuracy: 0.9060, F1 score: 0.6562, F2 score: 0.6672\n",
      "\n",
      "Out of patience!\n",
      "Best epoch: 17\n",
      "Guardando mejor modelo en ../models/alexnet/base/0/26L_weighted_1.pth\n",
      "CURRENT BATCH SIZE:  124\n",
      "CURRENT BATCH SIZE:  124\n",
      "Test Trial [1/1], LR: 0.0001, W: 0.001, Avg Loss: 12.2843, Accuracy: 0.8943, F1 score: 0.6377, F2 score: 0.6605\n",
      "Predicciones guardadas en ../outputs/alexnet/base/26L_weighted_1/0/predictions.csv\n",
      "HS fold 1: 0.4829\n",
      "Usando top_labels previamente generados para 26 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mavergar/anaconda3/envs/Kunisch_10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mavergar/anaconda3/envs/Kunisch_10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Trial [0/200], LR: 0.001, W: 0.01, Avg Loss: 1.1080, Accuracy: 0.8574, F1 score: 0.4288, F2 score: 0.4072\n",
      "Valid Trial [0/200], LR: 0.001, W: 0.01, Avg Loss: 1.5745, Accuracy: 0.8739, F1 score: 0.5462, F2 score: 0.5617\n",
      "\n",
      "Train Trial [1/200], LR: 0.001, W: 0.01, Avg Loss: 0.8609, Accuracy: 0.8828, F1 score: 0.5847, F2 score: 0.5981\n",
      "Valid Trial [1/200], LR: 0.001, W: 0.01, Avg Loss: 1.4899, Accuracy: 0.8659, F1 score: 0.5635, F2 score: 0.6141\n",
      "\n",
      "Train Trial [2/200], LR: 0.001, W: 0.01, Avg Loss: 0.8129, Accuracy: 0.8861, F1 score: 0.6025, F2 score: 0.6221\n",
      "Valid Trial [2/200], LR: 0.001, W: 0.01, Avg Loss: 1.4264, Accuracy: 0.8814, F1 score: 0.5934, F2 score: 0.6279\n",
      "\n",
      "Train Trial [3/200], LR: 0.001, W: 0.01, Avg Loss: 0.7456, Accuracy: 0.8937, F1 score: 0.6293, F2 score: 0.6501\n",
      "Valid Trial [3/200], LR: 0.001, W: 0.01, Avg Loss: 1.7178, Accuracy: 0.8851, F1 score: 0.6055, F2 score: 0.6400\n",
      "\n",
      "Train Trial [4/200], LR: 0.001, W: 0.01, Avg Loss: 0.7563, Accuracy: 0.8929, F1 score: 0.6277, F2 score: 0.6495\n",
      "Valid Trial [4/200], LR: 0.001, W: 0.01, Avg Loss: 1.4324, Accuracy: 0.8649, F1 score: 0.5690, F2 score: 0.6273\n",
      "\n",
      "Train Trial [5/200], LR: 0.001, W: 0.01, Avg Loss: 0.7267, Accuracy: 0.8959, F1 score: 0.6385, F2 score: 0.6612\n",
      "Valid Trial [5/200], LR: 0.001, W: 0.01, Avg Loss: 1.5639, Accuracy: 0.8868, F1 score: 0.5985, F2 score: 0.6211\n",
      "\n",
      "Train Trial [6/200], LR: 0.001, W: 0.01, Avg Loss: 0.6983, Accuracy: 0.9007, F1 score: 0.6556, F2 score: 0.6793\n",
      "Valid Trial [6/200], LR: 0.00075, W: 0.01, Avg Loss: 1.5153, Accuracy: 0.8809, F1 score: 0.5908, F2 score: 0.6245\n",
      "\n",
      "Train Trial [7/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6315, Accuracy: 0.9078, F1 score: 0.6798, F2 score: 0.7040\n",
      "Valid Trial [7/200], LR: 0.00075, W: 0.01, Avg Loss: 1.5015, Accuracy: 0.8739, F1 score: 0.5889, F2 score: 0.6411\n",
      "\n",
      "Train Trial [8/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6127, Accuracy: 0.9104, F1 score: 0.6881, F2 score: 0.7116\n",
      "Valid Trial [8/200], LR: 0.00075, W: 0.01, Avg Loss: 1.5193, Accuracy: 0.8771, F1 score: 0.5756, F2 score: 0.6065\n",
      "\n",
      "Train Trial [9/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6000, Accuracy: 0.9120, F1 score: 0.6938, F2 score: 0.7177\n",
      "Valid Trial [9/200], LR: 0.00075, W: 0.01, Avg Loss: 1.7737, Accuracy: 0.8542, F1 score: 0.5604, F2 score: 0.6374\n",
      "\n",
      "Train Trial [10/200], LR: 0.00075, W: 0.01, Avg Loss: 0.5772, Accuracy: 0.9145, F1 score: 0.7037, F2 score: 0.7293\n",
      "Valid Trial [10/200], LR: 0.00075, W: 0.01, Avg Loss: 1.7879, Accuracy: 0.8868, F1 score: 0.6103, F2 score: 0.6444\n",
      "\n",
      "Train Trial [11/200], LR: 0.00075, W: 0.01, Avg Loss: 0.5689, Accuracy: 0.9168, F1 score: 0.7104, F2 score: 0.7346\n",
      "Valid Trial [11/200], LR: 0.00075, W: 0.01, Avg Loss: 1.7621, Accuracy: 0.8910, F1 score: 0.6250, F2 score: 0.6599\n",
      "\n",
      "Train Trial [12/200], LR: 0.00075, W: 0.01, Avg Loss: 0.5755, Accuracy: 0.9150, F1 score: 0.7044, F2 score: 0.7285\n",
      "Valid Trial [12/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.7744, Accuracy: 0.8755, F1 score: 0.5861, F2 score: 0.6312\n",
      "\n",
      "Train Trial [13/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5135, Accuracy: 0.9239, F1 score: 0.7338, F2 score: 0.7565\n",
      "Valid Trial [13/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.7240, Accuracy: 0.8825, F1 score: 0.6043, F2 score: 0.6462\n",
      "\n",
      "Train Trial [14/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.4962, Accuracy: 0.9255, F1 score: 0.7401, F2 score: 0.7644\n",
      "Valid Trial [14/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.9186, Accuracy: 0.8761, F1 score: 0.5812, F2 score: 0.6202\n",
      "\n",
      "Train Trial [15/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5091, Accuracy: 0.9241, F1 score: 0.7350, F2 score: 0.7582\n",
      "Valid Trial [15/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.9551, Accuracy: 0.8835, F1 score: 0.6007, F2 score: 0.6357\n",
      "\n",
      "Train Trial [16/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.4808, Accuracy: 0.9282, F1 score: 0.7485, F2 score: 0.7707\n",
      "Valid Trial [16/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.9007, Accuracy: 0.8830, F1 score: 0.6068, F2 score: 0.6495\n",
      "\n",
      "Train Trial [17/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.4783, Accuracy: 0.9286, F1 score: 0.7516, F2 score: 0.7772\n",
      "Valid Trial [17/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.9620, Accuracy: 0.8894, F1 score: 0.6057, F2 score: 0.6265\n",
      "\n",
      "Train Trial [18/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.4815, Accuracy: 0.9300, F1 score: 0.7549, F2 score: 0.7776\n",
      "Valid Trial [18/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 2.0019, Accuracy: 0.8745, F1 score: 0.5955, F2 score: 0.6528\n",
      "\n",
      "Train Trial [19/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.4370, Accuracy: 0.9347, F1 score: 0.7709, F2 score: 0.7936\n",
      "Valid Trial [19/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 1.9838, Accuracy: 0.8841, F1 score: 0.6033, F2 score: 0.6390\n",
      "\n",
      "Train Trial [20/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.4268, Accuracy: 0.9365, F1 score: 0.7764, F2 score: 0.7973\n",
      "Valid Trial [20/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 2.1275, Accuracy: 0.8803, F1 score: 0.5789, F2 score: 0.6034\n",
      "\n",
      "Train Trial [21/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.4179, Accuracy: 0.9377, F1 score: 0.7812, F2 score: 0.8034\n",
      "Valid Trial [21/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 2.0428, Accuracy: 0.8862, F1 score: 0.6106, F2 score: 0.6468\n",
      "\n",
      "Out of patience!\n",
      "Best epoch: 11\n",
      "Guardando mejor modelo en ../models/alexnet/base/1/26L_weighted_1.pth\n",
      "CURRENT BATCH SIZE:  124\n",
      "CURRENT BATCH SIZE:  124\n",
      "Test Trial [1/1], LR: 0.0001, W: 0.001, Avg Loss: 12.6086, Accuracy: 0.8971, F1 score: 0.6542, F2 score: 0.6828\n",
      "Predicciones guardadas en ../outputs/alexnet/base/26L_weighted_1/1/predictions.csv\n",
      "HS fold 1: 0.5012\n",
      "Usando top_labels previamente generados para 26 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mavergar/anaconda3/envs/Kunisch_10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mavergar/anaconda3/envs/Kunisch_10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Trial [0/200], LR: 0.001, W: 0.01, Avg Loss: 1.1375, Accuracy: 0.8567, F1 score: 0.4053, F2 score: 0.3773\n",
      "Valid Trial [0/200], LR: 0.001, W: 0.01, Avg Loss: 1.6233, Accuracy: 0.8958, F1 score: 0.5929, F2 score: 0.5613\n",
      "\n",
      "Train Trial [1/200], LR: 0.001, W: 0.01, Avg Loss: 0.9093, Accuracy: 0.8810, F1 score: 0.5640, F2 score: 0.5667\n",
      "Valid Trial [1/200], LR: 0.001, W: 0.01, Avg Loss: 1.4698, Accuracy: 0.8798, F1 score: 0.6101, F2 score: 0.6456\n",
      "\n",
      "Train Trial [2/200], LR: 0.001, W: 0.01, Avg Loss: 0.8266, Accuracy: 0.8899, F1 score: 0.6028, F2 score: 0.6113\n",
      "Valid Trial [2/200], LR: 0.001, W: 0.01, Avg Loss: 1.2994, Accuracy: 0.8942, F1 score: 0.6502, F2 score: 0.6805\n",
      "\n",
      "Train Trial [3/200], LR: 0.001, W: 0.01, Avg Loss: 0.7680, Accuracy: 0.8941, F1 score: 0.6266, F2 score: 0.6442\n",
      "Valid Trial [3/200], LR: 0.001, W: 0.01, Avg Loss: 1.4060, Accuracy: 0.8894, F1 score: 0.6437, F2 score: 0.6840\n",
      "\n",
      "Train Trial [4/200], LR: 0.001, W: 0.01, Avg Loss: 0.7263, Accuracy: 0.8986, F1 score: 0.6426, F2 score: 0.6604\n",
      "Valid Trial [4/200], LR: 0.001, W: 0.01, Avg Loss: 1.5382, Accuracy: 0.8905, F1 score: 0.6447, F2 score: 0.6823\n",
      "\n",
      "Train Trial [5/200], LR: 0.001, W: 0.01, Avg Loss: 0.6902, Accuracy: 0.9018, F1 score: 0.6569, F2 score: 0.6787\n",
      "Valid Trial [5/200], LR: 0.001, W: 0.01, Avg Loss: 1.4311, Accuracy: 0.9049, F1 score: 0.6775, F2 score: 0.6988\n",
      "\n",
      "Train Trial [6/200], LR: 0.001, W: 0.01, Avg Loss: 0.6650, Accuracy: 0.9047, F1 score: 0.6662, F2 score: 0.6873\n",
      "Valid Trial [6/200], LR: 0.00075, W: 0.01, Avg Loss: 1.2987, Accuracy: 0.8916, F1 score: 0.6656, F2 score: 0.7251\n",
      "\n",
      "Train Trial [7/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6222, Accuracy: 0.9112, F1 score: 0.6885, F2 score: 0.7099\n",
      "Valid Trial [7/200], LR: 0.00075, W: 0.01, Avg Loss: 1.6743, Accuracy: 0.8851, F1 score: 0.6287, F2 score: 0.6667\n",
      "\n",
      "Train Trial [8/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6285, Accuracy: 0.9087, F1 score: 0.6808, F2 score: 0.7034\n",
      "Valid Trial [8/200], LR: 0.00075, W: 0.01, Avg Loss: 1.3872, Accuracy: 0.8841, F1 score: 0.6365, F2 score: 0.6869\n",
      "\n",
      "Train Trial [9/200], LR: 0.00075, W: 0.01, Avg Loss: 0.5977, Accuracy: 0.9133, F1 score: 0.6967, F2 score: 0.7195\n",
      "Valid Trial [9/200], LR: 0.00075, W: 0.01, Avg Loss: 1.7740, Accuracy: 0.8894, F1 score: 0.6387, F2 score: 0.6733\n",
      "\n",
      "Train Trial [10/200], LR: 0.00075, W: 0.01, Avg Loss: 0.5896, Accuracy: 0.9143, F1 score: 0.7012, F2 score: 0.7256\n",
      "Valid Trial [10/200], LR: 0.00075, W: 0.01, Avg Loss: 1.3696, Accuracy: 0.8766, F1 score: 0.6219, F2 score: 0.6800\n",
      "\n",
      "Train Trial [11/200], LR: 0.00075, W: 0.01, Avg Loss: 0.5777, Accuracy: 0.9153, F1 score: 0.7036, F2 score: 0.7266\n",
      "Valid Trial [11/200], LR: 0.00075, W: 0.01, Avg Loss: 1.3988, Accuracy: 0.8926, F1 score: 0.6516, F2 score: 0.6897\n",
      "\n",
      "Train Trial [12/200], LR: 0.00075, W: 0.01, Avg Loss: 0.5478, Accuracy: 0.9187, F1 score: 0.7164, F2 score: 0.7410\n",
      "Valid Trial [12/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.5443, Accuracy: 0.8958, F1 score: 0.6620, F2 score: 0.7007\n",
      "\n",
      "Train Trial [13/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5170, Accuracy: 0.9225, F1 score: 0.7290, F2 score: 0.7530\n",
      "Valid Trial [13/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.6434, Accuracy: 0.8900, F1 score: 0.6436, F2 score: 0.6818\n",
      "\n",
      "Train Trial [14/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5290, Accuracy: 0.9214, F1 score: 0.7255, F2 score: 0.7498\n",
      "Valid Trial [14/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.4884, Accuracy: 0.9081, F1 score: 0.6742, F2 score: 0.6773\n",
      "\n",
      "Train Trial [15/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.4979, Accuracy: 0.9259, F1 score: 0.7398, F2 score: 0.7624\n",
      "Valid Trial [15/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.5405, Accuracy: 0.8942, F1 score: 0.6667, F2 score: 0.7174\n",
      "\n",
      "Train Trial [16/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.4818, Accuracy: 0.9278, F1 score: 0.7475, F2 score: 0.7718\n",
      "Valid Trial [16/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.6191, Accuracy: 0.8974, F1 score: 0.6655, F2 score: 0.7022\n",
      "\n",
      "Out of patience!\n",
      "Best epoch: 6\n",
      "Guardando mejor modelo en ../models/alexnet/base/2/26L_weighted_1.pth\n",
      "CURRENT BATCH SIZE:  124\n",
      "CURRENT BATCH SIZE:  124\n",
      "Test Trial [1/1], LR: 0.0001, W: 0.001, Avg Loss: 12.4299, Accuracy: 0.8945, F1 score: 0.6424, F2 score: 0.6721\n",
      "Predicciones guardadas en ../outputs/alexnet/base/26L_weighted_1/2/predictions.csv\n",
      "HS fold 1: 0.5051\n",
      "Usando top_labels previamente generados para 26 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mavergar/anaconda3/envs/Kunisch_10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mavergar/anaconda3/envs/Kunisch_10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Trial [0/200], LR: 0.001, W: 0.01, Avg Loss: 1.1626, Accuracy: 0.8526, F1 score: 0.3600, F2 score: 0.3244\n",
      "Valid Trial [0/200], LR: 0.001, W: 0.01, Avg Loss: 1.6752, Accuracy: 0.8745, F1 score: 0.5253, F2 score: 0.5134\n",
      "\n",
      "Train Trial [1/200], LR: 0.001, W: 0.01, Avg Loss: 0.9149, Accuracy: 0.8804, F1 score: 0.5631, F2 score: 0.5647\n",
      "Valid Trial [1/200], LR: 0.001, W: 0.01, Avg Loss: 1.6749, Accuracy: 0.8996, F1 score: 0.6194, F2 score: 0.6047\n",
      "\n",
      "Train Trial [2/200], LR: 0.001, W: 0.01, Avg Loss: 0.8322, Accuracy: 0.8911, F1 score: 0.6122, F2 score: 0.6232\n",
      "Valid Trial [2/200], LR: 0.001, W: 0.01, Avg Loss: 1.5274, Accuracy: 0.8809, F1 score: 0.5908, F2 score: 0.6117\n",
      "\n",
      "Train Trial [3/200], LR: 0.001, W: 0.01, Avg Loss: 0.7989, Accuracy: 0.8925, F1 score: 0.6207, F2 score: 0.6355\n",
      "Valid Trial [3/200], LR: 0.001, W: 0.01, Avg Loss: 1.4895, Accuracy: 0.8884, F1 score: 0.6122, F2 score: 0.6298\n",
      "\n",
      "Train Trial [4/200], LR: 0.001, W: 0.01, Avg Loss: 0.7416, Accuracy: 0.8984, F1 score: 0.6439, F2 score: 0.6618\n",
      "Valid Trial [4/200], LR: 0.001, W: 0.01, Avg Loss: 1.5959, Accuracy: 0.8542, F1 score: 0.5517, F2 score: 0.6087\n",
      "\n",
      "Train Trial [5/200], LR: 0.001, W: 0.01, Avg Loss: 0.7208, Accuracy: 0.8981, F1 score: 0.6484, F2 score: 0.6727\n",
      "Valid Trial [5/200], LR: 0.001, W: 0.01, Avg Loss: 1.5978, Accuracy: 0.8665, F1 score: 0.5719, F2 score: 0.6162\n",
      "\n",
      "Train Trial [6/200], LR: 0.001, W: 0.01, Avg Loss: 0.6972, Accuracy: 0.9014, F1 score: 0.6582, F2 score: 0.6809\n",
      "Valid Trial [6/200], LR: 0.00075, W: 0.01, Avg Loss: 1.5948, Accuracy: 0.8643, F1 score: 0.5753, F2 score: 0.6282\n",
      "\n",
      "Train Trial [7/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6635, Accuracy: 0.9046, F1 score: 0.6699, F2 score: 0.6939\n",
      "Valid Trial [7/200], LR: 0.00075, W: 0.01, Avg Loss: 1.7165, Accuracy: 0.8798, F1 score: 0.5887, F2 score: 0.6108\n",
      "\n",
      "Train Trial [8/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6343, Accuracy: 0.9088, F1 score: 0.6839, F2 score: 0.7076\n",
      "Valid Trial [8/200], LR: 0.00075, W: 0.01, Avg Loss: 1.5371, Accuracy: 0.8665, F1 score: 0.5660, F2 score: 0.6050\n",
      "\n",
      "Train Trial [9/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6493, Accuracy: 0.9059, F1 score: 0.6752, F2 score: 0.7002\n",
      "Valid Trial [9/200], LR: 0.00075, W: 0.01, Avg Loss: 1.5416, Accuracy: 0.8868, F1 score: 0.6187, F2 score: 0.6481\n",
      "\n",
      "Train Trial [10/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6226, Accuracy: 0.9082, F1 score: 0.6839, F2 score: 0.7106\n",
      "Valid Trial [10/200], LR: 0.00075, W: 0.01, Avg Loss: 1.7980, Accuracy: 0.8707, F1 score: 0.5842, F2 score: 0.6282\n",
      "\n",
      "Train Trial [11/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6168, Accuracy: 0.9098, F1 score: 0.6882, F2 score: 0.7131\n",
      "Valid Trial [11/200], LR: 0.00075, W: 0.01, Avg Loss: 1.7781, Accuracy: 0.8868, F1 score: 0.6074, F2 score: 0.6255\n",
      "\n",
      "Train Trial [12/200], LR: 0.00075, W: 0.01, Avg Loss: 0.5861, Accuracy: 0.9128, F1 score: 0.7008, F2 score: 0.7294\n",
      "Valid Trial [12/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.5823, Accuracy: 0.8841, F1 score: 0.5959, F2 score: 0.6116\n",
      "\n",
      "Train Trial [13/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5631, Accuracy: 0.9162, F1 score: 0.7107, F2 score: 0.7371\n",
      "Valid Trial [13/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.6633, Accuracy: 0.8814, F1 score: 0.6022, F2 score: 0.6321\n",
      "\n",
      "Train Trial [14/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5305, Accuracy: 0.9203, F1 score: 0.7247, F2 score: 0.7514\n",
      "Valid Trial [14/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.7602, Accuracy: 0.8851, F1 score: 0.6084, F2 score: 0.6326\n",
      "\n",
      "Train Trial [15/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5383, Accuracy: 0.9203, F1 score: 0.7243, F2 score: 0.7503\n",
      "Valid Trial [15/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.8311, Accuracy: 0.8814, F1 score: 0.6050, F2 score: 0.6377\n",
      "\n",
      "Train Trial [16/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5172, Accuracy: 0.9221, F1 score: 0.7325, F2 score: 0.7616\n",
      "Valid Trial [16/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.7846, Accuracy: 0.8857, F1 score: 0.6037, F2 score: 0.6217\n",
      "\n",
      "Train Trial [17/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5123, Accuracy: 0.9232, F1 score: 0.7336, F2 score: 0.7588\n",
      "Valid Trial [17/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 1.7704, Accuracy: 0.8750, F1 score: 0.5880, F2 score: 0.6236\n",
      "\n",
      "Train Trial [18/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5023, Accuracy: 0.9250, F1 score: 0.7405, F2 score: 0.7669\n",
      "Valid Trial [18/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 1.7853, Accuracy: 0.8691, F1 score: 0.5826, F2 score: 0.6296\n",
      "\n",
      "Train Trial [19/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.4822, Accuracy: 0.9275, F1 score: 0.7492, F2 score: 0.7759\n",
      "Valid Trial [19/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 1.7922, Accuracy: 0.8926, F1 score: 0.6271, F2 score: 0.6450\n",
      "\n",
      "Out of patience!\n",
      "Best epoch: 9\n",
      "Guardando mejor modelo en ../models/alexnet/base/3/26L_weighted_1.pth\n",
      "CURRENT BATCH SIZE:  124\n",
      "CURRENT BATCH SIZE:  124\n",
      "Test Trial [1/1], LR: 0.0001, W: 0.001, Avg Loss: 10.7152, Accuracy: 0.8954, F1 score: 0.6313, F2 score: 0.6475\n",
      "Predicciones guardadas en ../outputs/alexnet/base/26L_weighted_1/3/predictions.csv\n",
      "HS fold 1: 0.4856\n",
      "HS Final:  0.4937\n",
      "F1 Final:  0.6414\n",
      "F2 Final:  0.6657\n",
      "1MR Final:  0.9111\n",
      "5MR Final:  0.4253\n"
     ]
    }
   ],
   "source": [
    "# Instanciamos un pruner con la cantidad de etiquetas en estudio\n",
    "pruner = KunischPruner(NUM_LABELS)\n",
    "\n",
    "# Inicializamos sumas de scores en 0, para cada score de interés\n",
    "sum_f1 = 0\n",
    "sum_f2 = 0\n",
    "sum_recall = 0\n",
    "sum_precision = 0\n",
    "sum_acc = 0\n",
    "sum_hl = 0\n",
    "sum_emr = 0\n",
    "sum_hs = 0\n",
    "sum_mr1 = 0\n",
    "sum_mr2 = 0\n",
    "sum_mr3 = 0\n",
    "sum_mr4 = 0\n",
    "sum_mr5 = 0\n",
    "\n",
    "# Iterar sobre los folds, entrenando desde 0 (sin data leakage) y reportando\n",
    "# los resultados en las variables anteriores (sum_...)\n",
    "for i in range(0, K):\n",
    "    fold = Kfolds[i]\n",
    "    labels_dir = fold['labels_dir']\n",
    "    patterns_dir = fold['patterns_dir']\n",
    "    output_dir = fold['output_dir']\n",
    "    model_path = fold['model_path']\n",
    "    \n",
    "    # Carga de top labels\n",
    "    train_labels = pd.read_json(os.path.join(labels_dir, 'augmented_train_df.json'), orient='index')\n",
    "    \n",
    "    if not os.path.isfile(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle')):\n",
    "        print(f\"Creando top_labels para {NUM_LABELS} labels\")\n",
    "        top_labels = pruner.filter_labels(train_labels)\n",
    "        pruner.set_top_labels(top_labels)\n",
    "        \n",
    "        save = input(f\"Se creará un archivo nuevo para {len(top_labels)} labels. Desea continuar? (y/n)\")\n",
    "        if save == \"y\":\n",
    "            with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'wb') as f:\n",
    "                pickle.dump(top_labels, f)\n",
    "            print(\"Top labels creado con éxito\")\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"No se logró cargar top_labels\")\n",
    "            \n",
    "    else: \n",
    "        print(f\"Usando top_labels previamente generados para {NUM_LABELS} labels\")\n",
    "        with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'rb') as f:\n",
    "            top_labels = pickle.load(f)\n",
    "\n",
    "    REAL_NUM_LABELS = len(top_labels) # la cantidad final de etiquetas a trabajar\n",
    "\n",
    "    # Alexnet requires 227 x 227\n",
    "    # Training\n",
    "    kunischTrainSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'train'),\n",
    "                                     labels_file=os.path.join(labels_dir, 'augmented_train_df.json'),\n",
    "                                     transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                   transforms.ToTensor(),\n",
    "                                                                   transforms.Normalize(\n",
    "                                                                       mean=[0.485, 0.456, 0.406],\n",
    "                                                                       std=[0.229, 0.224, 0.225])]),\n",
    "                                     top_labels=top_labels)\n",
    "\n",
    "    kunischTrainLoader = torch.utils.data.DataLoader(kunischTrainSet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Validation\n",
    "    kunischValidationSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'val'),\n",
    "                                          labels_file=os.path.join(labels_dir, 'val_df.json'),\n",
    "                                          transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                        transforms.ToTensor(),\n",
    "                                                                        transforms.Normalize(\n",
    "                                                                            mean=[0.485, 0.456, 0.406],\n",
    "                                                                            std=[0.229, 0.224, 0.225])]),\n",
    "                                          top_labels=top_labels)\n",
    "\n",
    "    kunischValidationLoader = torch.utils.data.DataLoader(kunischValidationSet, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                                          num_workers=0)\n",
    "\n",
    "    # Test\n",
    "    kunischTestSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'test'),\n",
    "                                    labels_file=os.path.join(labels_dir, 'test_df.json'),\n",
    "                                    transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                  transforms.ToTensor(),\n",
    "                                                                  transforms.Normalize(\n",
    "                                                                      mean=[0.485, 0.456, 0.406],\n",
    "                                                                      std=[0.229, 0.224, 0.225])]),\n",
    "                                    top_labels=top_labels)\n",
    "\n",
    "    kunischTestLoader = torch.utils.data.DataLoader(kunischTestSet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Creacion de pesos positivos\n",
    "    if use_pos_weights:\n",
    "        pos_weights = make_positive_weights(top_labels, len(kunischTrainSet), pos_weights_factor)\n",
    "        pos_weights = torch.Tensor(pos_weights).float().to(device)\n",
    "\n",
    "    else:\n",
    "        pos_weights = None\n",
    "        \n",
    "    hyperval = \"\"\"\n",
    "    # Hyper Parameter Tuning\n",
    "    alex_net = models.alexnet(pretrained=True)\n",
    "    for param in alex_net.parameters():\n",
    "        param.requires_grad = False\n",
    "    alex_net.classifier._modules['6'] = nn.Linear(4096, REAL_NUM_LABELS)\n",
    "\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    num_epochs = 5\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      learning_rate = round(np.exp(random.uniform(np.log(.0001), np.log(.01))), 4)  # pull geometrically\n",
    "      w = round(np.exp(random.uniform(np.log(3.1e-7), np.log(3.1e-5))), 10)  # pull geometrically\n",
    "\n",
    "      # Reset Model per test\n",
    "      alex_net = models.alexnet(pretrained=True)\n",
    "      alex_net.classifier._modules['6'] = nn.Linear(4096, REAL_NUM_LABELS)\n",
    "      alex_net.to(device)\n",
    "\n",
    "      optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "      criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=5, min_lr=0.00005)\n",
    "\n",
    "      alex_train(epoch, num_epochs, train_losses, learning_rate, w, REAL_NUM_LABELS)\n",
    "      if SAVE:\n",
    "          train_losses_df = pd.DataFrame(train_losses)\n",
    "          train_losses_df.to_csv(os.path.join(output_dir, 'loss_hypertrain.csv'))\n",
    "\n",
    "      alex_valid(epoch, num_epochs, validation_losses, learning_rate, w, REAL_NUM_LABELS)\n",
    "      if SAVE:\n",
    "          validation_losses_df = pd.DataFrame(validation_losses)\n",
    "          validation_losses_df.to_csv(os.path.join(output_dir, 'loss_hyperval.csv'))\n",
    "     \"\"\"\n",
    "\n",
    "    # Training\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    num_epochs = 200\n",
    "    learning_rate = 0.001\n",
    "    w = 0.01\n",
    "\n",
    "    # Early Stopping\n",
    "    patience = 10\n",
    "    bad_epochs = 0\n",
    "    best_score = 0.0\n",
    "    best_weights = None\n",
    "\n",
    "    alex_net = models.alexnet(pretrained=True)\n",
    "    alex_net.classifier._modules['6'] = nn.Linear(4096, REAL_NUM_LABELS)\n",
    "    alex_net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=5, min_lr=0.0001)\n",
    "    # scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      score_train = alex_train(epoch, num_epochs, train_losses, learning_rate, w, REAL_NUM_LABELS)\n",
    "      score_valid = alex_valid(epoch, num_epochs, validation_losses, learning_rate, w, REAL_NUM_LABELS)\n",
    "      print(\"\")\n",
    "\n",
    "      # Early Stopping\n",
    "      if score_valid > best_score:\n",
    "        bad_epochs = 0\n",
    "        best_epoch = epoch\n",
    "        best_score = score_valid\n",
    "        best_weights = alex_net.state_dict()\n",
    "      else:\n",
    "        bad_epochs += 1\n",
    "\n",
    "      if bad_epochs == patience:\n",
    "        print(\"Out of patience!\")\n",
    "        print(f\"Best epoch: {best_epoch}\")\n",
    "        break\n",
    "\n",
    "    if SAVE:\n",
    "        print(f\"Guardando mejor modelo en {model_path}\")\n",
    "        torch.save(best_weights, model_path)\n",
    "\n",
    "\n",
    "    # Testing\n",
    "    test_losses = []\n",
    "    test_pred = []\n",
    "    learning_rate = 0.0001\n",
    "    w = 0.001\n",
    "\n",
    "    # Reset Model\n",
    "    alex_net = models.alexnet(pretrained=True)\n",
    "    alex_net.classifier._modules['6'] = nn.Linear(4096, REAL_NUM_LABELS)\n",
    "    alex_net.load_state_dict(torch.load(model_path))\n",
    "    alex_net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    alex_test(1, 1, test_pred, test_losses, learning_rate, w, REAL_NUM_LABELS, show_images = 0)\n",
    "    test_pred_df = pd.DataFrame(test_pred)\n",
    "    \n",
    "    # Guardar resultados si corresponde\n",
    "    preds = test_pred[0][3].cpu().detach().numpy()\n",
    "    for i in range(1, len(test_pred)):\n",
    "      pbi = test_pred[i][3].cpu().detach().numpy()\n",
    "      preds = np.concatenate((preds, pbi), axis=0)\n",
    "        \n",
    "    if SAVE:\n",
    "        save_df = pd.DataFrame(preds)\n",
    "        save_df.to_csv(os.path.join(output_dir, 'predictions.csv'))\n",
    "        print(f\"Predicciones guardadas en {os.path.join(output_dir, 'predictions.csv')}\")\n",
    "        preds = pd.read_csv(os.path.join(output_dir, 'predictions.csv'), index_col=0)\n",
    "        preds = preds.values\n",
    "\n",
    "    pruner = KunischPruner(preds.shape[1])\n",
    "    pruner.set_top_labels(top_labels)\n",
    "    labels_test = pd.read_json(os.path.join(labels_dir, 'test_df.json'), orient='index')\n",
    "    test = pruner.filter_df(labels_test)\n",
    "    \n",
    "    metrics = KunischMetrics(test.values, preds)\n",
    "    sum_f1 += metrics.f1()\n",
    "    sum_f2 += metrics.f2()\n",
    "    sum_recall += metrics.recall()\n",
    "    sum_precision += metrics.precision()\n",
    "    sum_acc += metrics.acc()\n",
    "    sum_hl += metrics.hl()\n",
    "    sum_emr += metrics.emr()\n",
    "    sum_hs += metrics.hs()\n",
    "    sum_mr1 += metrics.mr1()\n",
    "    sum_mr2 += metrics.mr2()\n",
    "    sum_mr3 += metrics.mr3()\n",
    "    sum_mr4 += metrics.mr4()\n",
    "    sum_mr5 += metrics.mr5()\n",
    "\n",
    "    print(f\"HS fold {i}: {metrics.hs()}\")\n",
    "\n",
    "# Los scores finales como el promedio de los scores individuales\n",
    "avg_f1 = round(sum_f1/K, 4)\n",
    "avg_f2 = round(sum_f2/K, 4)\n",
    "avg_recall = round(sum_recall/K, 4)\n",
    "avg_precision = round(sum_precision/K, 4)\n",
    "avg_acc = round(sum_acc/K, 4)\n",
    "avg_hl = round(sum_hl/K, 4)\n",
    "avg_emr = round(sum_emr/K, 4)\n",
    "avg_hs = round(sum_hs/K, 4)\n",
    "avg_mr1 = round(sum_mr1/K, 4)\n",
    "avg_mr2 = round(sum_mr2/K, 4)\n",
    "avg_mr3 = round(sum_mr3/K, 4)\n",
    "avg_mr4 = round(sum_mr4/K, 4)\n",
    "avg_mr5 = round(sum_mr5/K, 4)\n",
    "\n",
    "# Generar un archivo de metadatos \n",
    "metadata = {\n",
    "'data_flags': data_flags,\n",
    "'use_pos_weights': use_pos_weights,\n",
    "'pos_weights_factor': pos_weights_factor,\n",
    "'patience': patience,\n",
    "'batch_size': BATCH_SIZE,\n",
    "'optimizer': (type (optimizer).__name__),\n",
    "'scheduler': (type (scheduler).__name__),\n",
    "'criterion': (type (criterion).__name__),\n",
    "'epochs': num_epochs,\n",
    "'best_epoch': best_epoch,\n",
    "'num_labels': REAL_NUM_LABELS,\n",
    "'TH_TRAIN': TH_TRAIN,\n",
    "'TH_VAL': TH_VAL,\n",
    "'TH_TEST': TH_TEST,\n",
    "'f1': avg_f1,\n",
    "'f2': avg_f2,\n",
    "'recall': avg_recall,\n",
    "'precision': avg_precision,\n",
    "'acc': avg_acc,\n",
    "'hl': avg_hl,\n",
    "'emr': avg_emr,\n",
    "'hs': avg_hs,\n",
    "'mr1': avg_mr1,\n",
    "'mr2': avg_mr2,\n",
    "'mr3': avg_mr3,\n",
    "'mr4': avg_mr4,\n",
    "'mr5': avg_mr5\n",
    "}\n",
    "\n",
    "# Reportar resultados interesantes por print\n",
    "print(\"HS Final: \", avg_hs)\n",
    "print(\"F1 Final: \", avg_f1)\n",
    "print(\"F2 Final: \", avg_f2)\n",
    "print(\"1MR Final: \", avg_mr1)\n",
    "print(\"5MR Final: \", avg_mr5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar metadatos si corresponde\n",
    "if SAVE:\n",
    "    metadf = pd.DataFrame.from_dict(metadata, orient='index')\n",
    "    # output_dir pero sin numero de fold\n",
    "    metadata_dir = os.path.join(root_dir, \"outputs\", \"alexnet\", data_flags, exp_name)\n",
    "    os.makedirs(metadata_dir, exist_ok = True)\n",
    "    metadf.to_csv(os.path.join(metadata_dir, 'metadata.csv'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN Multilabeling through AlexNet",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
