{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzn7gj5P3DXt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# Split and data augmentation\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "El objetivo de este notebook es realizar data augmentation en los patrones a través de la aplicación de distintas transformaciones lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woMl7NKb3LyB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bDe9EneU2rIG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import imgaug.augmenters as aug\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuración de datos y modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Es suficiente editar la celda a continuación para seleccionar las DS_FLAGS y el número de veces que se repetirán las flags múltiples (crop, randaug, elastic, gausblur) y luego correr el resto de las celdas. De esta forma, dos carpetas se crearán: una de etiquetas y una de patrones. Ambas tendrán un nombre dado por las flags seleccionadas, separadas por \"_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DS_FLAGS = ['ref', 'rot', 'rain', 'elastic', 'blur', 'crop', 'randaug']\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'gausblur': [gausblur]\n",
    "              # 'msblur': [msblur]\n",
    "              # 'mtnblur': [mtnblur]\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "CROP_TIMES = 1\n",
    "RANDOM_TIMES = 3\n",
    "ELASTIC_TIMES = 1\n",
    "GAUSBLUR_TIMES = 1\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "         'gausblur': GAUSBLUR_TIMES,\n",
    "}\n",
    "\n",
    "K = 4 # k fold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Definición de las transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blur', 'crop', 'elastic', 'rain', 'randaug', 'ref', 'rot']\n"
     ]
    }
   ],
   "source": [
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic', 'gausblur']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "        \n",
    "print(DS_FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rotate90(path):\n",
    "    image = cv2.imread(path)\n",
    "    rotated = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "    # cv2.imshow(\"90\", rotated)\n",
    "    return rotated, \"rot90\"\n",
    "\n",
    "\n",
    "def rotate180(path):\n",
    "    image = cv2.imread(path)\n",
    "    rotated = cv2.rotate(image, cv2.ROTATE_180)\n",
    "    # cv2.imshow(\"180\", rotated)\n",
    "    return rotated, \"rot180\"\n",
    "\n",
    "\n",
    "def rotate270(path):\n",
    "    image = cv2.imread(path)\n",
    "    rotated = cv2.rotate(image, cv2.ROTATE_180)\n",
    "    rotated = cv2.rotate(rotated, cv2.ROTATE_90_CLOCKWISE)\n",
    "    # cv2.imshow(\"270\", rotated)\n",
    "    return rotated, \"rot270\"\n",
    "\n",
    "\n",
    "def invertX(path):\n",
    "    image = cv2.imread(path)\n",
    "    flipped = cv2.flip(image, 1)\n",
    "    # cv2.imshow(\"flipX\", flipped)\n",
    "    return flipped, \"invX\"\n",
    "\n",
    "\n",
    "def invertY(path):\n",
    "    image = cv2.imread(path)\n",
    "    flipped = cv2.flip(image, 0)\n",
    "    # cv2.imshow(\"flipY\", flipped)\n",
    "    return flipped, \"invY\"\n",
    "\n",
    "\n",
    "def crop(path, min_width = 1/2, min_height= 1/2, max_width = 1/1.1,\n",
    "         max_height = 1/1.1):\n",
    "    image = cv2.imread(path)\n",
    "    height, width = image.shape[0], image.shape[1] # Caution: there are images in RGB and GS\n",
    "    min_width = math.ceil(width * min_width)\n",
    "    min_height = math.ceil(height * min_height)\n",
    "    max_width = math.ceil(width * max_width)\n",
    "    max_height = math.ceil(height * max_height)\n",
    "    x1 = random.randint(0, width - min_width)\n",
    "    w = random.randint(min_width, width - x1)\n",
    "    y1 = random.randint(0, height - min_height)\n",
    "    h = random.randint(min_height, height - y1)\n",
    "    crop = image[y1:y1+h, x1:x1+w]\n",
    "    return crop, \"crop\"\n",
    "\n",
    "def blur(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.AverageBlur(k=(4, 11))(image=image)\n",
    "    return image_aug, \"blur\"\n",
    "\n",
    "def gausblur(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.GaussianBlur(sigma=random.uniform(2,10))(image=image)\n",
    "    return image_aug, \"gausblur\"\n",
    "\n",
    "def msblur(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.MeanShiftBlur()(image=image)\n",
    "    return image_aug, \"msblur\"\n",
    "\n",
    "def mtnblur(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.MotionBlur(random.randint(10,359))(image=image)\n",
    "    return image_aug, \"mtnblur\"\n",
    "\n",
    "\n",
    "def emboss(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))(image = image)\n",
    "    return image_aug, \"embs\"\n",
    "\n",
    "def elastic(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.PiecewiseAffine(scale=(0.03, 0.075))(image = image)\n",
    "    return image_aug, \"elastic\"\n",
    "\n",
    "def randaug(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.RandAugment(m=(2, 9))(image = image)\n",
    "    return image_aug, \"randaug\"\n",
    "\n",
    "def snow(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.Snowflakes(flake_size=(0.6, 0.5), speed=(0.2, 0.5))(image = image)\n",
    "    return image_aug, \"snow\"\n",
    "\n",
    "\n",
    "def rain(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.Rain(speed=(0.1, 0.5))(image = image)\n",
    "    return image_aug, \"rain\"\n",
    "\n",
    "\n",
    "def apply_transformations(pin, pout, transformations):\n",
    "    # ../patterns/originals/84e/84e.png\n",
    "    new_names = []\n",
    "    i = 0\n",
    "    for transformation in transformations:\n",
    "        result, transf_name = transformation(pin)\n",
    "        if transf_name in MULTIPLE_TRANSF: # special treatment for crops and randoms\n",
    "          transf_name += str(i)\n",
    "          i+=1\n",
    "        pin = os.path.normpath(pin)\n",
    "        path_els = pin.split(os.sep)\n",
    "        obj_name = path_els[3] + \"_\" + transf_name\n",
    "        filename = obj_name + \".png\"\n",
    "        os.makedirs(pout, exist_ok = True)\n",
    "        cv2.imwrite(os.path.join(pout, filename), result)\n",
    "        new_names.append(obj_name)\n",
    "    return new_names\n",
    "\n",
    "# Select data augmentation functions based on data flags\n",
    "\n",
    "MAP_FLAGS = {'ref': [invertX, invertY],\n",
    "             'rot': [rotate90, rotate180, rotate270],\n",
    "             'crop': [crop],\n",
    "             'blur': [blur],\n",
    "             'gausblur': [gausblur],\n",
    "             'mtnblur': [mtnblur],\n",
    "             'msblur': [msblur],\n",
    "             'emboss': [emboss],\n",
    "             'randaug': [randaug],\n",
    "             'rain': [rain],\n",
    "             'elastic': [elastic]\n",
    "             # snow is not working properly\n",
    "             }\n",
    "\n",
    "# Las transformaciones horizontales son solo aquellas que respetan la etiqueta 'horizontal'.\n",
    "# Las transformaciones verticales son solo aquellas que respetan la etiqueta 'vertical'.\n",
    "# Las transformaciones comunes son el conjunto completo, es decir, aquellas que son aplicables\n",
    "# a patrones sin etiquetas horizontal ni vertical.\n",
    "ALLOWED_TRANSFORMATIONS = []\n",
    "for f in DS_FLAGS:\n",
    "    ALLOWED_TRANSFORMATIONS += MAP_FLAGS[f]\n",
    "\n",
    "HOR_TRANSFORMATIONS = [invertX, invertY, rotate180, blur, rain, emboss, mtnblur, gausblur, msblur]\n",
    "VER_TRANSFORMATIONS = [invertY, invertX, rotate180, blur, rain, emboss, mtnblur, gausblur, msblur]\n",
    "COMMON_TRANSFORMATIONS = [invertX, invertY, rotate90, rotate180, rotate270,\n",
    "                          blur, rain, emboss, mtnblur, gausblur, msblur]\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        HOR_TRANSFORMATIONS += MAP_FLAGS[t] * MAP_TIMES[t]\n",
    "        VER_TRANSFORMATIONS += MAP_FLAGS[t] * MAP_TIMES[t]\n",
    "        COMMON_TRANSFORMATIONS += MAP_FLAGS[t] * MAP_TIMES[t]\n",
    "\n",
    "\n",
    "def merge_transformations(flags, map_flags, map_times, trans_list): \n",
    "    # could be improved a lot \n",
    "    for k, v in map_flags.items():\n",
    "        if k not in flags:\n",
    "            for el in v:\n",
    "                while el in trans_list:\n",
    "                    trans_list.remove(el)\n",
    "    print(trans_list)\n",
    "    return trans_list\n",
    "\n",
    "def apply_transformations(pin, pout, transformations):\n",
    "    # ../dibujos/aumentar/84e.png\n",
    "    new_names = []\n",
    "    i = 0\n",
    "    for transformation in transformations:\n",
    "        result, transf_name = transformation(pin)\n",
    "        if transf_name in MULTIPLE_TRANSF: # special treatment for crops and randoms\n",
    "          transf_name += str(i)\n",
    "          i+=1\n",
    "        pin = os.path.normpath(pin)\n",
    "        path_els = pin.split(os.sep)\n",
    "        obj_name = path_els[3].split('.')[0] + \"_\" + transf_name\n",
    "        filename = obj_name + \".png\"\n",
    "        os.makedirs(pout, exist_ok = True)\n",
    "        cv2.imwrite(os.path.join(pout, filename), result)\n",
    "        new_names.append(obj_name)\n",
    "    return new_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split en folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta celda se encarga de generar el holdout en K folds. \n",
    "- Primero intenta leer un índice de holdout ya existente en labels/holdout. \n",
    "- Si no lo encuentra, y si es que GENERAR=True, lo genera y continua con el proceso. \n",
    "- Si no lo encuentra y Generar=True, da error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando indices previamente generados\n",
      "Elementos del fold 0\n",
      "-- Elementos de entrenamiento: 504 - Muestra: ['14j' '3i' '24b' '37b' '44n']\n",
      "-- Elementos de validación: 78 - Muestra: ['64c' '89e' '51c' '76h' '32h']\n",
      "-- Elementos de test: 194 - Muestra: ['40b' '16l' '19e' '77j' '70q']\n",
      "Elementos del fold 1\n",
      "-- Elementos de entrenamiento: 504 - Muestra: ['3e' '3a' '83a' '78b' '16e']\n",
      "-- Elementos de validación: 78 - Muestra: ['10b' '42b' '70k' '45c' '37i']\n",
      "-- Elementos de test: 194 - Muestra: ['46b' '17f' '92d' '8d' '31d']\n",
      "Elementos del fold 2\n",
      "-- Elementos de entrenamiento: 504 - Muestra: ['29c' '4d' '72e' '34d' '11j']\n",
      "-- Elementos de validación: 78 - Muestra: ['77e' '65d' '81g' '16h' '16a']\n",
      "-- Elementos de test: 194 - Muestra: ['46e' '49b' '14a' '26b' '20a']\n",
      "Elementos del fold 3\n",
      "-- Elementos de entrenamiento: 504 - Muestra: ['44l' '66f' '18g' '8c' '95c']\n",
      "-- Elementos de validación: 78 - Muestra: ['64h' '88c' '27c' '91a' '45c']\n",
      "-- Elementos de test: 194 - Muestra: ['44c' '86d' '11k' '47k' '78m']\n"
     ]
    }
   ],
   "source": [
    "GENERAR = True\n",
    "\n",
    "labels_dir = os.path.join(root_dir, \"labels\")\n",
    "df = pd.read_json(os.path.join(labels_dir, \"normalized_df.json\"), orient='index')\n",
    "classes = pd.read_csv(os.path.join(labels_dir, \"class_labels.csv\"), index_col=0)\n",
    "colnames = df.columns\n",
    "holdout_dir = os.path.join(labels_dir, \"holdout\")\n",
    "os.makedirs(holdout_dir, exist_ok = True)\n",
    "\n",
    "ERROR = False\n",
    "\n",
    "train_sets = []\n",
    "val_sets = []\n",
    "test_sets = []\n",
    "\n",
    "for i in range(0, K):\n",
    "    found_train_elems = os.path.isfile(os.path.join(holdout_dir, \"elem_train_\" + str(i) + \".npy\")) \n",
    "    found_val_elems = os.path.isfile(os.path.join(holdout_dir, \"elem_val_\" + str(i) + \".npy\"))\n",
    "    found_test_elems = os.path.isfile(os.path.join(holdout_dir, \"elem_test_\" + str(i) + \".npy\"))\n",
    "    \n",
    "    if (not found_train_elems) or (not found_val_elems) or (not found_test_elems):\n",
    "        print(\"No se encontraron los datos del fold \", i)\n",
    "        ERROR = True\n",
    "\n",
    "if ERROR and not GENERAR:\n",
    "        raise Exception(\"\"\"\n",
    "        No hay particiones para CV pero GENERAR está seteado como False. \n",
    "        Revise los paths o cambie el valor de GENERAR a True.\n",
    "        \"\"\")\n",
    "        \n",
    "if not ERROR: #archivos existian desde antes\n",
    "    print(\"Cargando indices previamente generados\")\n",
    "    for i in range(0, K):\n",
    "        elem_train = elem_test = elem_val = None\n",
    "        with open(os.path.join(holdout_dir, f'elem_train_{i}.npy'), 'rb') as f:\n",
    "            elem_train = np.load(f, allow_pickle = True)\n",
    "\n",
    "        with open(os.path.join(holdout_dir, f'elem_val_{i}.npy'), 'rb') as f:\n",
    "            elem_val = np.load(f, allow_pickle = True)\n",
    "\n",
    "        with open(os.path.join(holdout_dir, f'elem_test_{i}.npy'), 'rb') as f:\n",
    "            elem_test = np.load(f, allow_pickle = True)\n",
    "\n",
    "        train_sets.append(elem_train)\n",
    "        val_sets.append(elem_val)\n",
    "        test_sets.append(elem_test)\n",
    "\n",
    "        print(f\"Elementos del fold {i}\")\n",
    "        print(f\"-- Elementos de entrenamiento: {len(elem_train)} - Muestra: {elem_train[0:5]}\" )\n",
    "        print(f\"-- Elementos de validación: {len(elem_val)} - Muestra: {elem_val[0:5]}\")\n",
    "        print(f\"-- Elementos de test: {len(elem_test)} - Muestra: {elem_test[0:5]}\")\n",
    "        \n",
    "if ERROR and GENERAR:\n",
    "    print(\"GENERAR está activado. Generando particiones nuevas\")\n",
    "    \n",
    "    df = df.sample(frac=1)\n",
    "    index = df.index.values\n",
    "\n",
    "    testNumber = math.ceil(len(index)/K)\n",
    "    valNumber = math.ceil(0.1 * len(index))\n",
    "    trainNumber = len(index) - valNumber - testNumber\n",
    "    print(valNumber, testNumber, trainNumber)\n",
    "    \n",
    "    assert (valNumber + testNumber + trainNumber) == len(index)\n",
    "\n",
    "    for i in range(0, K):\n",
    "        first_test_index = i * testNumber\n",
    "        last_test_index = (i + 1) * testNumber\n",
    "        print(first_test_index, last_test_index)\n",
    "        \n",
    "        test_set = index[first_test_index : last_test_index]\n",
    "        \n",
    "        resto = np.setdiff1d(index, test_set)\n",
    "        np.random.shuffle(resto)\n",
    "            \n",
    "        val_set = resto[0 : valNumber]\n",
    "        resto = np.setdiff1d(resto, val_set)\n",
    "        np.random.shuffle(resto)\n",
    "                \n",
    "        train_set = resto\n",
    "        \n",
    "        test_sets.append(test_set)\n",
    "        val_sets.append(val_set)\n",
    "        train_sets.append(train_set)\n",
    "\n",
    "        print(f\"Fold {i}\")\n",
    "        print(\"-- Patterns for training: {} - Muestra: {}\".format(len(train_set), sorted(train_set[0:5])))\n",
    "        print(\"- Patterns for validation: {} - Muestra: {}\".format(len(val_set), sorted(val_set[0:5])))\n",
    "        print(\"-- Patterns for testing: {} - Muestra: {}\".format(len(test_set), sorted(test_set[0:5])))\n",
    "        \n",
    "        with open(os.path.join(holdout_dir, f'elem_train_{i}.npy'), 'wb') as f:\n",
    "            np.save(f, train_set)\n",
    "\n",
    "        with open(os.path.join(holdout_dir, f'elem_val_{i}.npy'), 'wb') as f:\n",
    "            np.save(f, val_set)\n",
    "\n",
    "        with open(os.path.join(holdout_dir, f'elem_test_{i}.npy'), 'wb') as f:\n",
    "            np.save(f, test_set)\n",
    "\n",
    "    # Chequear que la K-Cross-validation está sin overlap en test\n",
    "    for i, set1 in enumerate(test_sets):\n",
    "        for j, set2 in enumerate(test_sets):\n",
    "            if i != j:\n",
    "                assert len( np.intersect1d(set1, set2) ) == 0\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Augmentation\n",
    "La celda a continuación lleva a cabo la generación de datos sintéticos, solo sobre el dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5060\n",
      "330\n",
      "5066\n",
      "330\n",
      "5042\n",
      "330\n",
      "5034\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, K):\n",
    "    \n",
    "    new_entries = {}\n",
    "    train_set = list(train_sets[i])\n",
    "    val_set = val_sets[i]\n",
    "    test_set = test_sets[i]\n",
    "        \n",
    "    for pattern in train_set: # only training set\n",
    "        labels = df.loc[[pattern]]\n",
    "        lbl_class = classes.loc[[pattern]]['chapter'].values[0]\n",
    "\n",
    "        path_in = os.path.join(root_dir, \"patterns\", \"originals\", pattern, pattern + \".png\")\n",
    "        path_out = os.path.join(root_dir, \"patterns\", data_flags, str(i), \"train\", lbl_class)\n",
    "        is_hor = labels['horizontal'].values[0]\n",
    "        is_ver = labels['vertical'].values[0]\n",
    "\n",
    "        if is_hor and is_ver: \n",
    "            pass\n",
    "        \n",
    "        if is_hor and not is_ver:\n",
    "            new_names = apply_transformations(path_in, path_out, HOR_TRANSFORMATIONS)\n",
    "            labels = df.loc[[pattern]].values[0]\n",
    "\n",
    "        elif is_ver and not is_hor:\n",
    "            new_names = apply_transformations(path_in, path_out, VER_TRANSFORMATIONS)\n",
    "            labels = df.loc[[pattern]].values[0]\n",
    "\n",
    "        else: #if not is_hor and not is_ver:\n",
    "            new_names = apply_transformations(path_in, path_out, COMMON_TRANSFORMATIONS)\n",
    "            labels = df.loc[[pattern]].values[0]\n",
    "\n",
    "        for name in new_names:\n",
    "            new_entries[name] = labels\n",
    "\n",
    "        # add the base pattern to the folder\n",
    "        os.makedirs(path_out, exist_ok = True)\n",
    "        shutil.copy(path_in, path_out)\n",
    "\n",
    "    for pattern in val_set:\n",
    "        lbl_class = classes.loc[[pattern]]['chapter'].values[0]\n",
    "        path_in = os.path.join(root_dir, \"patterns\", \"originals\", pattern, pattern + \".png\")\n",
    "        path_out = os.path.join(root_dir, \"patterns\", data_flags, str(i), \"val\", lbl_class)\n",
    "        os.makedirs(path_out, exist_ok = True)\n",
    "        shutil.copy(path_in, path_out)\n",
    "\n",
    "    for pattern in test_set:\n",
    "        lbl_class = classes.loc[[pattern]]['chapter'].values[0]\n",
    "        path_in = os.path.join(root_dir, \"patterns\", \"originals\", pattern, pattern + \".png\")\n",
    "        path_out = os.path.join(root_dir, \"patterns\", data_flags, str(i), \"test\", lbl_class)\n",
    "        os.makedirs(path_out, exist_ok = True)\n",
    "        shutil.copy(path_in, path_out)\n",
    "\n",
    "    # agregar todas las entradas de train a new_entries, y crear \n",
    "    # el dataset \"augmented_train_df.json\"\n",
    "\n",
    "    for p in train_set:\n",
    "      labels = df.loc[p]\n",
    "      new_entries[p] = labels.values\n",
    "\n",
    "    labels_output = os.path.join(labels_dir, data_flags, str(i))\n",
    "\n",
    "    os.makedirs(labels_output, exist_ok = True)\n",
    "    \n",
    "    print(len(new_entries))\n",
    "    print(len(colnames))\n",
    "    \n",
    "    df_train = pd.DataFrame.from_dict(new_entries, columns=colnames, orient='index')\n",
    "    df_train.to_json(os.path.join(labels_output, \"augmented_train_df.json\"), orient='index')\n",
    "\n",
    "    # agregar todas las entradas de val a val_entries, y crear \n",
    "    # el dataset \"val_df.json\"\n",
    "    val_entries = {}\n",
    "    for p in val_set:\n",
    "      labels = df.loc[p]\n",
    "      val_entries[p] = labels.values\n",
    "\n",
    "    df_val = pd.DataFrame.from_dict(val_entries, columns=colnames, orient='index')\n",
    "    df_val.to_json(os.path.join(labels_output, \"val_df.json\"), orient='index')\n",
    "\n",
    "    # agregar todas las entradas de test a test_entries, y crear\n",
    "    # el dataset \"test_df.json\"\n",
    "    test_entries = {}\n",
    "    for p in test_set:\n",
    "      labels = df.loc[p]\n",
    "      test_entries[p] = labels.values\n",
    "\n",
    "    df_test = pd.DataFrame.from_dict(test_entries, columns=colnames, orient='index')\n",
    "    df_test.to_json(os.path.join(labels_output, \"test_df.json\"), orient='index')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "woMl7NKb3LyB",
    "IRTWM7ng3lwE",
    "lVaAM8Qw3Oo-",
    "boCmALkT3gro",
    "6IsXXq4cAlQp"
   ],
   "name": "Split and augmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
