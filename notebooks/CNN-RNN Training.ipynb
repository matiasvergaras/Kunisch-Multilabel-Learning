{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1589c7c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# CNN-RNN Multilabeling\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "\n",
    "El objetivo de este notebook es realizar predicciones multilabel sobre patrones geométricos mediante CNN-RNN.\n",
    "\n",
    "Código basado en la implementación del usuario velej en https://github.com/jennynanap/Yelp_Image_Classification/blob/master/ML%20Models/CNN_RNN.py, agregando importantes modificaciones (principalmente relacionadas al Beam Search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74042862",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac1fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e1f4e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import math \n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "%matplotlib inline\n",
    "import sys \n",
    "import cv2 \n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from utils import KunischMetrics\n",
    "from utils import KunischPruner\n",
    "from utils import DataExplorer\n",
    "from utils import KunischPlotter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71546f2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuración de dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b29774a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 0 es 3090, 1 y 2 son 2080\n",
    "CUDA_ID = 0\n",
    "\n",
    "device = torch.device(f'cuda:{CUDA_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46f8c2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Selección de dataset y experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab5994d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DS_FLAGS = []\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'gausblur': [gausblur]\n",
    "              # 'msblur': [msblur]\n",
    "              # 'mtnblur': [mtnblur]\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "CROP_TIMES = 1\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1\n",
    "GAUSBLUR_TIMES = 1\n",
    "\n",
    "use_pos_weights = True\n",
    "pos_weights_factor = 1\n",
    "NLABELS = [5, 14, 26, 34, 54, 63, 72, 82, 91, 107, 131, 169, 281]\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "# 0 es 3090, 1 y 2 son 2080\n",
    "CUDA_ID = 0\n",
    "\n",
    "SAVE = True\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27505ed7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Nombre del experimento: 14L_weighted_1\n",
      "--Pattern set encontrado en ..\\patterns\\base\\0\n",
      "--Labels set encontrado en ..\\labels\\base\\0\n",
      "\n",
      "Los resultados se guardarán en: ..\\outputs\\cnn-rnn\\base\\14L_weighted_1\\0\n",
      "Los modelos se guardarán en: ..\\models\\cnn-rnn\\base\\0\n",
      "Fold  1\n",
      "Nombre del experimento: 14L_weighted_1\n",
      "--Pattern set encontrado en ..\\patterns\\base\\1\n",
      "--Labels set encontrado en ..\\labels\\base\\1\n",
      "\n",
      "Los resultados se guardarán en: ..\\outputs\\cnn-rnn\\base\\14L_weighted_1\\1\n",
      "Los modelos se guardarán en: ..\\models\\cnn-rnn\\base\\1\n",
      "Fold  2\n",
      "Nombre del experimento: 14L_weighted_1\n",
      "--Pattern set encontrado en ..\\patterns\\base\\2\n",
      "--Labels set encontrado en ..\\labels\\base\\2\n",
      "\n",
      "Los resultados se guardarán en: ..\\outputs\\cnn-rnn\\base\\14L_weighted_1\\2\n",
      "Los modelos se guardarán en: ..\\models\\cnn-rnn\\base\\2\n",
      "Fold  3\n",
      "Nombre del experimento: 14L_weighted_1\n",
      "--Pattern set encontrado en ..\\patterns\\base\\3\n",
      "--Labels set encontrado en ..\\labels\\base\\3\n",
      "\n",
      "Los resultados se guardarán en: ..\\outputs\\cnn-rnn\\base\\14L_weighted_1\\3\n",
      "Los modelos se guardarán en: ..\\models\\cnn-rnn\\base\\3\n"
     ]
    }
   ],
   "source": [
    "# This cells builds the data_flags variable, that will be used\n",
    "# to map the requested data treatment to folders\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "         'gausblur': GAUSBLUR_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic', 'gausblur']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "Kfolds = {}\n",
    "\n",
    "for i in range(0, K):\n",
    "    print(\"Fold \", i)\n",
    "    patterns_dir = os.path.join(root_dir, 'patterns', data_flags, str(i))\n",
    "    labels_dir = os.path.join(root_dir, 'labels', data_flags, str(i))\n",
    "\n",
    "    if not (os.path.isdir(patterns_dir) and os.path.isdir(labels_dir)):\n",
    "        print(patterns_dir)\n",
    "        print(labels_dir)\n",
    "        raise FileNotFoundError(\"\"\"\n",
    "        No existen directorios de datos para el conjunto de flags seleccionado. \n",
    "        Verifique que el dataset exista y, de lo contrario, llame a Split and Augmentation.\n",
    "        \"\"\")\n",
    "        \n",
    "    exp_name = f\"{NUM_LABELS}L\"\n",
    "    weights_str = str(pos_weights_factor)\n",
    "    weights_str = weights_str.replace('.','_')\n",
    "    exp_name += f'_weighted_{weights_str}' if use_pos_weights else ''\n",
    "    print(f\"Nombre del experimento: {exp_name}\")\n",
    "     \n",
    "    output_dir = os.path.join(root_dir, \"outputs\", \"cnn-rnn\", data_flags, exp_name, str(i))\n",
    "    model_dir = os.path.join(root_dir, \"models\", \"cnn-rnn\", data_flags, str(i))\n",
    "    model_path = os.path.join(model_dir, exp_name)\n",
    "    \n",
    "    path_co = os.path.join(patterns_dir, 'train', 'circular ornaments')\n",
    "    path_lz = os.path.join(patterns_dir, 'train', 'lozenge')\n",
    "    path_pc = os.path.join(patterns_dir, 'train', 'pictographics')\n",
    "    path_ro = os.path.join(patterns_dir, 'train', 'rectangular ornaments')\n",
    "    path_sl = os.path.join(patterns_dir, 'train', 'strokes and lines')\n",
    "    path_to = os.path.join(patterns_dir, 'train', 'triangular ornaments')\n",
    "    \n",
    "    ex_co = os.listdir(path_co)[random.randint(0, 5)].split('.')[0]\n",
    "    ex_co = os.path.join(path_co, ex_co)\n",
    "\n",
    "    ex_lz = os.listdir(path_lz)[random.randint(0, 5)].split('.')[0]\n",
    "    ex_lz = os.path.join(path_lz, ex_lz)\n",
    "\n",
    "    ex_pc = os.listdir(path_pc)[random.randint(0, 5)].split('.')[0]\n",
    "    ex_pc = os.path.join(path_pc, ex_pc)\n",
    "    \n",
    "    ex_ro = os.listdir(path_ro)[random.randint(0, 5)].split('.')[0]\n",
    "    ex_ro = os.path.join(path_ro, ex_ro)\n",
    "\n",
    "    ex_sl = os.listdir(path_sl)[random.randint(0, 5)].split('.')[0]\n",
    "    ex_sl = os.path.join(path_sl, ex_sl)\n",
    "\n",
    "    ex_to = os.listdir(path_to)[random.randint(0, 5)].split('.')[0]\n",
    "    ex_to = os.path.join(path_to, ex_to)\n",
    "    \n",
    "    Kfolds[i] = {\n",
    "        'patterns_dir': patterns_dir,\n",
    "        'labels_dir': labels_dir,\n",
    "        'output_dir': output_dir,\n",
    "        'model_path': model_path,\n",
    "        'ex_photos': [ex_co, ex_lz, ex_pc, ex_ro, ex_sl, ex_to]\n",
    "    }\n",
    "    \n",
    "    print(\"--Pattern set encontrado en {}\".format(patterns_dir))\n",
    "    print(\"--Labels set encontrado en {}\".format(labels_dir))\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "    if SAVE:\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        os.makedirs(model_dir, exist_ok = True)\n",
    "        print(f\"Los resultados se guardarán en: {output_dir}\")\n",
    "        print(f\"Los modelos se guardarán en: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ed397",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "395ff724",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Construct Data Loader\n",
    "class KunischDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, images_dir, labels_file, transform, top_labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file(string): path to text file\n",
    "            root_dir(string): directory with all train images\n",
    "        \"\"\"\n",
    "        self.pruner = KunischPruner(len(top_labels))\n",
    "        self.pruner.set_top_labels(top_labels)\n",
    "        labels = pd.read_json(labels_file, orient='index')\n",
    "        self.labels_frame = self.pruner.filter_df(labels)\n",
    "        self.num_labels = len(top_labels)\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_file = labels_file\n",
    "        self.transform = transform\n",
    "        self.flags = data_flags\n",
    "        self.top_labels = top_labels\n",
    "                                       \n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.labels_frame.iloc[idx].name + '.png'\n",
    "        img_name = None\n",
    "        for chapter in os.listdir(self.images_dir):\n",
    "          if img_id in os.listdir(os.path.join(self.images_dir, chapter)):\n",
    "            img_name = os.path.join(self.images_dir, chapter, img_id)\n",
    "            break\n",
    "        if img_name is None:\n",
    "          raise Exception(f'No se encontró la imagen para {img_id}')\n",
    "        image = Image.open(img_name)\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        labels = self.labels_frame.iloc[idx].values\n",
    "        labels = np.where(labels)[0].tolist()\n",
    "        labels = list(map(int, labels))\n",
    "        labels = [self.num_labels] + labels + [self.num_labels + 1] #start and end\n",
    "        # [26 ... 0 ... ... 27]\n",
    "        labels = [x + 1 for x in labels] # add 1 to all labels so 0 has no meaning\n",
    "        # [27 ... 1 ... ... 28]\n",
    "        length = len(labels) \n",
    "        for i in range(self.num_labels + 2 - length): #Pad the labels (There are num_labels unique labels)\n",
    "            labels = labels + [0]\n",
    "        # [27 ... 1 ... ... 28 0 0 0 ... 0]\n",
    "        target = torch.Tensor(labels).long()\n",
    "        \n",
    "        sample = {'image': image, 'labels': target, 'lengths': length,\n",
    "                  'paths': img_name}\n",
    "        \n",
    "        return sample\n",
    " \n",
    "\n",
    "# Define Beam Search   \n",
    "def beam_search(k, s, predicted, x, y, pred_sequence_list, prob_sequence_list):\n",
    "  #Inputs Definitions:\n",
    "  #k: Top labels to consider\n",
    "  #s: current state\n",
    "  #predicted: result of lstm (softmax)\n",
    "  #x: current path of labels\n",
    "  #y: current paht of probabilities \n",
    "  #prediction_paths: list of all label paths\n",
    "  #probability_paths: list of all probability paths\n",
    "    \n",
    "    #si se predice end, terminar\n",
    "    if predicted == NUM_LABELS + 2:\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "        pred_sequence_list.append(x)\n",
    "        prob_sequence_list.append(y)\n",
    "        #print(\"PREDICHO END\")\n",
    "    # caso contrario: calculo un nuevo estado\n",
    "    else:\n",
    "        inputs = decoder.embedding(predicted) \n",
    "        outputs, s = decoder.execute_lstm(inputs, s)\n",
    "        scores = torch.softmax(outputs[0],dim=0)\n",
    "        top_k_scores = scores.topk(k)[1].unsqueeze(0)\n",
    "        top_k_probs = scores.topk(k)[0].unsqueeze(0)\n",
    "        #print(top_k_scores)\n",
    "        #print(top_k_probs)\n",
    "\n",
    "        sequences = x.expand(k,len(x))\n",
    "        prob_sequences = y.expand(k,len(x))\n",
    "        #print(sequences)\n",
    "        #print(top_k_scores[0][0].unsqueeze(0))\n",
    "        #step =1\n",
    "        \n",
    "        # para cada top k, itero (bajo un nivel del arbol)\n",
    "        for i in range(top_k_scores.size(1)):\n",
    "            x = torch.cat((sequences[i], top_k_scores[0][i].unsqueeze(0) ))\n",
    "            y = torch.cat((prob_sequences[i], top_k_probs[0][i].unsqueeze(0) ))\n",
    "            \n",
    "            #el nuevo predicho es el ultimo elemento de x \n",
    "            predicted = x[len(x)-1].unsqueeze(0)\n",
    "            \n",
    "             #si todavia no predigo ningun label aparte de start y end o si hay uno nuevo, itero\n",
    "            if (x[len(x)-2]==NUM_LABELS + 1 and len(pred_sequence_list)<2) or predicted not in x[:-1]:\n",
    "                #print('This is predicted: ', x[:-1])\n",
    "                #print('This is x: ',x)\n",
    "                # itero de nuevo pero con este nuevo estado, este nuevo predicted,\n",
    "                # x e y, ..\n",
    "                beam_search(k,s,predicted,x,y,pred_sequence_list,prob_sequence_list)\n",
    "            \n",
    "\n",
    "#Generate Sample Images with Captions\n",
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    image = image.resize([227, 227], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "#Define the CNN RNN architecture\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the Alexnet Architecture modifying where appropriate\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        alex_net = models.alexnet(pretrained=True)\n",
    "        alex_net.classifier._modules['6'] = nn.Linear(4096, embed_size)\n",
    "\n",
    "        \n",
    "        self.alex_net = alex_net\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        \n",
    "        features = self.alex_net(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        #print(np.shape(features))\n",
    "        #print(features.size())\n",
    "        features = self.bn(features)\n",
    "        return features\n",
    "\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def execute_lstm(self, features, states=None):\n",
    "        \"\"\"Generate labels for given image \"\"\"\n",
    "\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(1): # range(1):#:\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "\n",
    "        return outputs, states\n",
    "    \n",
    "    def embedding(self, features, states=None):\n",
    "        \"\"\"Embedd predicted label\"\"\"\n",
    "        for i in range(1):#range(1):#self.max_seg_length):\n",
    "            inputs = self.embed(features)                       # inputs: (batch_size, embed_size)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee08d7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eb58504",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\outputs\\cnn-rnn\\base\\14L_weighted_1\\0\n",
      "Usando top_labels previamente generados para 5 labels\n",
      "5\n",
      "Vocabulary size:  8\n",
      "Epoch [0/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 16.103797791496156\n",
      "Epoch [0/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 23.543782552083332\n",
      "Epoch [1/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 11.900239974733383\n",
      "Epoch [1/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 15.871502216045673\n",
      "Epoch [2/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 9.441960168263268\n",
      "Epoch [2/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 14.176313351362179\n",
      "Epoch [3/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 8.715761457170759\n",
      "Epoch [3/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 13.367071689703526\n",
      "Epoch [4/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 8.214460100446429\n",
      "Epoch [4/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 12.596148368639824\n",
      "Epoch [5/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 7.774062868148562\n",
      "Epoch [5/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 11.957981989933895\n",
      "Epoch [6/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 7.308755178300161\n",
      "Epoch [6/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 11.252816224709536\n",
      "Epoch [7/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 6.811162797231523\n",
      "Epoch [7/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 10.459816174629408\n",
      "Epoch [8/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 6.306728060283358\n",
      "Epoch [8/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 9.636356451572516\n",
      "Epoch [9/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 5.7318517291356645\n",
      "Epoch [9/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 8.7496580466246\n",
      "Epoch [10/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 5.140096028645833\n",
      "Epoch [10/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 7.743470020783254\n",
      "Epoch [11/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 4.52437494671534\n",
      "Epoch [11/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 6.791244115584936\n",
      "Epoch [12/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 3.9089059375581288\n",
      "Epoch [12/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 5.807241586538462\n",
      "Epoch [13/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 3.3213636610243054\n",
      "Epoch [13/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 4.891859396910056\n",
      "Epoch [14/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 2.7897045196048795\n",
      "Epoch [14/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 4.114926460461739\n",
      "Epoch [15/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 2.2987432328481523\n",
      "Epoch [15/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 3.3821246807391825\n",
      "Epoch [16/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 1.905041285923549\n",
      "Epoch [16/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 2.723350133651342\n",
      "Epoch [17/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 1.5533142695351252\n",
      "Epoch [17/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 2.276873075045072\n",
      "Epoch [18/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 1.2539628346761067\n",
      "Epoch [18/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 1.825468992575621\n",
      "Epoch [19/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 1.063241686139788\n",
      "Epoch [19/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 1.5491959009415064\n",
      "Epoch [20/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 0.8739755267188662\n",
      "Epoch [20/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 1.2908392686110277\n",
      "Epoch [21/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 0.7598114921933129\n",
      "Epoch [21/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 1.0875753745054588\n",
      "Epoch [22/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 0.6463752019973028\n",
      "Epoch [22/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 0.9510360130896935\n",
      "Epoch [23/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 0.5889223946465386\n",
      "Epoch [23/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 0.8258997599283854\n",
      "Epoch [24/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 0.5040934729197669\n",
      "Epoch [24/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 0.762068626208183\n",
      "Epoch [25/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 0.45070448375883554\n",
      "Epoch [25/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 0.6910464947040265\n",
      "Epoch [26/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 0.4047473877195328\n",
      "Epoch [26/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 0.6406700427715595\n",
      "Epoch [27/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 0.3839295705159505\n",
      "Epoch [27/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 0.6114410987267127\n",
      "Epoch [28/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Avg Loss: 0.3552102815537226\n",
      "Epoch [28/300], Embedded Size: 2, Hidden Size: 256, Layers: 1, LR: 0.001, W: 2.2e-05, Val Loss: 0.563204349615635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    169\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    170\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 171\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(kunischTrainLoader\u001b[38;5;241m.\u001b[39mdataset)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1024\u001b[39m\n\u001b[0;32m    174\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend([epoch, train_loss])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for NUM_LABELS in NLABELS:\n",
    "    recall = 0\n",
    "    prec = 0\n",
    "    acc = 0\n",
    "    f1 = 0\n",
    "    f2 = 0\n",
    "    hs = 0\n",
    "    emr = 0\n",
    "    mr1 = 0\n",
    "    mr2 = 0\n",
    "    mr3 = 0\n",
    "    mr4 = 0\n",
    "    mr5 = 0\n",
    "    for fold_i in range(0, K):\n",
    "        fold = Kfolds[fold_i]\n",
    "        labels_dir = fold['labels_dir']\n",
    "        patterns_dir = fold['patterns_dir']\n",
    "        output_dir = fold['output_dir']\n",
    "        print(output_dir)\n",
    "        model_path = fold['model_path']\n",
    "        photos = fold['ex_photos']\n",
    "        # Carga de top labels\n",
    "        train_labels = pd.read_json(os.path.join(labels_dir, 'augmented_train_df.json'), orient='index')\n",
    "\n",
    "        if not os.path.isfile(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle')):\n",
    "            print(f\"Creando top_labels para {NUM_LABELS} labels\")\n",
    "            top_labels = pruner.filter_labels(train_labels)\n",
    "            pruner.set_top_labels(top_labels)\n",
    "\n",
    "            save = input(f\"Se creará un archivo nuevo para {len(top_labels)} labels. Desea continuar? (y/n)\")\n",
    "            if save == \"y\":\n",
    "                with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'wb') as f:\n",
    "                    pickle.dump(top_labels, f)\n",
    "                print(\"Top labels creado con éxito\")\n",
    "\n",
    "            else:\n",
    "                raise Exception(\"No se logró cargar top_labels\")\n",
    "\n",
    "        else: \n",
    "            print(f\"Usando top_labels previamente generados para {NUM_LABELS} labels\")\n",
    "            with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'rb') as f:\n",
    "                top_labels = pickle.load(f)\n",
    "\n",
    "        kunischTrainSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'train'),\n",
    "                                         labels_file=os.path.join(labels_dir, 'augmented_train_df.json'),\n",
    "                                         transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                       transforms.ToTensor(),\n",
    "                                                                       transforms.Normalize(\n",
    "                                                                           mean=[0.485, 0.456, 0.406],\n",
    "                                                                           std=[0.229, 0.224, 0.225])]),\n",
    "                                         top_labels=top_labels)\n",
    "\n",
    "        kunischTrainLoader = torch.utils.data.DataLoader(kunischTrainSet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "        # Validation\n",
    "        kunischValidationSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'val'),\n",
    "                                              labels_file=os.path.join(labels_dir, 'val_df.json'),\n",
    "                                              transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                            transforms.ToTensor(),\n",
    "                                                                            transforms.Normalize(\n",
    "                                                                                mean=[0.485, 0.456, 0.406],\n",
    "                                                                                std=[0.229, 0.224, 0.225])]),\n",
    "                                              top_labels=top_labels)\n",
    "\n",
    "        kunischValidationLoader = torch.utils.data.DataLoader(kunischValidationSet, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                                              num_workers=0)\n",
    "\n",
    "        # Test\n",
    "        kunischTestSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'test'),\n",
    "                                        labels_file=os.path.join(labels_dir, 'test_df.json'),\n",
    "                                        transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                      transforms.ToTensor(),\n",
    "                                                                      transforms.Normalize(\n",
    "                                                                          mean=[0.485, 0.456, 0.406],\n",
    "                                                                          std=[0.229, 0.224, 0.225])]),\n",
    "                                        top_labels=top_labels)\n",
    "\n",
    "        kunischTestLoader = torch.utils.data.DataLoader(kunischTestSet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "        print(len(top_labels))\n",
    "\n",
    "        #### \n",
    "        # Training the Model\n",
    "        os.makedirs(model_path, exist_ok = True)\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        encoder_path = os.path.join(model_path, 'encoder.pth')\n",
    "        decoder_path = os.path.join(model_path, 'decoder.pth')\n",
    "\n",
    "\n",
    "        vocab = list(top_labels.index.values)\n",
    "        vocab = ['<padding>'] + vocab + ['<start>', '<end>']\n",
    "        print(\"Vocabulary size: \", len(vocab))\n",
    "\n",
    "        embed_size = NUM_LABELS//2\n",
    "        hidden_size = 256\n",
    "        num_layers = 1\n",
    "        # Build the models\n",
    "        encoder = EncoderCNN(embed_size).to(device)\n",
    "        decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "        learning_rate = 0.001\n",
    "        w = 2.20e-05\n",
    "\n",
    "        def get_class_weight(labels):\n",
    "            samples = len(labels)\n",
    "            sums = labels.sum(axis=0).values.tolist()\n",
    "            sums = [0] + sums + [0] + [0]\n",
    "            weights = []\n",
    "            for s in sums:\n",
    "                #print(\"Suma:\" ,s)\n",
    "                #print(\"Samples:\" ,samples)\n",
    "                weight = samples/(s+1)\n",
    "                #print(\"Weight:\", weight)\n",
    "                weights.append(weight)\n",
    "            return weights\n",
    "\n",
    "        class_weights = get_class_weight(kunischTrainSet.labels_frame)\n",
    "        # Loss and optimizer\n",
    "        class_weights=torch.tensor(class_weights,dtype=torch.float).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        params = list(decoder.parameters()) + list(encoder.alex_net.parameters()) + list(encoder.bn.parameters())\n",
    "        optimizer = torch.optim.RMSprop(params, lr=learning_rate, alpha=0.9, eps=1e-08, weight_decay=w)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=5, min_lr=0.0001)\n",
    "\n",
    "\n",
    "        num_epochs = 300\n",
    "        best_loss = math.inf\n",
    "        patience = 15\n",
    "        bad_epochs = 0\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0\n",
    "            val_loss = 0\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            for i, sample_batched in enumerate(kunischTrainLoader):\n",
    "\n",
    "                # Set mini-batch dataset\n",
    "                unsorted_images = sample_batched['image']\n",
    "                unsorted_labels = sample_batched['labels']\n",
    "                unsorted_lengths = sample_batched['lengths']\n",
    "                sorted_length_index = sorted(range(len(unsorted_lengths)),key=unsorted_lengths.__getitem__,reverse=True)\n",
    "                inputs =[]\n",
    "                labels = []\n",
    "                lengths = []\n",
    "                for j in sorted_length_index:\n",
    "                    inputs.append(unsorted_images[j])\n",
    "                    labels.append(unsorted_labels[j])\n",
    "                    lengths.append(unsorted_lengths[j])\n",
    "                #inputs = unsorted_images\n",
    "                #labels = unsorted_labels\n",
    "                #lengths = unsorted_lengths\n",
    "                inputs =torch.stack(inputs).to(device)\n",
    "                labels = torch.stack(labels).to(device)\n",
    "                lengths = torch.stack(lengths)\n",
    "                targets = pack_padded_sequence(labels , lengths, batch_first=True)[0].to(device)\n",
    "\n",
    "                decoder.zero_grad()\n",
    "                encoder.zero_grad()\n",
    "\n",
    "                #print(labels)\n",
    "                #print(targets)\n",
    "                # Forward, backward and optimize\n",
    "                features = encoder(inputs)\n",
    "                outputs = decoder(features, labels , lengths)\n",
    "                #print(np.shape(outputs))\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss = train_loss/len(kunischTrainLoader.dataset)*1024\n",
    "            train_losses.append([epoch, train_loss])\n",
    "\n",
    "            print('Epoch [{}/{}], Embedded Size: {}, Hidden Size: {}, Layers: {}, LR: {}, W: {}, Avg Loss: {}'\n",
    "                  .format(epoch, num_epochs, embed_size, hidden_size,\n",
    "                          num_layers, learning_rate, w, train_loss ))\n",
    "\n",
    "\n",
    "            for i, sample_batched in enumerate(kunischValidationLoader):\n",
    "\n",
    "                # Set mini-batch dataset\n",
    "                unsorted_images = sample_batched['image']\n",
    "                unsorted_labels = sample_batched['labels']\n",
    "                unsorted_lengths = sample_batched['lengths']\n",
    "                sorted_length_index = sorted(range(len(unsorted_lengths)),key=unsorted_lengths.__getitem__,reverse=True)\n",
    "                inputs =[]\n",
    "                labels = []\n",
    "                lengths = []\n",
    "                for j in sorted_length_index:\n",
    "                    inputs.append(unsorted_images[j])\n",
    "                    labels.append(unsorted_labels[j])\n",
    "                    lengths.append(unsorted_lengths[j])\n",
    "                #inputs = unsorted_images\n",
    "                #labels = unsorted_labels\n",
    "                #lengths = unsorted_lengths\n",
    "                inputs =torch.stack(inputs).to(device)\n",
    "                labels = torch.stack(labels).to(device)\n",
    "                lengths = torch.stack(lengths)\n",
    "                targets = pack_padded_sequence(labels , lengths, batch_first=True)[0].to(device)\n",
    "\n",
    "                decoder.zero_grad()\n",
    "                encoder.zero_grad()\n",
    "\n",
    "                #print(labels)\n",
    "                #print(targets)\n",
    "                # Forward, backward and optimize\n",
    "                features = encoder(inputs)\n",
    "                outputs = decoder(features, labels , lengths)\n",
    "                #print(np.shape(outputs))\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            val_loss = val_loss/len(kunischValidationLoader.dataset)*1024\n",
    "            scheduler.step(val_loss)\n",
    "            val_losses.append([epoch, val_loss])\n",
    "\n",
    "            print('Epoch [{}/{}], Embedded Size: {}, Hidden Size: {}, Layers: {}, LR: {}, W: {}, Val Loss: {}'\n",
    "                  .format(epoch, num_epochs, embed_size, hidden_size,\n",
    "                          num_layers, learning_rate, w, val_loss ))\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                bad_epochs = 0\n",
    "                torch.save(encoder.state_dict(), encoder_path)\n",
    "                torch.save(decoder.state_dict(), decoder_path)\n",
    "            else:\n",
    "                print(\"Mala epoca\")\n",
    "                bad_epochs += 1\n",
    "                if bad_epochs == patience:\n",
    "                    print(\"Se acabo la paciencia\\n\")\n",
    "                    break\n",
    "\n",
    "        print(\"Cargando modelos\")\n",
    "\n",
    "        #Load Parameters\n",
    "        encoder.load_state_dict(torch.load(encoder_path))\n",
    "        decoder.load_state_dict(torch.load(decoder_path))\n",
    "\n",
    "        print(\"Modelos cargados\")\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                     (0.229, 0.224, 0.225))])\n",
    "        show_examples = True\n",
    "        if show_examples:\n",
    "            for i in photos:\n",
    "                print(\"Working photo \", i)\n",
    "                img_name = i  +'.png'\n",
    "\n",
    "                image = load_image(img_name, transform).to(device)\n",
    "                encoder.eval()\n",
    "                decoder.eval()\n",
    "                prediction_paths = []\n",
    "                probability_paths = []\n",
    "                # Encode - read the image features\n",
    "                encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "                output_sample, s = decoder.execute_lstm(encoder_out)\n",
    "                _, predicted = output_sample.max(1)  \n",
    "\n",
    "                sequences = predicted\n",
    "                prob_sequences = torch.sigmoid(output_sample.max()).detach()\n",
    "\n",
    "                #print(\"Calling to beam search\")\n",
    "                beam_search(2, s, predicted, sequences, prob_sequences, prediction_paths, probability_paths)\n",
    "                index_val = 0\n",
    "                #print(\"After beam search\")\n",
    "\n",
    "                for j in range(0, len(prediction_paths)):\n",
    "                    score = np.sum(np.log(probability_paths[j].detach().cpu().numpy()))\n",
    "                    if j == 0:\n",
    "                        best = score\n",
    "                    elif best < score:\n",
    "                        best = score\n",
    "                        index_val = j\n",
    "\n",
    "                sampled_ids = prediction_paths[index_val].cpu().numpy()\n",
    "                sampled_prob = best\n",
    "                sampled_caption = []\n",
    "\n",
    "                for word_id in sampled_ids:\n",
    "                    word = vocab[word_id]\n",
    "                    sampled_caption.append(word)\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                sentence = ' '.join(sampled_caption)\n",
    "\n",
    "                # Print out the image and the generated caption\n",
    "\n",
    "                image = cv2.imread(img_name)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "                print (sentence)\n",
    "                print('Labels: {} \\nPrediction Path Probability: {:.4f}'.format( sampled_ids,sampled_prob))\n",
    "                print()\n",
    "\n",
    "            ####\n",
    "\n",
    "        #Run Models on Test data using Beam Search\n",
    "\n",
    "        vocab = list(top_labels.index.values)\n",
    "        vocab = ['<padding>'] + vocab + ['<start>', '<end>']\n",
    "        print(vocab)\n",
    "\n",
    "        TP, TN, FP, FN = 0, 0, 0, 0\n",
    "        test_pred = []\n",
    "\n",
    "        images = []\n",
    "        im_labels = []\n",
    "        for sample in kunischTestLoader:\n",
    "            images = images + sample['paths']\n",
    "            lbl = sample['labels'].cpu().detach().numpy()\n",
    "            for i in lbl:\n",
    "                im_labels.append(i)\n",
    "\n",
    "        for i, j  in zip(images, im_labels):\n",
    "            img_name = i\n",
    "            labels = j\n",
    "            labels = list(map(int, labels))\n",
    "            try:\n",
    "                labels.remove(NUM_LABELS + 1)\n",
    "                labels.remove(NUM_LABELS + 2)\n",
    "            except:\n",
    "                print(\"Etiquetas no incorporan start, end\")\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                     (0.229, 0.224, 0.225))])\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "\n",
    "            # Prepare an image\n",
    "            image = load_image(img_name, transform)\n",
    "            image_tensor = image.to(device)\n",
    "\n",
    "            # Generate an caption from the image\n",
    "            prediction_paths = []\n",
    "            probability_paths = []\n",
    "            # Encode - read the image features\n",
    "            encoder_out = encoder(image_tensor)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "            output_sample, s = decoder.execute_lstm(encoder_out)\n",
    "            _, predicted = output_sample.max(1)  \n",
    "\n",
    "            predicted = torch.tensor([NUM_LABELS + 1]).to(device)\n",
    "            sequences = predicted\n",
    "            prob_sequences = torch.sigmoid(output_sample.max())\n",
    "            prob_sequences\n",
    "\n",
    "            beam_search(2, s, predicted, sequences, prob_sequences,prediction_paths, probability_paths)\n",
    "            for k in range(len(prediction_paths)):\n",
    "                score_k = np.sum(np.log(probability_paths[k].detach().cpu().numpy()))\n",
    "                if k==0:\n",
    "                    best = score_k\n",
    "                    index_val = k \n",
    "                elif best < score_k:\n",
    "                    best = score_k\n",
    "                    index_val = k\n",
    "\n",
    "            sampled_ids = prediction_paths[index_val].cpu().numpy()\n",
    "\n",
    "            pred = sampled_ids\n",
    "\n",
    "\n",
    "            end = np.argwhere(pred==NUM_LABELS + 2)[0][0]\n",
    "            start_where = np.argwhere(pred==NUM_LABELS + 1)\n",
    "            start = start_where[len(start_where)-1][0]+1\n",
    "            pred = pred[start:end]\n",
    "\n",
    "            pred2 = np.zeros(NUM_LABELS)\n",
    "            labels2 = np.zeros(NUM_LABELS)\n",
    "\n",
    "            for p in pred:\n",
    "                pred2[p-1]=1\n",
    "            for l in labels:\n",
    "                labels2[l-1]=1   \n",
    "            test_pred.append([img_name,labels2,pred2])\n",
    "            TP += ((pred2==1)&(labels2==1)).sum()\n",
    "            TN += ((pred2==0)&(labels2==0)).sum()\n",
    "            FP += ((pred2==1)&(labels2==0)).sum()\n",
    "            FN += ((pred2==0)&(labels2==1)).sum()\n",
    "\n",
    "        ####\n",
    "        final_labels = []\n",
    "        final_preds = []\n",
    "        for elem in test_pred:\n",
    "            final_preds.append(elem[2])\n",
    "            final_labels.append(elem[1])\n",
    "\n",
    "        metrics = KunischMetrics(np.array(final_labels), np.array(final_preds))\n",
    "        recall += metrics.recall()\n",
    "        prec += metrics.precision()\n",
    "        acc += metrics.acc()\n",
    "        f1 += metrics.f1()\n",
    "        hs += metrics.hs()\n",
    "        emr += metrics.emr()\n",
    "        mr1 += metrics.mr1()\n",
    "        mr2 += metrics.mr2()\n",
    "        mr3 += metrics.mr3()\n",
    "        mr4 += metrics.mr4()\n",
    "        mr5 += metrics.mr5()\n",
    "        f2 += metrics.f2()\n",
    "\n",
    "        if SAVE:\n",
    "            df = pd.DataFrame(np.array(final_preds))\n",
    "            df.to_csv(os.path.join(output_dir, 'predictions.csv'))\n",
    "\n",
    "    avg_recall = round(recall/K, 4)\n",
    "    avg_prec = round(prec/K, 4)\n",
    "    avg_acc = round(acc/K, 4)\n",
    "    avg_f1 = round(f1/K, 4)\n",
    "    avg_f2 = round(f2/K, 4)\n",
    "    avg_hs = round(hs/K, 4)\n",
    "    avg_emr = round(emr/K, 4)\n",
    "    avg_mr1 = round(mr1/K, 4)\n",
    "    avg_mr2 = round(mr2/K, 4)\n",
    "    avg_mr3 = round(mr3/K, 4)\n",
    "    avg_mr4 = round(mr4/K, 4)\n",
    "    avg_mr5 = round(mr5/K, 4)\n",
    "\n",
    "    print(\"Recall\", avg_recall)\n",
    "    print(\"Precision\", avg_prec)\n",
    "    print(\"Accuracy\", avg_acc)\n",
    "    print(\"F1\", avg_f1)\n",
    "    print(\"F2\", avg_f2)\n",
    "    print(\"HS\", avg_hs)\n",
    "    print(\"EMR\", avg_emr)\n",
    "    print(\"1MR\", avg_mr1)\n",
    "    print(\"2MR\", avg_mr2)\n",
    "\n",
    "    if SAVE:\n",
    "        results_dict = {\n",
    "            'recall': avg_recall,\n",
    "            'precision': avg_prec,\n",
    "            'acc': avg_acc,\n",
    "            'f1': avg_f1,\n",
    "            'f2': avg_f2,\n",
    "            'hs': avg_hs,\n",
    "            'emr': avg_emr,\n",
    "            'mr1': avg_mr1,\n",
    "            'mr2': avg_mr2,\n",
    "            'mr3': avg_mr3,\n",
    "            'mr4': avg_mr4,\n",
    "            'mr5': avg_mr5\n",
    "        }\n",
    "\n",
    "        final_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "        final_df.to_csv(os.path.join(output_dir, '..', 'metadata.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bd636",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "outputs": [],
   "source": [
    "## Hyper Parameter Tuning\n",
    "vocab = list(top_labels.index.values)\n",
    "vocab = ['<padding>'] + vocab + ['<start>', '<end>']\n",
    "print(vocab)\n",
    "\n",
    "num_epochs = 64\n",
    "total_step = len(kunischTrainLoader)\n",
    "log_step = 10000\n",
    "train_losses = []\n",
    "validation_losses =[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "     \n",
    "    embed_size = random.choice([512,1024,2048])\n",
    "    hidden_size = random.choice([256,512,1024,2048])\n",
    "    num_layers = random.choice([1,2])\n",
    "    \n",
    "    # Build the models\n",
    "    encoder = EncoderCNN(embed_size).to(device)\n",
    "    decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "    learning_rate = round(np.exp(random.uniform(np.log(.0001), np.log(.01))),4) #pull geometrically\n",
    "    w = round(np.exp(random.uniform(np.log(3.1e-7), np.log(3.1e-5))),10) #pull geometrically\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    params = list(decoder.parameters()) + list(encoder.alex_net.parameters()) + list(encoder.bn.parameters())\n",
    "    #optimizer = torch.optim.Adam(params, lr=learning_rate, weight_decay = w)\n",
    "    optimizer = torch.optim.RMSprop(params, lr=learning_rate, alpha=0.99, eps=1e-08, weight_decay=w)\n",
    "    train_loss = 0\n",
    "    validation_loss = 0\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for i, sample_batched in enumerate(kunischTrainLoader):\n",
    "        \n",
    "        # Set mini-batch dataset\n",
    "        unsorted_images = sample_batched['image']\n",
    "        unsorted_labels = sample_batched['labels']\n",
    "        unsorted_lengths = sample_batched['lengths']\n",
    "        sorted_length_index = sorted(range(len(unsorted_lengths)),key=unsorted_lengths.__getitem__,reverse=True)\n",
    "        inputs =[]\n",
    "        labels = []\n",
    "        lengths = []\n",
    "        for j in sorted_length_index:\n",
    "            inputs.append(unsorted_images[j])\n",
    "            labels.append(unsorted_labels[j])\n",
    "            lengths.append(unsorted_lengths[j])\n",
    "            \n",
    "        inputs =torch.stack(inputs).to(device)\n",
    "        labels = torch.stack(labels).to(device)\n",
    "        lengths = torch.stack(lengths)\n",
    "        targets = pack_padded_sequence(labels, lengths, batch_first=True)[0].to(device)\n",
    "        \n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        #print(labels)\n",
    "        #print(targets)\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(inputs)\n",
    "        outputs = decoder(features, labels, lengths)\n",
    "        #print(outputs)\n",
    "        #print(np.shape(outputs))\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # Print log info\n",
    "        #if i % log_step == 0:\n",
    "        #    print('Train Trial [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "        #          .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
    "        \n",
    "    \n",
    "    train_loss = train_loss/len(kunischTrainLoader.dataset)*1024 #1024 is the batch size\n",
    "    train_losses.append([epoch,  embed_size, hidden_size, num_layers, learning_rate, w, train_loss ])\n",
    "    \n",
    "    print('Train Trial [{}/{}], Embedded Size: {}, Hidden Size: {}, Layers: {}, LR: {}, W: {}, Avg Loss: {}'\n",
    "          .format(epoch, num_epochs, embed_size, hidden_size,\n",
    "                  num_layers, learning_rate, w, train_loss ))\n",
    "\n",
    "\n",
    "    train_losses_df = pd.DataFrame(train_losses)\n",
    "    train_losses_df.to_csv('hypertrain_cnn_rnn_losses.csv')\n",
    "    \n",
    "    #Begin Validation\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "      for i, sample_batched in enumerate(kunischValidationLoader):\n",
    "          # Set mini-batch dataset\n",
    "          unsorted_images = sample_batched['image']\n",
    "          unsorted_labels = sample_batched['labels']\n",
    "          unsorted_lengths = sample_batched['lengths']\n",
    "          sorted_length_index = sorted(range(len(unsorted_lengths)),\n",
    "                                       key=unsorted_lengths.__getitem__,reverse=True)\n",
    "          inputs =[]\n",
    "          labels = []\n",
    "          lengths = []\n",
    "          for j in sorted_length_index:\n",
    "              inputs.append(unsorted_images[j])\n",
    "              labels.append(unsorted_labels[j])\n",
    "              lengths.append(unsorted_lengths[j])\n",
    "\n",
    "          inputs =torch.stack(inputs).to(device)\n",
    "          labels = torch.stack(labels).to(device)\n",
    "          lengths = torch.stack(lengths)\n",
    "          targets = pack_padded_sequence(labels , lengths, batch_first=True)[0].to(device)\n",
    "          #print(labels)\n",
    "          #print(targets)\n",
    "          # Forward, backward and optimize\n",
    "          features = encoder(inputs)\n",
    "          outputs = decoder(features, labels , lengths)\n",
    "          #print(outputs)\n",
    "          #print(np.shape(outputs))\n",
    "          loss = criterion(outputs, targets)\n",
    "          validation_loss += loss.item()\n",
    "          \n",
    "    validation_loss = validation_loss/len(kunischValidationLoader.dataset)*1024\n",
    "    validation_losses.append([epoch,  embed_size, hidden_size, num_layers, learning_rate, w,  validation_loss ])\n",
    "    \n",
    "    print('Valid Trial [{}/{}], Embedded Size: {}, Hidden Size: {}, Layers: {}, LR: {}, W: {}, Avg Loss: {}'\n",
    "          .format(epoch, num_epochs, embed_size, hidden_size,\n",
    "                  num_layers, learning_rate, w, validation_loss ))\n",
    "    validation_losses_df = pd.DataFrame(validation_losses)\n",
    "    validation_losses_df.to_csv('hypervalid_cnn_rnn_losses.csv')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
