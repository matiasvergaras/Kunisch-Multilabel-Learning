{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# C2AE Training\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "\n",
    "El objetivo de este notebook es realizar el etiquetado múltiple de patrones geométricos mediante C2AE.\n",
    "El código se basa en la implementación del usuario ssmele en github: https://github.com/ssmele/C2AEinTorch.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from C2AE import C2AE, save_model, load_model, Fe, Fx, Fd, eval_metrics, get_predictions\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "from textwrap import wrap\n",
    "\n",
    "\n",
    "from utils import KunischMetrics\n",
    "from utils import KunischPruner\n",
    "from utils import DataExplorer\n",
    "from utils import KunischPlotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuración de dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando device: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# 0 es 3090, 1 y 2 son 2080\n",
    "CUDA_ID = 0\n",
    "\n",
    "device = torch.device(f'cuda:{CUDA_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuración de experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Flags para los datos sintéticos\n",
    "# Cada flag está asociada a una o más funciones de data augmentation.\n",
    "# Los datos deben existir previamente \n",
    "# (se generan a partir del notebook split and augmentation)\n",
    "DS_FLAGS = []\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'gausblur': [gausblur]\n",
    "              # 'msblur': [msblur]\n",
    "              # 'mtnblur': [mtnblur]\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "            \n",
    "# Las flags crop, randaug, elastic y gausblur \n",
    "# se pueden aplicar más de una vez c/u. \n",
    "# (si no están en DS_FLAGS, serán ignoradas).\n",
    "CROP_TIMES = 1\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1\n",
    "GAUSBLUR_TIMES = 1\n",
    "\n",
    "NUM_LABELS = 26\n",
    "BATCH_SIZE = 100\n",
    "PATIENCE = 100\n",
    "NUM_EPOCHS = 600\n",
    "FEATURES_DIM = 4096\n",
    "\n",
    "# C2AE Scene config\n",
    "latent_dim = 70\n",
    "num_labels = NUM_LABELS\n",
    "fx_h_dim= 120\n",
    "fe_h_dim= 120\n",
    "fd_h_dim= 120\n",
    "\n",
    "# 0 es 3090, 1 y 2 son 2080\n",
    "CUDA_ID = 0\n",
    "\n",
    "SAVE = True\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Nombre del experimento: 26L\n",
      "--Feature set encontrado en ../features/alexnet_retrained/base/K0\n",
      "--Labels set encontrado en ../labels/base/0\n",
      "\n",
      "Los resultados se guardarán en: ../outputs/C2AE_alexnet_retrained/base/26L/0\n",
      "Los modelos se guardarán en: ../models/C2AE_alexnet_retrained/base/0\n",
      "Fold  1\n",
      "Nombre del experimento: 26L\n",
      "--Feature set encontrado en ../features/alexnet_retrained/base/K1\n",
      "--Labels set encontrado en ../labels/base/1\n",
      "\n",
      "Los resultados se guardarán en: ../outputs/C2AE_alexnet_retrained/base/26L/1\n",
      "Los modelos se guardarán en: ../models/C2AE_alexnet_retrained/base/1\n",
      "Fold  2\n",
      "Nombre del experimento: 26L\n",
      "--Feature set encontrado en ../features/alexnet_retrained/base/K2\n",
      "--Labels set encontrado en ../labels/base/2\n",
      "\n",
      "Los resultados se guardarán en: ../outputs/C2AE_alexnet_retrained/base/26L/2\n",
      "Los modelos se guardarán en: ../models/C2AE_alexnet_retrained/base/2\n",
      "Fold  3\n",
      "Nombre del experimento: 26L\n",
      "--Feature set encontrado en ../features/alexnet_retrained/base/K3\n",
      "--Labels set encontrado en ../labels/base/3\n",
      "\n",
      "Los resultados se guardarán en: ../outputs/C2AE_alexnet_retrained/base/26L/3\n",
      "Los modelos se guardarán en: ../models/C2AE_alexnet_retrained/base/3\n"
     ]
    }
   ],
   "source": [
    "# Esta celda construye la variable data_flags, que lee DS_FLAGS de \n",
    "# la celda anterior y mapea su contenido a distintas rutas de \n",
    "# patrones, etiquetas y outputs\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "         'gausblur': GAUSBLUR_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic', 'gausblur']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "# Revisión de los folds y creación de diccionario con paths\n",
    "Kfolds = {}\n",
    "\n",
    "\n",
    "for i in range(0, K):\n",
    "    print(\"Fold \", i)\n",
    "    \n",
    "    exp_name = f\"{NUM_LABELS}L\"\n",
    "    print(f\"Nombre del experimento: {exp_name}\")\n",
    "    \n",
    "\n",
    "    features_dir = os.path.join(root_dir, 'features', 'alexnet_retrained', data_flags, f'K{str(i)}')\n",
    "    labels_dir = os.path.join(root_dir, 'labels', data_flags, str(i))\n",
    "    output_dir = os.path.join(root_dir, \"outputs\", \"C2AE_alexnet_retrained\", data_flags, exp_name, str(i))\n",
    "    model_dir = os.path.join(root_dir, 'models', 'C2AE_alexnet_retrained', data_flags, str(i))\n",
    "    model_path = os.path.join(model_dir, exp_name + '.pth')\n",
    "\n",
    "\n",
    "    Kfolds[i] = {\n",
    "        'labels_dir': labels_dir,\n",
    "        'output_dir': output_dir,\n",
    "        'model_path': model_path,\n",
    "        'features_dir': features_dir,\n",
    "    }\n",
    "    \n",
    "    if not (os.path.isdir(features_dir) and os.path.isdir(labels_dir)):\n",
    "        print(features_dir)\n",
    "        print(labels_dir)\n",
    "        raise FileNotFoundError(\"\"\"\n",
    "        No existen directorios de datos para el conjunto de flags seleccionado. \n",
    "        Verifique que el dataset exista y, de lo contrario, llame a Split and Augmentation.\n",
    "        \"\"\")\n",
    "        \n",
    "    print(\"--Feature set encontrado en {}\".format(features_dir))\n",
    "    print(\"--Labels set encontrado en {}\".format(labels_dir))\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "    if SAVE:\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        os.makedirs(model_dir, exist_ok = True)\n",
    "        print(f\"Los resultados se guardarán en: {output_dir}\")\n",
    "        print(f\"Los modelos se guardarán en: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando top_labels previamente generados para 26 labels\n",
      "Starting training!\n",
      "Epoch: 0, Loss: 20.247953414916992,  L-Loss: 2.732123613357544, C-Loss: 1.8881890773773193\n",
      "Epoch: 1, Loss: 19.669337272644043,  L-Loss: 1.7019063234329224, C-Loss: 1.8818383812904358\n",
      "Epoch: 2, Loss: 19.425138473510742,  L-Loss: 1.3926793336868286, C-Loss: 1.8728798627853394\n",
      "Epoch: 3, Loss: 19.267860412597656,  L-Loss: 1.2664667963981628, C-Loss: 1.863462746143341\n",
      "Epoch: 4, Loss: 19.15009880065918,  L-Loss: 1.325123906135559, C-Loss: 1.848753571510315\n",
      "Epoch: 5, Loss: 19.138456344604492,  L-Loss: 1.5735853910446167, C-Loss: 1.835166335105896\n",
      "Epoch: 6, Loss: 19.082763671875,  L-Loss: 1.7490448355674744, C-Loss: 1.820824146270752\n",
      "Epoch: 7, Loss: 18.9682674407959,  L-Loss: 1.735338568687439, C-Loss: 1.810059666633606\n",
      "Epoch: 8, Loss: 18.814847946166992,  L-Loss: 1.6947866678237915, C-Loss: 1.796745479106903\n",
      "Epoch: 9, Loss: 18.702113151550293,  L-Loss: 1.6335653066635132, C-Loss: 1.7885330319404602\n",
      "Epoch: 10, Loss: 18.61328411102295,  L-Loss: 1.621585726737976, C-Loss: 1.7802491784095764\n",
      "Epoch: 11, Loss: 18.56378173828125,  L-Loss: 1.6633158922195435, C-Loss: 1.7732124328613281\n",
      "Epoch: 12, Loss: 18.50429344177246,  L-Loss: 1.6992251873016357, C-Loss: 1.7654681205749512\n",
      "Epoch: 13, Loss: 18.498737335205078,  L-Loss: 1.7608721852302551, C-Loss: 1.7618300914764404\n",
      "Epoch: 14, Loss: 18.469414710998535,  L-Loss: 1.8384060263633728, C-Loss: 1.7550211548805237\n",
      "Epoch: 15, Loss: 18.494324684143066,  L-Loss: 1.8946917653083801, C-Loss: 1.7546979188919067\n",
      "Epoch: 16, Loss: 18.525317192077637,  L-Loss: 1.9744036197662354, C-Loss: 1.753811538219452\n",
      "Epoch: 17, Loss: 18.525272369384766,  L-Loss: 2.041640341281891, C-Loss: 1.7504451274871826\n",
      "Epoch: 18, Loss: 18.571046829223633,  L-Loss: 2.114059567451477, C-Loss: 1.7514017224311829\n",
      "Epoch: 19, Loss: 18.58274269104004,  L-Loss: 2.137648344039917, C-Loss: 1.7513919472694397\n",
      "Epoch: 20, Loss: 18.606815338134766,  L-Loss: 2.177274227142334, C-Loss: 1.751817762851715\n",
      "Epoch: 21, Loss: 18.651183128356934,  L-Loss: 2.212263345718384, C-Loss: 1.7545050978660583\n",
      "Epoch: 22, Loss: 18.669950485229492,  L-Loss: 2.2093424797058105, C-Loss: 1.7565279603004456\n",
      "Epoch: 23, Loss: 18.643855094909668,  L-Loss: 2.1919718980789185, C-Loss: 1.7547869682312012\n",
      "Epoch: 24, Loss: 18.605875968933105,  L-Loss: 2.1727648973464966, C-Loss: 1.7519493103027344\n",
      "Epoch: 25, Loss: 18.64716911315918,  L-Loss: 2.1746917963027954, C-Loss: 1.7559823989868164\n",
      "Epoch: 26, Loss: 18.635713577270508,  L-Loss: 2.157616376876831, C-Loss: 1.755690574645996\n",
      "Epoch: 27, Loss: 18.61617088317871,  L-Loss: 2.112839937210083, C-Loss: 1.7559750080108643\n",
      "Epoch: 28, Loss: 18.6609525680542,  L-Loss: 2.1148561239242554, C-Loss: 1.760352373123169\n",
      "Epoch: 29, Loss: 18.66507625579834,  L-Loss: 2.0906115770339966, C-Loss: 1.7619771361351013\n",
      "Epoch: 30, Loss: 18.626914978027344,  L-Loss: 2.063582181930542, C-Loss: 1.7595123648643494\n",
      "Epoch: 31, Loss: 18.63916301727295,  L-Loss: 2.0324186086654663, C-Loss: 1.7622953057289124\n",
      "Epoch: 32, Loss: 18.617738723754883,  L-Loss: 2.018435001373291, C-Loss: 1.7608521580696106\n",
      "Epoch: 33, Loss: 18.615120887756348,  L-Loss: 2.008840262889862, C-Loss: 1.7610700726509094\n",
      "Epoch: 34, Loss: 18.585861206054688,  L-Loss: 1.9838327765464783, C-Loss: 1.7593945264816284\n",
      "Epoch: 35, Loss: 18.55420684814453,  L-Loss: 1.932891845703125, C-Loss: 1.7587761282920837\n",
      "Epoch: 36, Loss: 18.555964469909668,  L-Loss: 1.9107611179351807, C-Loss: 1.7600584030151367\n",
      "Epoch: 37, Loss: 18.54975414276123,  L-Loss: 1.888922393321991, C-Loss: 1.7605292797088623\n",
      "Epoch: 38, Loss: 18.534708976745605,  L-Loss: 1.8561956286430359, C-Loss: 1.7606611251831055\n",
      "Epoch: 39, Loss: 18.509455680847168,  L-Loss: 1.8205471634864807, C-Loss: 1.7599181532859802\n",
      "Epoch: 40, Loss: 18.473360061645508,  L-Loss: 1.7998983263969421, C-Loss: 1.7573411464691162\n",
      "Epoch: 41, Loss: 18.48159408569336,  L-Loss: 1.7827184796333313, C-Loss: 1.7590235471725464\n",
      "Epoch: 42, Loss: 18.432964324951172,  L-Loss: 1.7339149117469788, C-Loss: 1.756600797176361\n",
      "Epoch: 43, Loss: 18.461220741271973,  L-Loss: 1.7140581607818604, C-Loss: 1.7604190111160278\n",
      "Epoch: 44, Loss: 18.43210506439209,  L-Loss: 1.6956264972686768, C-Loss: 1.7584291696548462\n",
      "Epoch: 45, Loss: 18.41575527191162,  L-Loss: 1.6863762140274048, C-Loss: 1.7572566866874695\n",
      "Epoch: 46, Loss: 18.39360237121582,  L-Loss: 1.6720758080482483, C-Loss: 1.7557563781738281\n",
      "Epoch: 47, Loss: 18.389787673950195,  L-Loss: 1.650545597076416, C-Loss: 1.7564513683319092\n",
      "Epoch: 48, Loss: 18.335622787475586,  L-Loss: 1.6232057213783264, C-Loss: 1.7524020075798035\n",
      "Epoch: 49, Loss: 18.32412052154541,  L-Loss: 1.5985296368598938, C-Loss: 1.7524855136871338\n",
      "Epoch: 50, Loss: 18.337284088134766,  L-Loss: 1.6013767719268799, C-Loss: 1.7536595463752747\n",
      "Epoch: 51, Loss: 18.29619026184082,  L-Loss: 1.57579904794693, C-Loss: 1.750829041004181\n",
      "Epoch: 52, Loss: 18.26868438720703,  L-Loss: 1.5695158243179321, C-Loss: 1.7483927011489868\n",
      "Epoch: 53, Loss: 18.223114013671875,  L-Loss: 1.5509584546089172, C-Loss: 1.744763433933258\n",
      "Epoch: 54, Loss: 18.233731269836426,  L-Loss: 1.5350814461708069, C-Loss: 1.746619164943695\n",
      "Epoch: 55, Loss: 18.158400535583496,  L-Loss: 1.5237812995910645, C-Loss: 1.7396509051322937\n",
      "Epoch: 56, Loss: 18.17637348175049,  L-Loss: 1.5001052021980286, C-Loss: 1.7426321506500244\n",
      "Epoch: 57, Loss: 18.217413902282715,  L-Loss: 1.495759665966034, C-Loss: 1.7469534277915955\n",
      "Epoch: 58, Loss: 18.158562660217285,  L-Loss: 1.4784277081489563, C-Loss: 1.7419349551200867\n",
      "Epoch: 59, Loss: 18.15459156036377,  L-Loss: 1.475527822971344, C-Loss: 1.741682767868042\n",
      "Epoch: 60, Loss: 18.091307640075684,  L-Loss: 1.451425850391388, C-Loss: 1.736559510231018\n",
      "Epoch: 61, Loss: 18.0595703125,  L-Loss: 1.4328217506408691, C-Loss: 1.7343158721923828\n",
      "Epoch: 62, Loss: 18.077445030212402,  L-Loss: 1.415919840335846, C-Loss: 1.736948549747467\n",
      "Epoch: 63, Loss: 18.07087993621826,  L-Loss: 1.41020929813385, C-Loss: 1.7365776896476746\n",
      "Epoch: 64, Loss: 18.03832721710205,  L-Loss: 1.3920661807060242, C-Loss: 1.7342294454574585\n",
      "Epoch: 65, Loss: 18.048080444335938,  L-Loss: 1.3836999535560608, C-Loss: 1.7356230020523071\n",
      "Epoch: 66, Loss: 18.00596809387207,  L-Loss: 1.3592846989631653, C-Loss: 1.7326324582099915\n",
      "Epoch: 67, Loss: 18.01322364807129,  L-Loss: 1.3447393774986267, C-Loss: 1.7340853214263916\n",
      "Epoch: 68, Loss: 18.022631645202637,  L-Loss: 1.335634708404541, C-Loss: 1.7354814410209656\n",
      "Epoch: 69, Loss: 18.04364776611328,  L-Loss: 1.3258743286132812, C-Loss: 1.7380710244178772\n",
      "Epoch: 70, Loss: 18.014713287353516,  L-Loss: 1.309083640575409, C-Loss: 1.7360172271728516\n",
      "Epoch: 71, Loss: 17.98318386077881,  L-Loss: 1.29229074716568, C-Loss: 1.733703851699829\n",
      "Epoch: 72, Loss: 17.97418975830078,  L-Loss: 1.2948945760726929, C-Loss: 1.732674241065979\n",
      "Epoch: 73, Loss: 17.952346801757812,  L-Loss: 1.2695286273956299, C-Loss: 1.731758177280426\n",
      "Epoch: 74, Loss: 17.971412658691406,  L-Loss: 1.2669149041175842, C-Loss: 1.733795404434204\n",
      "Epoch: 75, Loss: 17.967857360839844,  L-Loss: 1.2626356482505798, C-Loss: 1.7336539030075073\n",
      "Epoch: 76, Loss: 17.95388126373291,  L-Loss: 1.2511433959007263, C-Loss: 1.732831060886383\n",
      "Epoch: 77, Loss: 17.936948776245117,  L-Loss: 1.2264211177825928, C-Loss: 1.7323737740516663\n",
      "Epoch: 78, Loss: 17.946303367614746,  L-Loss: 1.2161503434181213, C-Loss: 1.7338228225708008\n",
      "Epoch: 79, Loss: 17.936684608459473,  L-Loss: 1.2058706879615784, C-Loss: 1.733374834060669\n",
      "Epoch: 80, Loss: 17.936026573181152,  L-Loss: 1.1894934177398682, C-Loss: 1.7341280579566956\n",
      "Epoch: 81, Loss: 17.927618980407715,  L-Loss: 1.1708353161811829, C-Loss: 1.7342202067375183\n",
      "Epoch: 82, Loss: 17.945786476135254,  L-Loss: 1.1571749448776245, C-Loss: 1.7367198467254639\n",
      "Epoch: 83, Loss: 17.92242431640625,  L-Loss: 1.1466438174247742, C-Loss: 1.734910249710083\n",
      "Epoch: 84, Loss: 17.90147590637207,  L-Loss: 1.1232157349586487, C-Loss: 1.7339868545532227\n",
      "Epoch: 85, Loss: 17.896191596984863,  L-Loss: 1.103062927722931, C-Loss: 1.7344659566879272\n",
      "Epoch: 86, Loss: 17.912837028503418,  L-Loss: 1.1067281365394592, C-Loss: 1.73594731092453\n",
      "Epoch: 87, Loss: 17.886686325073242,  L-Loss: 1.08561509847641, C-Loss: 1.7343878746032715\n",
      "Epoch: 88, Loss: 17.923773765563965,  L-Loss: 1.0696435570716858, C-Loss: 1.7388952374458313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89, Loss: 17.9044246673584,  L-Loss: 1.0634883642196655, C-Loss: 1.737268090248108\n",
      "Epoch: 90, Loss: 17.923744201660156,  L-Loss: 1.0399297773838043, C-Loss: 1.7403780221939087\n",
      "Epoch: 91, Loss: 17.92375087738037,  L-Loss: 1.03075110912323, C-Loss: 1.7408375144004822\n",
      "Epoch: 92, Loss: 17.89631175994873,  L-Loss: 1.0207402110099792, C-Loss: 1.738594114780426\n",
      "Epoch: 93, Loss: 17.886780738830566,  L-Loss: 1.0063888430595398, C-Loss: 1.738358736038208\n",
      "Epoch: 94, Loss: 17.891791343688965,  L-Loss: 0.9956810772418976, C-Loss: 1.739395022392273\n",
      "Epoch: 95, Loss: 17.873581886291504,  L-Loss: 0.9822920560836792, C-Loss: 1.7382436394691467\n",
      "Epoch: 96, Loss: 17.879032135009766,  L-Loss: 0.9695059657096863, C-Loss: 1.739427924156189\n",
      "Epoch: 97, Loss: 17.86746120452881,  L-Loss: 0.9559260904788971, C-Loss: 1.7389497756958008\n",
      "Epoch: 98, Loss: 17.87734031677246,  L-Loss: 0.9455108642578125, C-Loss: 1.7404584884643555\n",
      "Epoch: 99, Loss: 17.866747856140137,  L-Loss: 0.9357843697071075, C-Loss: 1.7398855686187744\n",
      "Epoch: 100, Loss: 17.860301971435547,  L-Loss: 0.9312130808830261, C-Loss: 1.7394695281982422\n",
      "Epoch: 101, Loss: 17.847290992736816,  L-Loss: 0.9161965548992157, C-Loss: 1.738919198513031\n",
      "Epoch: 102, Loss: 17.8314208984375,  L-Loss: 0.9053855836391449, C-Loss: 1.737872838973999\n",
      "Epoch: 103, Loss: 17.84724998474121,  L-Loss: 0.8914919495582581, C-Loss: 1.7401503920555115\n",
      "Epoch: 104, Loss: 17.863128662109375,  L-Loss: 0.8824048042297363, C-Loss: 1.742192566394806\n",
      "Epoch: 105, Loss: 17.81987762451172,  L-Loss: 0.8727256655693054, C-Loss: 1.7383514046669006\n",
      "Epoch: 106, Loss: 17.84048557281494,  L-Loss: 0.869296669960022, C-Loss: 1.7405838370323181\n",
      "Epoch: 107, Loss: 17.831947326660156,  L-Loss: 0.8561616837978363, C-Loss: 1.7403864860534668\n",
      "Epoch: 108, Loss: 17.78156089782715,  L-Loss: 0.8416323661804199, C-Loss: 1.7360745072364807\n",
      "Epoch: 109, Loss: 17.786856651306152,  L-Loss: 0.8325260877609253, C-Loss: 1.7370592951774597\n",
      "Epoch: 110, Loss: 17.8308687210083,  L-Loss: 0.8260234892368317, C-Loss: 1.7417856454849243\n",
      "Epoch: 111, Loss: 17.76602077484131,  L-Loss: 0.8121986389160156, C-Loss: 1.7359921336174011\n",
      "Epoch: 112, Loss: 17.780661582946777,  L-Loss: 0.805822342634201, C-Loss: 1.7377749681472778\n",
      "Epoch: 113, Loss: 17.77130699157715,  L-Loss: 0.7957008183002472, C-Loss: 1.7373456358909607\n",
      "Epoch: 114, Loss: 17.781206130981445,  L-Loss: 0.7891475856304169, C-Loss: 1.7386631965637207\n",
      "Epoch: 115, Loss: 17.732763290405273,  L-Loss: 0.7764110565185547, C-Loss: 1.7344557642936707\n",
      "Epoch: 116, Loss: 17.72251033782959,  L-Loss: 0.7696177661418915, C-Loss: 1.7337701916694641\n",
      "Epoch: 117, Loss: 17.778918266296387,  L-Loss: 0.766778826713562, C-Loss: 1.739552915096283\n",
      "Epoch: 118, Loss: 17.724740982055664,  L-Loss: 0.7571544945240021, C-Loss: 1.7346163392066956\n",
      "Epoch: 119, Loss: 17.73897647857666,  L-Loss: 0.7485189437866211, C-Loss: 1.7364717721939087\n",
      "Epoch: 120, Loss: 17.704130172729492,  L-Loss: 0.736262708902359, C-Loss: 1.7335999608039856\n",
      "Epoch: 121, Loss: 17.745659828186035,  L-Loss: 0.7282331585884094, C-Loss: 1.7381543517112732\n",
      "Epoch: 122, Loss: 17.723090171813965,  L-Loss: 0.7233270406723022, C-Loss: 1.7361427545547485\n",
      "Epoch: 123, Loss: 17.75093364715576,  L-Loss: 0.7154086828231812, C-Loss: 1.7393229007720947\n",
      "Epoch: 124, Loss: 17.747703552246094,  L-Loss: 0.7101296186447144, C-Loss: 1.7392637729644775\n",
      "Epoch: 125, Loss: 17.726030349731445,  L-Loss: 0.7020676732063293, C-Loss: 1.7374996542930603\n",
      "Epoch: 126, Loss: 17.689040184020996,  L-Loss: 0.6923789083957672, C-Loss: 1.7342851161956787\n",
      "Epoch: 127, Loss: 17.689064025878906,  L-Loss: 0.6825922131538391, C-Loss: 1.734776794910431\n",
      "Epoch: 128, Loss: 17.68601703643799,  L-Loss: 0.6781233549118042, C-Loss: 1.734695553779602\n",
      "Epoch: 129, Loss: 17.677206993103027,  L-Loss: 0.6694186329841614, C-Loss: 1.734249770641327\n",
      "Epoch: 130, Loss: 17.68197250366211,  L-Loss: 0.6627539098262787, C-Loss: 1.7350595593452454\n",
      "Epoch: 131, Loss: 17.693893432617188,  L-Loss: 0.6542056500911713, C-Loss: 1.7366790771484375\n",
      "Epoch: 132, Loss: 17.663930892944336,  L-Loss: 0.6506931483745575, C-Loss: 1.7338584661483765\n",
      "Epoch: 133, Loss: 17.682618141174316,  L-Loss: 0.6391631662845612, C-Loss: 1.736303687095642\n",
      "Epoch: 134, Loss: 17.704509735107422,  L-Loss: 0.6329949796199799, C-Loss: 1.7388012409210205\n",
      "Epoch: 135, Loss: 17.68932056427002,  L-Loss: 0.6281148493289948, C-Loss: 1.7375263571739197\n",
      "Epoch: 136, Loss: 17.653701782226562,  L-Loss: 0.6192882359027863, C-Loss: 1.7344056963920593\n",
      "Epoch: 137, Loss: 17.676700592041016,  L-Loss: 0.6167181432247162, C-Loss: 1.7368342280387878\n",
      "Epoch: 138, Loss: 17.683077812194824,  L-Loss: 0.6061054766178131, C-Loss: 1.7380025386810303\n",
      "Epoch: 139, Loss: 17.668511390686035,  L-Loss: 0.6010704040527344, C-Loss: 1.7367976307868958\n",
      "Epoch: 140, Loss: 17.645657539367676,  L-Loss: 0.5979901850223541, C-Loss: 1.7346662878990173\n",
      "Epoch: 141, Loss: 17.6954984664917,  L-Loss: 0.5923263728618622, C-Loss: 1.7399335503578186\n",
      "Epoch: 142, Loss: 17.695332527160645,  L-Loss: 0.5878605246543884, C-Loss: 1.7401402592658997\n",
      "Epoch: 143, Loss: 17.727383613586426,  L-Loss: 0.5916066467761993, C-Loss: 1.7431580424308777\n",
      "Epoch: 144, Loss: 17.716649055480957,  L-Loss: 0.599998950958252, C-Loss: 1.741665005683899\n",
      "Epoch: 145, Loss: 17.732584953308105,  L-Loss: 0.6045455038547516, C-Loss: 1.7430311441421509\n",
      "Epoch: 146, Loss: 17.75101661682129,  L-Loss: 0.604149580001831, C-Loss: 1.74489426612854\n",
      "Epoch: 147, Loss: 17.739283561706543,  L-Loss: 0.6007637083530426, C-Loss: 1.7438901662826538\n",
      "Epoch: 148, Loss: 17.74367046356201,  L-Loss: 0.601042628288269, C-Loss: 1.7443150281906128\n",
      "Epoch: 149, Loss: 17.70344352722168,  L-Loss: 0.5924970209598541, C-Loss: 1.740719497203827\n",
      "Epoch: 150, Loss: 17.71285343170166,  L-Loss: 0.591314971446991, C-Loss: 1.7417196035385132\n",
      "Epoch: 151, Loss: 17.719871520996094,  L-Loss: 0.5802600979804993, C-Loss: 1.7429741621017456\n",
      "Epoch: 152, Loss: 17.731764793395996,  L-Loss: 0.5797365009784698, C-Loss: 1.7441896200180054\n",
      "Epoch: 153, Loss: 17.692818641662598,  L-Loss: 0.5701002478599548, C-Loss: 1.7407768964767456\n",
      "Epoch: 154, Loss: 17.73312473297119,  L-Loss: 0.5694090723991394, C-Loss: 1.744841992855072\n",
      "Epoch: 155, Loss: 17.682828903198242,  L-Loss: 0.5623445510864258, C-Loss: 1.7401655912399292\n",
      "Epoch: 156, Loss: 17.68031883239746,  L-Loss: 0.5550387799739838, C-Loss: 1.7402798533439636\n",
      "Epoch: 157, Loss: 17.679354667663574,  L-Loss: 0.5484862625598907, C-Loss: 1.7405111193656921\n",
      "Epoch: 158, Loss: 17.66938877105713,  L-Loss: 0.5426672995090485, C-Loss: 1.7398055791854858\n",
      "Epoch: 159, Loss: 17.713653564453125,  L-Loss: 0.5386246740818024, C-Loss: 1.7444341778755188\n",
      "Epoch: 160, Loss: 17.716181755065918,  L-Loss: 0.5317035019397736, C-Loss: 1.7450329065322876\n",
      "Epoch: 161, Loss: 17.685782432556152,  L-Loss: 0.5314963757991791, C-Loss: 1.742003321647644\n",
      "Epoch: 162, Loss: 17.74270248413086,  L-Loss: 0.5195013731718063, C-Loss: 1.7482951879501343\n",
      "Epoch: 163, Loss: 17.72318935394287,  L-Loss: 0.5220325291156769, C-Loss: 1.7462172508239746\n",
      "Epoch: 164, Loss: 17.70335102081299,  L-Loss: 0.511950820684433, C-Loss: 1.744737446308136\n",
      "Epoch: 165, Loss: 17.688907623291016,  L-Loss: 0.5048018246889114, C-Loss: 1.7436506152153015\n",
      "Epoch: 166, Loss: 17.701415061950684,  L-Loss: 0.501082718372345, C-Loss: 1.7450873255729675\n",
      "Epoch: 167, Loss: 17.67662525177002,  L-Loss: 0.4970823675394058, C-Loss: 1.7428084015846252\n",
      "Epoch: 168, Loss: 17.70099449157715,  L-Loss: 0.4881318360567093, C-Loss: 1.7456929087638855\n",
      "Epoch: 169, Loss: 17.717631340026855,  L-Loss: 0.48865653574466705, C-Loss: 1.7473303079605103\n",
      "Epoch: 170, Loss: 17.707286834716797,  L-Loss: 0.4804873764514923, C-Loss: 1.746704339981079\n",
      "Epoch: 171, Loss: 17.686809539794922,  L-Loss: 0.4791489839553833, C-Loss: 1.7447234988212585\n",
      "Epoch: 172, Loss: 17.633822441101074,  L-Loss: 0.4724600315093994, C-Loss: 1.7397592663764954\n",
      "Epoch: 173, Loss: 17.68185329437256,  L-Loss: 0.4650150388479233, C-Loss: 1.7449346780776978\n",
      "Out of patience at epoch 173\n",
      "Guardando mejor modelo en  ../models/C2AE_alexnet_retrained/base/0/26L.pth\n",
      "HS fold 0: 0.2335\n",
      "Predicciones guardadas en ../outputs/C2AE_alexnet_retrained/base/26L/0/predictions.csv\n",
      "Usando top_labels previamente generados para 26 labels\n",
      "Starting training!\n",
      "Epoch: 0, Loss: 21.70053768157959,  L-Loss: 4.800477981567383, C-Loss: 1.9300298690795898\n",
      "Epoch: 1, Loss: 20.3112154006958,  L-Loss: 2.353434920310974, C-Loss: 1.9134497046470642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 19.790912628173828,  L-Loss: 1.6210241317749023, C-Loss: 1.8980400562286377\n",
      "Epoch: 3, Loss: 19.466350555419922,  L-Loss: 1.3188530802726746, C-Loss: 1.8806923627853394\n",
      "Epoch: 4, Loss: 19.225903511047363,  L-Loss: 1.2204980254173279, C-Loss: 1.8615654110908508\n",
      "Epoch: 5, Loss: 19.083560943603516,  L-Loss: 1.2910804748535156, C-Loss: 1.843802034854889\n",
      "Epoch: 6, Loss: 18.93904399871826,  L-Loss: 1.3772651553153992, C-Loss: 1.8250410556793213\n",
      "Epoch: 7, Loss: 18.820331573486328,  L-Loss: 1.4318936467170715, C-Loss: 1.8104384541511536\n",
      "Epoch: 8, Loss: 18.717524528503418,  L-Loss: 1.4224196672439575, C-Loss: 1.8006314635276794\n",
      "Epoch: 9, Loss: 18.651254653930664,  L-Loss: 1.4229938387870789, C-Loss: 1.793975830078125\n",
      "Epoch: 10, Loss: 18.59363842010498,  L-Loss: 1.4226353168487549, C-Loss: 1.7882322072982788\n",
      "Epoch: 11, Loss: 18.589316368103027,  L-Loss: 1.4564395546913147, C-Loss: 1.7861098051071167\n",
      "Epoch: 12, Loss: 18.595656394958496,  L-Loss: 1.526605248451233, C-Loss: 1.7832353711128235\n",
      "Epoch: 13, Loss: 18.63099765777588,  L-Loss: 1.6169294118881226, C-Loss: 1.7822532653808594\n",
      "Epoch: 14, Loss: 18.67775058746338,  L-Loss: 1.7092785835266113, C-Loss: 1.782311201095581\n",
      "Epoch: 15, Loss: 18.711112022399902,  L-Loss: 1.7758944630622864, C-Loss: 1.7823163866996765\n",
      "Epoch: 16, Loss: 18.780941009521484,  L-Loss: 1.8615020513534546, C-Loss: 1.7850189805030823\n",
      "Epoch: 17, Loss: 18.805158615112305,  L-Loss: 1.9003600478172302, C-Loss: 1.7854978442192078\n",
      "Epoch: 18, Loss: 18.823429107666016,  L-Loss: 1.935610294342041, C-Loss: 1.7855623960494995\n",
      "Epoch: 19, Loss: 18.873061180114746,  L-Loss: 1.98450767993927, C-Loss: 1.788080632686615\n",
      "Epoch: 20, Loss: 18.875732421875,  L-Loss: 1.9795653223991394, C-Loss: 1.7885949611663818\n",
      "Epoch: 21, Loss: 18.898863792419434,  L-Loss: 1.986948549747467, C-Loss: 1.7905388474464417\n",
      "Epoch: 22, Loss: 18.891806602478027,  L-Loss: 1.9985477924346924, C-Loss: 1.7892532348632812\n",
      "Epoch: 23, Loss: 18.891512870788574,  L-Loss: 2.0041374564170837, C-Loss: 1.7889444828033447\n",
      "Epoch: 24, Loss: 18.876855850219727,  L-Loss: 1.9704442024230957, C-Loss: 1.7891634106636047\n",
      "Epoch: 25, Loss: 18.86767864227295,  L-Loss: 1.9621247053146362, C-Loss: 1.7886616587638855\n",
      "Epoch: 26, Loss: 18.861835479736328,  L-Loss: 1.9477515816688538, C-Loss: 1.788796067237854\n",
      "Epoch: 27, Loss: 18.86661434173584,  L-Loss: 1.9258719086647034, C-Loss: 1.790367841720581\n",
      "Epoch: 28, Loss: 18.857614517211914,  L-Loss: 1.9200453758239746, C-Loss: 1.78975909948349\n",
      "Epoch: 29, Loss: 18.88987159729004,  L-Loss: 1.9133722186088562, C-Loss: 1.7933184504508972\n",
      "Epoch: 30, Loss: 18.8898983001709,  L-Loss: 1.9107783436775208, C-Loss: 1.793450951576233\n",
      "Epoch: 31, Loss: 18.879474639892578,  L-Loss: 1.8828299045562744, C-Loss: 1.7938059568405151\n",
      "Epoch: 32, Loss: 18.881251335144043,  L-Loss: 1.8693925142288208, C-Loss: 1.7946555018424988\n",
      "Epoch: 33, Loss: 18.91132926940918,  L-Loss: 1.858220398426056, C-Loss: 1.7982219457626343\n",
      "Epoch: 34, Loss: 18.915081024169922,  L-Loss: 1.8321168422698975, C-Loss: 1.7999023795127869\n",
      "Epoch: 35, Loss: 18.90633773803711,  L-Loss: 1.8102903366088867, C-Loss: 1.8001193404197693\n",
      "Epoch: 36, Loss: 18.898256301879883,  L-Loss: 1.7878048419952393, C-Loss: 1.8004353642463684\n",
      "Epoch: 37, Loss: 18.89943790435791,  L-Loss: 1.7697773575782776, C-Loss: 1.8014549612998962\n",
      "Epoch: 38, Loss: 18.91902256011963,  L-Loss: 1.7577381134033203, C-Loss: 1.8040153980255127\n",
      "Epoch: 39, Loss: 18.890085220336914,  L-Loss: 1.7352052927017212, C-Loss: 1.802248239517212\n",
      "Epoch: 40, Loss: 18.901888847351074,  L-Loss: 1.7088719606399536, C-Loss: 1.804745376110077\n",
      "Epoch: 41, Loss: 18.897594451904297,  L-Loss: 1.6951852440834045, C-Loss: 1.8050002455711365\n",
      "Epoch: 42, Loss: 18.884696006774902,  L-Loss: 1.6696914434432983, C-Loss: 1.8049850463867188\n",
      "Epoch: 43, Loss: 18.894726753234863,  L-Loss: 1.6666566133499146, C-Loss: 1.8061398267745972\n",
      "Epoch: 44, Loss: 18.829575538635254,  L-Loss: 1.6461528539657593, C-Loss: 1.8006499409675598\n",
      "Epoch: 45, Loss: 18.872580528259277,  L-Loss: 1.6329287886619568, C-Loss: 1.8056116104125977\n",
      "Epoch: 46, Loss: 18.816411018371582,  L-Loss: 1.6184420585632324, C-Loss: 1.8007190227508545\n",
      "Epoch: 47, Loss: 18.843857765197754,  L-Loss: 1.5941026210784912, C-Loss: 1.8046806454658508\n",
      "Epoch: 48, Loss: 18.818116188049316,  L-Loss: 1.570292890071869, C-Loss: 1.8032970428466797\n",
      "Epoch: 49, Loss: 18.812533378601074,  L-Loss: 1.5531603693962097, C-Loss: 1.8035953044891357\n",
      "Epoch: 50, Loss: 18.805267333984375,  L-Loss: 1.5513200163841248, C-Loss: 1.8029606342315674\n",
      "Epoch: 51, Loss: 18.762432098388672,  L-Loss: 1.5267325639724731, C-Loss: 1.7999066710472107\n",
      "Epoch: 52, Loss: 18.717092514038086,  L-Loss: 1.5122092962265015, C-Loss: 1.7960987091064453\n",
      "Epoch: 53, Loss: 18.74946117401123,  L-Loss: 1.4814284443855286, C-Loss: 1.8008747100830078\n",
      "Epoch: 54, Loss: 18.718676567077637,  L-Loss: 1.476653814315796, C-Loss: 1.7980349659919739\n",
      "Epoch: 55, Loss: 18.744726181030273,  L-Loss: 1.4629061818122864, C-Loss: 1.8013271689414978\n",
      "Epoch: 56, Loss: 18.671435356140137,  L-Loss: 1.444337248802185, C-Loss: 1.7949267625808716\n",
      "Epoch: 57, Loss: 18.649015426635742,  L-Loss: 1.4250118136405945, C-Loss: 1.7936508655548096\n",
      "Epoch: 58, Loss: 18.64309310913086,  L-Loss: 1.4116317629814148, C-Loss: 1.7937277555465698\n",
      "Epoch: 59, Loss: 18.665419578552246,  L-Loss: 1.391808807849884, C-Loss: 1.7969515323638916\n",
      "Epoch: 60, Loss: 18.611586570739746,  L-Loss: 1.3727261424064636, C-Loss: 1.7925222516059875\n",
      "Epoch: 61, Loss: 18.588480949401855,  L-Loss: 1.3712424635887146, C-Loss: 1.7902860045433044\n",
      "Epoch: 62, Loss: 18.548707008361816,  L-Loss: 1.3469375967979431, C-Loss: 1.7875238060951233\n",
      "Epoch: 63, Loss: 18.5418643951416,  L-Loss: 1.3240444660186768, C-Loss: 1.7879841923713684\n",
      "Epoch: 64, Loss: 18.577869415283203,  L-Loss: 1.3142918944358826, C-Loss: 1.7920724153518677\n",
      "Epoch: 65, Loss: 18.54985523223877,  L-Loss: 1.294615924358368, C-Loss: 1.790254831314087\n",
      "Epoch: 66, Loss: 18.53304386138916,  L-Loss: 1.2847538590431213, C-Loss: 1.789066731929779\n",
      "Epoch: 67, Loss: 18.527311325073242,  L-Loss: 1.2679569721221924, C-Loss: 1.7893332242965698\n",
      "Epoch: 68, Loss: 18.41793441772461,  L-Loss: 1.2505406141281128, C-Loss: 1.7792662978172302\n",
      "Epoch: 69, Loss: 18.503265380859375,  L-Loss: 1.2410719990730286, C-Loss: 1.7882729768753052\n",
      "Epoch: 70, Loss: 18.449122428894043,  L-Loss: 1.2346683740615845, C-Loss: 1.7831788659095764\n",
      "Epoch: 71, Loss: 18.45924472808838,  L-Loss: 1.2133808135986328, C-Loss: 1.785255491733551\n",
      "Epoch: 72, Loss: 18.490382194519043,  L-Loss: 1.2089247107505798, C-Loss: 1.788591980934143\n",
      "Epoch: 73, Loss: 18.373026847839355,  L-Loss: 1.1898704171180725, C-Loss: 1.777809202671051\n",
      "Epoch: 74, Loss: 18.476547241210938,  L-Loss: 1.181204915046692, C-Loss: 1.7885944247245789\n",
      "Epoch: 75, Loss: 18.49644374847412,  L-Loss: 1.1738845109939575, C-Loss: 1.7909501194953918\n",
      "Epoch: 76, Loss: 18.44582462310791,  L-Loss: 1.155716896057129, C-Loss: 1.786796510219574\n",
      "Epoch: 77, Loss: 18.444159507751465,  L-Loss: 1.1463783979415894, C-Loss: 1.787097156047821\n",
      "Epoch: 78, Loss: 18.364657402038574,  L-Loss: 1.133080780506134, C-Loss: 1.7798117399215698\n",
      "Epoch: 79, Loss: 18.464998245239258,  L-Loss: 1.1227424144744873, C-Loss: 1.7903627157211304\n",
      "Epoch: 80, Loss: 18.44404411315918,  L-Loss: 1.1049888134002686, C-Loss: 1.7891550660133362\n",
      "Epoch: 81, Loss: 18.29401397705078,  L-Loss: 1.0950475931167603, C-Loss: 1.7746490836143494\n",
      "Epoch: 82, Loss: 18.409372329711914,  L-Loss: 1.088050127029419, C-Loss: 1.7865347266197205\n",
      "Epoch: 83, Loss: 18.398134231567383,  L-Loss: 1.0794193744659424, C-Loss: 1.785842478275299\n",
      "Epoch: 84, Loss: 18.4351749420166,  L-Loss: 1.0637497305870056, C-Loss: 1.7903300523757935\n",
      "Epoch: 85, Loss: 18.390073776245117,  L-Loss: 1.0518401861190796, C-Loss: 1.7864152789115906\n",
      "Epoch: 86, Loss: 18.412328720092773,  L-Loss: 1.040584683418274, C-Loss: 1.7892036437988281\n",
      "Epoch: 87, Loss: 18.372063636779785,  L-Loss: 1.0301499366760254, C-Loss: 1.7856987118721008\n",
      "Epoch: 88, Loss: 18.41437816619873,  L-Loss: 1.0237366557121277, C-Loss: 1.790251076221466\n",
      "Epoch: 89, Loss: 18.38449001312256,  L-Loss: 1.0131764709949493, C-Loss: 1.7877901792526245\n",
      "Epoch: 90, Loss: 18.292842864990234,  L-Loss: 1.0019391775131226, C-Loss: 1.7791873216629028\n",
      "Epoch: 91, Loss: 18.349260330200195,  L-Loss: 0.9898414015769958, C-Loss: 1.7854338884353638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92, Loss: 18.38786506652832,  L-Loss: 0.986676424741745, C-Loss: 1.789452612400055\n",
      "Epoch: 93, Loss: 18.300519943237305,  L-Loss: 0.9740830361843109, C-Loss: 1.7813478112220764\n",
      "Epoch: 94, Loss: 18.308748245239258,  L-Loss: 0.9537104070186615, C-Loss: 1.783189296722412\n",
      "Epoch: 95, Loss: 18.334956169128418,  L-Loss: 0.9458479285240173, C-Loss: 1.7862032055854797\n",
      "Epoch: 96, Loss: 18.3779354095459,  L-Loss: 0.9386035799980164, C-Loss: 1.7908633351325989\n",
      "Epoch: 97, Loss: 18.352495193481445,  L-Loss: 0.9237476885318756, C-Loss: 1.7890621423721313\n",
      "Epoch: 98, Loss: 18.27671241760254,  L-Loss: 0.9159929752349854, C-Loss: 1.7818716168403625\n",
      "Epoch: 99, Loss: 18.278773307800293,  L-Loss: 0.9019007384777069, C-Loss: 1.7827823758125305\n",
      "Epoch: 100, Loss: 18.38811492919922,  L-Loss: 0.8998062312602997, C-Loss: 1.7938212752342224\n",
      "Epoch: 101, Loss: 18.4038724899292,  L-Loss: 0.8894714415073395, C-Loss: 1.7959136962890625\n",
      "Epoch: 102, Loss: 18.376789093017578,  L-Loss: 0.8797943592071533, C-Loss: 1.793689250946045\n",
      "Epoch: 103, Loss: 18.29786491394043,  L-Loss: 0.8686657547950745, C-Loss: 1.7863531708717346\n",
      "Epoch: 104, Loss: 18.32399559020996,  L-Loss: 0.8578810691833496, C-Loss: 1.7895054817199707\n",
      "Epoch: 105, Loss: 18.261320114135742,  L-Loss: 0.855109453201294, C-Loss: 1.7833764553070068\n",
      "Epoch: 106, Loss: 18.23333168029785,  L-Loss: 0.8406791388988495, C-Loss: 1.7812991738319397\n",
      "Epoch: 107, Loss: 18.302870750427246,  L-Loss: 0.8350943326950073, C-Loss: 1.788532316684723\n",
      "Epoch: 108, Loss: 18.33351993560791,  L-Loss: 0.8229465186595917, C-Loss: 1.7922046184539795\n",
      "Epoch: 109, Loss: 18.33340072631836,  L-Loss: 0.8148635625839233, C-Loss: 1.7925969362258911\n",
      "Epoch: 110, Loss: 18.370004653930664,  L-Loss: 0.8127476274967194, C-Loss: 1.7963629961013794\n",
      "Epoch: 111, Loss: 18.381921768188477,  L-Loss: 0.8027678728103638, C-Loss: 1.798053801059723\n",
      "Epoch: 112, Loss: 18.23989486694336,  L-Loss: 0.7977712154388428, C-Loss: 1.7841010093688965\n",
      "Epoch: 113, Loss: 18.32748031616211,  L-Loss: 0.7843190431594849, C-Loss: 1.7935320734977722\n",
      "Epoch: 114, Loss: 18.46065902709961,  L-Loss: 0.7748309969902039, C-Loss: 1.8073244094848633\n",
      "Epoch: 115, Loss: 18.28736114501953,  L-Loss: 0.769502192735672, C-Loss: 1.7902610898017883\n",
      "Epoch: 116, Loss: 18.325105667114258,  L-Loss: 0.7558006048202515, C-Loss: 1.794720470905304\n",
      "Epoch: 117, Loss: 18.21189594268799,  L-Loss: 0.749183714389801, C-Loss: 1.7837303280830383\n",
      "Epoch: 118, Loss: 18.38433265686035,  L-Loss: 0.7460281848907471, C-Loss: 1.8011319637298584\n",
      "Epoch: 119, Loss: 18.330034255981445,  L-Loss: 0.7367131114006042, C-Loss: 1.79616779088974\n",
      "Epoch: 120, Loss: 18.341992378234863,  L-Loss: 0.7321140170097351, C-Loss: 1.7975934743881226\n",
      "Epoch: 121, Loss: 18.278549194335938,  L-Loss: 0.7250564396381378, C-Loss: 1.791602075099945\n",
      "Epoch: 122, Loss: 18.367968559265137,  L-Loss: 0.7114059031009674, C-Loss: 1.8012266159057617\n",
      "Epoch: 123, Loss: 18.326647758483887,  L-Loss: 0.701215922832489, C-Loss: 1.7976039052009583\n",
      "Epoch: 124, Loss: 18.31965160369873,  L-Loss: 0.7002785801887512, C-Loss: 1.796951174736023\n",
      "Epoch: 125, Loss: 18.32229518890381,  L-Loss: 0.6888720393180847, C-Loss: 1.797785997390747\n",
      "Epoch: 126, Loss: 18.363755226135254,  L-Loss: 0.6823364198207855, C-Loss: 1.8022586703300476\n",
      "Epoch: 127, Loss: 18.297088623046875,  L-Loss: 0.6807264983654022, C-Loss: 1.7956724762916565\n",
      "Epoch: 128, Loss: 18.360651969909668,  L-Loss: 0.6680542528629303, C-Loss: 1.802662432193756\n",
      "Epoch: 129, Loss: 18.26509189605713,  L-Loss: 0.6619307100772858, C-Loss: 1.793412685394287\n",
      "Epoch: 130, Loss: 18.368584632873535,  L-Loss: 0.6537701189517975, C-Loss: 1.8041699528694153\n",
      "Epoch: 131, Loss: 18.274765968322754,  L-Loss: 0.6494715809822083, C-Loss: 1.7950029969215393\n",
      "Epoch: 132, Loss: 18.20337200164795,  L-Loss: 0.6395841538906097, C-Loss: 1.78835791349411\n",
      "Epoch: 133, Loss: 18.357314109802246,  L-Loss: 0.630459725856781, C-Loss: 1.8042084574699402\n",
      "Epoch: 134, Loss: 18.36723041534424,  L-Loss: 0.6303147077560425, C-Loss: 1.805207371711731\n",
      "Epoch: 135, Loss: 18.321828842163086,  L-Loss: 0.6148010492324829, C-Loss: 1.801442801952362\n",
      "Epoch: 136, Loss: 18.23717975616455,  L-Loss: 0.6121749877929688, C-Loss: 1.7931092381477356\n",
      "Epoch: 137, Loss: 18.30559730529785,  L-Loss: 0.6023109257221222, C-Loss: 1.8004441261291504\n",
      "Epoch: 138, Loss: 18.218356132507324,  L-Loss: 0.5966835021972656, C-Loss: 1.7920014262199402\n",
      "Epoch: 139, Loss: 18.34147834777832,  L-Loss: 0.5929669141769409, C-Loss: 1.8044994473457336\n",
      "Epoch: 140, Loss: 18.318918228149414,  L-Loss: 0.5857099294662476, C-Loss: 1.8026063442230225\n",
      "Epoch: 141, Loss: 18.30491542816162,  L-Loss: 0.5772573053836823, C-Loss: 1.8016287088394165\n",
      "Epoch: 142, Loss: 18.278653144836426,  L-Loss: 0.5677102208137512, C-Loss: 1.7994797825813293\n",
      "Epoch: 143, Loss: 18.302021026611328,  L-Loss: 0.5659541189670563, C-Loss: 1.801904320716858\n",
      "Epoch: 144, Loss: 18.249926567077637,  L-Loss: 0.5646321773529053, C-Loss: 1.7967610955238342\n",
      "Epoch: 145, Loss: 18.24228286743164,  L-Loss: 0.549967348575592, C-Loss: 1.7967298030853271\n",
      "Epoch: 146, Loss: 18.414517402648926,  L-Loss: 0.5473688840866089, C-Loss: 1.8140832781791687\n",
      "Epoch: 147, Loss: 18.25501823425293,  L-Loss: 0.5490618050098419, C-Loss: 1.7980486750602722\n",
      "Epoch: 148, Loss: 18.31602382659912,  L-Loss: 0.5449913144111633, C-Loss: 1.8043527603149414\n",
      "Epoch: 149, Loss: 18.40072727203369,  L-Loss: 0.5358009338378906, C-Loss: 1.8132827281951904\n",
      "Epoch: 150, Loss: 18.307422637939453,  L-Loss: 0.5287573635578156, C-Loss: 1.8043044805526733\n",
      "Epoch: 151, Loss: 18.33669948577881,  L-Loss: 0.5205462574958801, C-Loss: 1.807642638683319\n",
      "Epoch: 152, Loss: 18.345134735107422,  L-Loss: 0.5174834430217743, C-Loss: 1.8086393475532532\n",
      "Epoch: 153, Loss: 18.38431167602539,  L-Loss: 0.5166462659835815, C-Loss: 1.8125988245010376\n",
      "Epoch: 154, Loss: 18.329605102539062,  L-Loss: 0.513839989900589, C-Loss: 1.8072683811187744\n",
      "Epoch: 155, Loss: 18.348591804504395,  L-Loss: 0.5068705230951309, C-Loss: 1.8095155954360962\n",
      "Epoch: 156, Loss: 18.43391513824463,  L-Loss: 0.4984307736158371, C-Loss: 1.8184700012207031\n",
      "Epoch: 157, Loss: 18.41091251373291,  L-Loss: 0.4936874955892563, C-Loss: 1.816406786441803\n",
      "Epoch: 158, Loss: 18.416956901550293,  L-Loss: 0.4862777590751648, C-Loss: 1.8173816800117493\n",
      "Epoch: 159, Loss: 18.408536911010742,  L-Loss: 0.4778044819831848, C-Loss: 1.8169634938240051\n",
      "Epoch: 160, Loss: 18.389453887939453,  L-Loss: 0.4712997376918793, C-Loss: 1.8153803944587708\n",
      "Epoch: 161, Loss: 18.414440155029297,  L-Loss: 0.47187843918800354, C-Loss: 1.8178500533103943\n",
      "Epoch: 162, Loss: 18.432663917541504,  L-Loss: 0.46313460171222687, C-Loss: 1.8201096653938293\n",
      "Epoch: 163, Loss: 18.37100887298584,  L-Loss: 0.45555104315280914, C-Loss: 1.814323365688324\n",
      "Epoch: 164, Loss: 18.45847225189209,  L-Loss: 0.45874379575252533, C-Loss: 1.8229100704193115\n",
      "Epoch: 165, Loss: 18.3605318069458,  L-Loss: 0.45400986075401306, C-Loss: 1.8133527040481567\n",
      "Epoch: 166, Loss: 18.42854118347168,  L-Loss: 0.44672542810440063, C-Loss: 1.8205177783966064\n",
      "Epoch: 167, Loss: 18.37172794342041,  L-Loss: 0.44306203722953796, C-Loss: 1.81501966714859\n",
      "Epoch: 168, Loss: 18.415328979492188,  L-Loss: 0.44008028507232666, C-Loss: 1.8195288181304932\n",
      "Epoch: 169, Loss: 18.481552124023438,  L-Loss: 0.4445362240076065, C-Loss: 1.8259283900260925\n",
      "Epoch: 170, Loss: 18.355327606201172,  L-Loss: 0.43528448045253754, C-Loss: 1.813768446445465\n",
      "Epoch: 171, Loss: 18.341103553771973,  L-Loss: 0.428522527217865, C-Loss: 1.812684178352356\n",
      "Epoch: 172, Loss: 18.4721622467041,  L-Loss: 0.42588962614536285, C-Loss: 1.8259217143058777\n",
      "Epoch: 173, Loss: 18.529394149780273,  L-Loss: 0.4242391139268875, C-Loss: 1.8317274451255798\n",
      "Epoch: 174, Loss: 18.568596839904785,  L-Loss: 0.4138495475053787, C-Loss: 1.8361672759056091\n",
      "Epoch: 175, Loss: 18.407343864440918,  L-Loss: 0.4073430001735687, C-Loss: 1.8203672170639038\n",
      "Epoch: 176, Loss: 18.497224807739258,  L-Loss: 0.4128088057041168, C-Loss: 1.8290820121765137\n",
      "Epoch: 177, Loss: 18.495038986206055,  L-Loss: 0.4036872237920761, C-Loss: 1.8293195962905884\n",
      "Epoch: 178, Loss: 18.438987731933594,  L-Loss: 0.3966579735279083, C-Loss: 1.824065923690796\n",
      "Epoch: 179, Loss: 18.354604721069336,  L-Loss: 0.3932635337114334, C-Loss: 1.8157972693443298\n",
      "Epoch: 180, Loss: 18.392515182495117,  L-Loss: 0.39089153707027435, C-Loss: 1.819706916809082\n",
      "Epoch: 181, Loss: 18.38718891143799,  L-Loss: 0.382711723446846, C-Loss: 1.819583237171173\n",
      "Out of patience at epoch 181\n",
      "Guardando mejor modelo en  ../models/C2AE_alexnet_retrained/base/1/26L.pth\n",
      "HS fold 1: 0.0512\n",
      "Predicciones guardadas en ../outputs/C2AE_alexnet_retrained/base/26L/1/predictions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando top_labels previamente generados para 26 labels\n",
      "Starting training!\n",
      "Epoch: 0, Loss: 22.81222629547119,  L-Loss: 7.175246477127075, C-Loss: 1.9224602580070496\n",
      "Epoch: 1, Loss: 20.716912269592285,  L-Loss: 3.216932535171509, C-Loss: 1.910844624042511\n",
      "Epoch: 2, Loss: 20.10086154937744,  L-Loss: 2.2094749212265015, C-Loss: 1.8996124267578125\n",
      "Epoch: 3, Loss: 19.721104621887207,  L-Loss: 1.720454752445221, C-Loss: 1.8860876560211182\n",
      "Epoch: 4, Loss: 19.497295379638672,  L-Loss: 1.595693290233612, C-Loss: 1.8699448704719543\n",
      "Epoch: 5, Loss: 19.356765747070312,  L-Loss: 1.631691575050354, C-Loss: 1.8540920615196228\n",
      "Epoch: 6, Loss: 19.221853256225586,  L-Loss: 1.682460069656372, C-Loss: 1.8380622863769531\n",
      "Epoch: 7, Loss: 19.114809036254883,  L-Loss: 1.6582016348838806, C-Loss: 1.8285707831382751\n",
      "Epoch: 8, Loss: 19.058125495910645,  L-Loss: 1.6395925283432007, C-Loss: 1.8238328695297241\n",
      "Epoch: 9, Loss: 18.97571849822998,  L-Loss: 1.6185225248336792, C-Loss: 1.816645860671997\n",
      "Epoch: 10, Loss: 18.940908432006836,  L-Loss: 1.6066888570785522, C-Loss: 1.8137564659118652\n",
      "Epoch: 11, Loss: 18.922842979431152,  L-Loss: 1.6173149943351746, C-Loss: 1.81141859292984\n",
      "Epoch: 12, Loss: 18.918185234069824,  L-Loss: 1.6311314702033997, C-Loss: 1.8102619051933289\n",
      "Epoch: 13, Loss: 18.91315746307373,  L-Loss: 1.6359428763389587, C-Loss: 1.8095185160636902\n",
      "Epoch: 14, Loss: 18.898829460144043,  L-Loss: 1.6412558555603027, C-Loss: 1.8078200817108154\n",
      "Epoch: 15, Loss: 18.919493675231934,  L-Loss: 1.652558147907257, C-Loss: 1.809321403503418\n",
      "Epoch: 16, Loss: 18.89712619781494,  L-Loss: 1.6470592021942139, C-Loss: 1.807359755039215\n",
      "Epoch: 17, Loss: 18.896434783935547,  L-Loss: 1.6657928824424744, C-Loss: 1.806353747844696\n",
      "Epoch: 18, Loss: 18.9015531539917,  L-Loss: 1.669350504875183, C-Loss: 1.806687831878662\n",
      "Epoch: 19, Loss: 18.90680980682373,  L-Loss: 1.6740549206733704, C-Loss: 1.8069782257080078\n",
      "Epoch: 20, Loss: 18.89997673034668,  L-Loss: 1.6588773727416992, C-Loss: 1.807053804397583\n",
      "Epoch: 21, Loss: 18.903759956359863,  L-Loss: 1.6629323959350586, C-Loss: 1.8072293400764465\n",
      "Epoch: 22, Loss: 18.881871223449707,  L-Loss: 1.652317225933075, C-Loss: 1.8055712580680847\n",
      "Epoch: 23, Loss: 18.900615692138672,  L-Loss: 1.6693244576454163, C-Loss: 1.806595265865326\n",
      "Epoch: 24, Loss: 18.917689323425293,  L-Loss: 1.6901350021362305, C-Loss: 1.807262122631073\n",
      "Epoch: 25, Loss: 18.912158012390137,  L-Loss: 1.6912805438041687, C-Loss: 1.806651771068573\n",
      "Epoch: 26, Loss: 18.923672676086426,  L-Loss: 1.7135048508644104, C-Loss: 1.8066920638084412\n",
      "Epoch: 27, Loss: 18.931904792785645,  L-Loss: 1.7304994463920593, C-Loss: 1.8066654801368713\n",
      "Epoch: 28, Loss: 18.933512687683105,  L-Loss: 1.7197465300559998, C-Loss: 1.8073638677597046\n",
      "Epoch: 29, Loss: 18.873634338378906,  L-Loss: 1.734147846698761, C-Loss: 1.800656020641327\n",
      "Epoch: 30, Loss: 18.911117553710938,  L-Loss: 1.7379023432731628, C-Loss: 1.8042166233062744\n",
      "Epoch: 31, Loss: 18.899629592895508,  L-Loss: 1.7365504503250122, C-Loss: 1.803135335445404\n",
      "Epoch: 32, Loss: 18.899150848388672,  L-Loss: 1.7439283728599548, C-Loss: 1.802718698978424\n",
      "Epoch: 33, Loss: 18.92039966583252,  L-Loss: 1.7400842308998108, C-Loss: 1.8050358295440674\n",
      "Epoch: 34, Loss: 18.860474586486816,  L-Loss: 1.7303752303123474, C-Loss: 1.7995285987854004\n",
      "Epoch: 35, Loss: 18.87330913543701,  L-Loss: 1.737913429737091, C-Loss: 1.8004352450370789\n",
      "Epoch: 36, Loss: 18.880556106567383,  L-Loss: 1.6958104968070984, C-Loss: 1.8032651543617249\n",
      "Epoch: 37, Loss: 18.857925415039062,  L-Loss: 1.6996630430221558, C-Loss: 1.800809383392334\n",
      "Epoch: 38, Loss: 18.7945556640625,  L-Loss: 1.6898985505104065, C-Loss: 1.794960618019104\n",
      "Epoch: 39, Loss: 18.794172286987305,  L-Loss: 1.6705582737922668, C-Loss: 1.7958893775939941\n",
      "Epoch: 40, Loss: 18.75385570526123,  L-Loss: 1.6494773626327515, C-Loss: 1.79291170835495\n",
      "Epoch: 41, Loss: 18.724804878234863,  L-Loss: 1.6385021805763245, C-Loss: 1.7905554175376892\n",
      "Epoch: 42, Loss: 18.67982006072998,  L-Loss: 1.625003457069397, C-Loss: 1.786731779575348\n",
      "Epoch: 43, Loss: 18.71694564819336,  L-Loss: 1.614774465560913, C-Loss: 1.7909557819366455\n",
      "Epoch: 44, Loss: 18.675609588623047,  L-Loss: 1.596194088459015, C-Loss: 1.7877511978149414\n",
      "Epoch: 45, Loss: 18.674147605895996,  L-Loss: 1.5809913277626038, C-Loss: 1.788365125656128\n",
      "Epoch: 46, Loss: 18.653969764709473,  L-Loss: 1.5611008405685425, C-Loss: 1.7873419523239136\n",
      "Epoch: 47, Loss: 18.643141746520996,  L-Loss: 1.5500138998031616, C-Loss: 1.7868133783340454\n",
      "Epoch: 48, Loss: 18.61697769165039,  L-Loss: 1.5371842980384827, C-Loss: 1.784838616847992\n",
      "Epoch: 49, Loss: 18.583520889282227,  L-Loss: 1.5333722233772278, C-Loss: 1.7816834449768066\n",
      "Epoch: 50, Loss: 18.47843360900879,  L-Loss: 1.5090091228485107, C-Loss: 1.772392988204956\n",
      "Epoch: 51, Loss: 18.57456111907959,  L-Loss: 1.5057122111320496, C-Loss: 1.7821704149246216\n",
      "Epoch: 52, Loss: 18.519777297973633,  L-Loss: 1.4873103499412537, C-Loss: 1.7776121497154236\n",
      "Epoch: 53, Loss: 18.521032333374023,  L-Loss: 1.471846580505371, C-Loss: 1.778510868549347\n",
      "Epoch: 54, Loss: 18.525907516479492,  L-Loss: 1.458316445350647, C-Loss: 1.7796748876571655\n",
      "Epoch: 55, Loss: 18.471285820007324,  L-Loss: 1.4382990002632141, C-Loss: 1.775213599205017\n",
      "Epoch: 56, Loss: 18.436673164367676,  L-Loss: 1.4240764379501343, C-Loss: 1.7724635004997253\n",
      "Epoch: 57, Loss: 18.410056114196777,  L-Loss: 1.4257992506027222, C-Loss: 1.769715666770935\n",
      "Epoch: 58, Loss: 18.35267162322998,  L-Loss: 1.4112622141838074, C-Loss: 1.7647040486335754\n",
      "Epoch: 59, Loss: 18.368348121643066,  L-Loss: 1.4020018577575684, C-Loss: 1.7667346596717834\n",
      "Epoch: 60, Loss: 18.41543197631836,  L-Loss: 1.3964928984642029, C-Loss: 1.7717185616493225\n",
      "Epoch: 61, Loss: 18.325551986694336,  L-Loss: 1.3766780495643616, C-Loss: 1.7637213468551636\n",
      "Epoch: 62, Loss: 18.22623062133789,  L-Loss: 1.3873158693313599, C-Loss: 1.7532572746276855\n",
      "Epoch: 63, Loss: 18.331223487854004,  L-Loss: 1.3776699900627136, C-Loss: 1.764238953590393\n",
      "Epoch: 64, Loss: 18.347283363342285,  L-Loss: 1.351975440979004, C-Loss: 1.7671296000480652\n",
      "Epoch: 65, Loss: 18.29923439025879,  L-Loss: 1.3525522947311401, C-Loss: 1.7622957825660706\n",
      "Epoch: 66, Loss: 18.26726531982422,  L-Loss: 1.353578805923462, C-Loss: 1.759047508239746\n",
      "Epoch: 67, Loss: 18.230138778686523,  L-Loss: 1.3459797501564026, C-Loss: 1.7557148337364197\n",
      "Epoch: 68, Loss: 18.30972385406494,  L-Loss: 1.3379274606704712, C-Loss: 1.764076054096222\n",
      "Epoch: 69, Loss: 18.259037971496582,  L-Loss: 1.3207025527954102, C-Loss: 1.7598687410354614\n",
      "Epoch: 70, Loss: 18.208444595336914,  L-Loss: 1.3181790709495544, C-Loss: 1.7549354434013367\n",
      "Epoch: 71, Loss: 18.203275680541992,  L-Loss: 1.3055923581123352, C-Loss: 1.7550479769706726\n",
      "Epoch: 72, Loss: 18.227981567382812,  L-Loss: 1.301177978515625, C-Loss: 1.757739245891571\n",
      "Epoch: 73, Loss: 18.244565963745117,  L-Loss: 1.2892217636108398, C-Loss: 1.759995460510254\n",
      "Epoch: 74, Loss: 18.23904800415039,  L-Loss: 1.2872521877288818, C-Loss: 1.759542167186737\n",
      "Epoch: 75, Loss: 18.156967163085938,  L-Loss: 1.276485562324524, C-Loss: 1.751872479915619\n",
      "Epoch: 76, Loss: 18.194849014282227,  L-Loss: 1.2592847347259521, C-Loss: 1.7565206289291382\n",
      "Epoch: 77, Loss: 18.179648399353027,  L-Loss: 1.251330554485321, C-Loss: 1.7553982734680176\n",
      "Epoch: 78, Loss: 18.139013290405273,  L-Loss: 1.2426002025604248, C-Loss: 1.7517712712287903\n",
      "Epoch: 79, Loss: 18.25420570373535,  L-Loss: 1.2383425831794739, C-Loss: 1.7635034918785095\n",
      "Epoch: 80, Loss: 18.17770290374756,  L-Loss: 1.2253987789154053, C-Loss: 1.7565004229545593\n",
      "Epoch: 81, Loss: 18.21257209777832,  L-Loss: 1.2093457579612732, C-Loss: 1.760789930820465\n",
      "Epoch: 82, Loss: 18.237382888793945,  L-Loss: 1.1909258961677551, C-Loss: 1.7641920447349548\n",
      "Epoch: 83, Loss: 18.13235378265381,  L-Loss: 1.1861379742622375, C-Loss: 1.7539284229278564\n",
      "Epoch: 84, Loss: 18.08549690246582,  L-Loss: 1.179368019104004, C-Loss: 1.7495812773704529\n",
      "Epoch: 85, Loss: 18.219868659973145,  L-Loss: 1.1691202521324158, C-Loss: 1.763530969619751\n",
      "Epoch: 86, Loss: 18.16863441467285,  L-Loss: 1.1544780731201172, C-Loss: 1.759139597415924\n",
      "Epoch: 87, Loss: 18.15452289581299,  L-Loss: 1.1440649032592773, C-Loss: 1.758249044418335\n",
      "Epoch: 88, Loss: 18.147825241088867,  L-Loss: 1.1352627873420715, C-Loss: 1.7580193281173706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89, Loss: 18.114069938659668,  L-Loss: 1.121392011642456, C-Loss: 1.7553372979164124\n",
      "Epoch: 90, Loss: 18.185181617736816,  L-Loss: 1.1187732815742493, C-Loss: 1.7625795006752014\n",
      "Epoch: 91, Loss: 17.99306869506836,  L-Loss: 1.113682508468628, C-Loss: 1.7436226606369019\n",
      "Epoch: 92, Loss: 17.996280670166016,  L-Loss: 1.0964971780776978, C-Loss: 1.7448033094406128\n",
      "Epoch: 93, Loss: 18.106996536254883,  L-Loss: 1.0790977478027344, C-Loss: 1.7567447423934937\n",
      "Epoch: 94, Loss: 18.170140266418457,  L-Loss: 1.0794628858566284, C-Loss: 1.7630409002304077\n",
      "Epoch: 95, Loss: 18.154702186584473,  L-Loss: 1.0684436559677124, C-Loss: 1.762048065662384\n",
      "Epoch: 96, Loss: 18.151122093200684,  L-Loss: 1.0593541264533997, C-Loss: 1.7621446251869202\n",
      "Epoch: 97, Loss: 18.114215850830078,  L-Loss: 1.0521060228347778, C-Loss: 1.7588163018226624\n",
      "Epoch: 98, Loss: 18.16058921813965,  L-Loss: 1.0433353781700134, C-Loss: 1.7638922333717346\n",
      "Epoch: 99, Loss: 18.146955490112305,  L-Loss: 1.0303648710250854, C-Loss: 1.7631772756576538\n",
      "Epoch: 100, Loss: 18.10923671722412,  L-Loss: 1.0231097340583801, C-Loss: 1.7597682476043701\n",
      "Epoch: 101, Loss: 18.045296669006348,  L-Loss: 1.0020962953567505, C-Loss: 1.754424810409546\n",
      "Epoch: 102, Loss: 18.079482078552246,  L-Loss: 1.0048139691352844, C-Loss: 1.7577075362205505\n",
      "Epoch: 103, Loss: 18.149746894836426,  L-Loss: 0.9927761256694794, C-Loss: 1.7653359174728394\n",
      "Epoch: 104, Loss: 18.132441520690918,  L-Loss: 0.9777550995349884, C-Loss: 1.7643564343452454\n",
      "Epoch: 105, Loss: 18.09940528869629,  L-Loss: 0.966734766960144, C-Loss: 1.7616038918495178\n",
      "Epoch: 106, Loss: 18.144055366516113,  L-Loss: 0.9544780552387238, C-Loss: 1.766681730747223\n",
      "Epoch: 107, Loss: 18.07817554473877,  L-Loss: 0.943747878074646, C-Loss: 1.760630190372467\n",
      "Epoch: 108, Loss: 18.108580589294434,  L-Loss: 0.9372961521148682, C-Loss: 1.7639933228492737\n",
      "Epoch: 109, Loss: 18.151793479919434,  L-Loss: 0.9256055951118469, C-Loss: 1.7688990831375122\n",
      "Epoch: 110, Loss: 18.15458393096924,  L-Loss: 0.9145943522453308, C-Loss: 1.769728660583496\n",
      "Epoch: 111, Loss: 18.03159523010254,  L-Loss: 0.9064500331878662, C-Loss: 1.7578369975090027\n",
      "Epoch: 112, Loss: 18.058897018432617,  L-Loss: 0.8914109468460083, C-Loss: 1.7613191604614258\n",
      "Epoch: 113, Loss: 18.155478477478027,  L-Loss: 0.8815639317035675, C-Loss: 1.7714697122573853\n",
      "Epoch: 114, Loss: 18.11299705505371,  L-Loss: 0.8682834506034851, C-Loss: 1.767885446548462\n",
      "Epoch: 115, Loss: 18.16384792327881,  L-Loss: 0.8613259196281433, C-Loss: 1.7733185291290283\n",
      "Epoch: 116, Loss: 18.149627685546875,  L-Loss: 0.8525808453559875, C-Loss: 1.7723336815834045\n",
      "Epoch: 117, Loss: 18.108360290527344,  L-Loss: 0.8367333710193634, C-Loss: 1.7689993381500244\n",
      "Epoch: 118, Loss: 18.106548309326172,  L-Loss: 0.8322598040103912, C-Loss: 1.769041895866394\n",
      "Epoch: 119, Loss: 18.109078407287598,  L-Loss: 0.8220327198505402, C-Loss: 1.7698062658309937\n",
      "Epoch: 120, Loss: 18.256461143493652,  L-Loss: 0.821089506149292, C-Loss: 1.7845916152000427\n",
      "Epoch: 121, Loss: 18.166275024414062,  L-Loss: 0.8035804033279419, C-Loss: 1.7764484882354736\n",
      "Epoch: 122, Loss: 18.09564971923828,  L-Loss: 0.8013589382171631, C-Loss: 1.7694969773292542\n",
      "Epoch: 123, Loss: 18.098417282104492,  L-Loss: 0.7868596911430359, C-Loss: 1.7704987525939941\n",
      "Epoch: 124, Loss: 18.136887550354004,  L-Loss: 0.7750519812107086, C-Loss: 1.7749361395835876\n",
      "Epoch: 125, Loss: 18.117494583129883,  L-Loss: 0.7648857831954956, C-Loss: 1.77350515127182\n",
      "Epoch: 126, Loss: 18.076645851135254,  L-Loss: 0.7628126740455627, C-Loss: 1.7695239186286926\n",
      "Epoch: 127, Loss: 18.1133394241333,  L-Loss: 0.7513721883296967, C-Loss: 1.7737652659416199\n",
      "Epoch: 128, Loss: 18.053730010986328,  L-Loss: 0.7379134893417358, C-Loss: 1.7684772610664368\n",
      "Epoch: 129, Loss: 17.9841890335083,  L-Loss: 0.7387443780899048, C-Loss: 1.7614815831184387\n",
      "Epoch: 130, Loss: 18.044377326965332,  L-Loss: 0.7269745767116547, C-Loss: 1.7680889964103699\n",
      "Epoch: 131, Loss: 18.06082820892334,  L-Loss: 0.7326332032680511, C-Loss: 1.769451081752777\n",
      "Epoch: 132, Loss: 18.056777000427246,  L-Loss: 0.7110405266284943, C-Loss: 1.770125687122345\n",
      "Epoch: 133, Loss: 18.047688484191895,  L-Loss: 0.6978444159030914, C-Loss: 1.7698765993118286\n",
      "Epoch: 134, Loss: 18.071949005126953,  L-Loss: 0.6948980391025543, C-Loss: 1.7724499106407166\n",
      "Epoch: 135, Loss: 18.04281234741211,  L-Loss: 0.6866489946842194, C-Loss: 1.7699488997459412\n",
      "Epoch: 136, Loss: 17.977407455444336,  L-Loss: 0.6741461753845215, C-Loss: 1.7640333771705627\n",
      "Epoch: 137, Loss: 18.07994270324707,  L-Loss: 0.6703983843326569, C-Loss: 1.7744742631912231\n",
      "Epoch: 138, Loss: 17.963502883911133,  L-Loss: 0.6617275774478912, C-Loss: 1.7632638812065125\n",
      "Epoch: 139, Loss: 17.97047710418701,  L-Loss: 0.6567864716053009, C-Loss: 1.7642083764076233\n",
      "Epoch: 140, Loss: 18.09032917022705,  L-Loss: 0.6536622643470764, C-Loss: 1.7763497829437256\n",
      "Epoch: 141, Loss: 17.913101196289062,  L-Loss: 0.6381393373012543, C-Loss: 1.7594030499458313\n",
      "Epoch: 142, Loss: 18.038429260253906,  L-Loss: 0.6313891112804413, C-Loss: 1.7722734808921814\n",
      "Epoch: 143, Loss: 18.055742263793945,  L-Loss: 0.6378818154335022, C-Loss: 1.7736800909042358\n",
      "Epoch: 144, Loss: 18.046245574951172,  L-Loss: 0.6206512451171875, C-Loss: 1.7735920548439026\n",
      "Epoch: 145, Loss: 18.072969436645508,  L-Loss: 0.6169480383396149, C-Loss: 1.7764495611190796\n",
      "Epoch: 146, Loss: 18.013710021972656,  L-Loss: 0.6077783405780792, C-Loss: 1.7709820866584778\n",
      "Epoch: 147, Loss: 18.057732582092285,  L-Loss: 0.6147986948490143, C-Loss: 1.7750332951545715\n",
      "Epoch: 148, Loss: 17.990657806396484,  L-Loss: 0.589852511882782, C-Loss: 1.769573152065277\n",
      "Epoch: 149, Loss: 18.016940116882324,  L-Loss: 0.5886023342609406, C-Loss: 1.7722638845443726\n",
      "Epoch: 150, Loss: 18.02710247039795,  L-Loss: 0.5810375511646271, C-Loss: 1.7736583948135376\n",
      "Epoch: 151, Loss: 17.995712280273438,  L-Loss: 0.581934779882431, C-Loss: 1.770474374294281\n",
      "Epoch: 152, Loss: 18.005988121032715,  L-Loss: 0.5706398487091064, C-Loss: 1.77206689119339\n",
      "Epoch: 153, Loss: 17.989018440246582,  L-Loss: 0.5689167082309723, C-Loss: 1.7704560160636902\n",
      "Epoch: 154, Loss: 18.057412147521973,  L-Loss: 0.5587731599807739, C-Loss: 1.7778025269508362\n",
      "Epoch: 155, Loss: 17.93564224243164,  L-Loss: 0.5433495342731476, C-Loss: 1.766396701335907\n",
      "Epoch: 156, Loss: 17.97245502471924,  L-Loss: 0.546939492225647, C-Loss: 1.7698984742164612\n",
      "Epoch: 157, Loss: 17.975647926330566,  L-Loss: 0.5427103042602539, C-Loss: 1.770429253578186\n",
      "Epoch: 158, Loss: 18.001358032226562,  L-Loss: 0.5309510231018066, C-Loss: 1.773588240146637\n",
      "Epoch: 159, Loss: 17.99043369293213,  L-Loss: 0.5253939628601074, C-Loss: 1.7727736234664917\n",
      "Epoch: 160, Loss: 18.027716636657715,  L-Loss: 0.5232873558998108, C-Loss: 1.7766072750091553\n",
      "Epoch: 161, Loss: 17.99104881286621,  L-Loss: 0.5142216086387634, C-Loss: 1.7733937501907349\n",
      "Epoch: 162, Loss: 17.9631929397583,  L-Loss: 0.5110825896263123, C-Loss: 1.770765244960785\n",
      "Epoch: 163, Loss: 17.9306640625,  L-Loss: 0.502348780632019, C-Loss: 1.7679489850997925\n",
      "Epoch: 164, Loss: 17.95343780517578,  L-Loss: 0.4937027245759964, C-Loss: 1.7706586122512817\n",
      "Epoch: 165, Loss: 17.866488456726074,  L-Loss: 0.4984452724456787, C-Loss: 1.7617264986038208\n",
      "Epoch: 166, Loss: 17.968185424804688,  L-Loss: 0.49846671521663666, C-Loss: 1.7718952298164368\n",
      "Epoch: 167, Loss: 17.969555854797363,  L-Loss: 0.4795605391263962, C-Loss: 1.7729775309562683\n",
      "Epoch: 168, Loss: 18.037667274475098,  L-Loss: 0.4795903265476227, C-Loss: 1.7797872424125671\n",
      "Epoch: 169, Loss: 17.967387199401855,  L-Loss: 0.46807996928691864, C-Loss: 1.773334801197052\n",
      "Epoch: 170, Loss: 17.937186241149902,  L-Loss: 0.4642718434333801, C-Loss: 1.7705050110816956\n",
      "Epoch: 171, Loss: 17.870906829833984,  L-Loss: 0.46817760169506073, C-Loss: 1.7636818289756775\n",
      "Epoch: 172, Loss: 18.000064849853516,  L-Loss: 0.4591062515974045, C-Loss: 1.7770511507987976\n",
      "Epoch: 173, Loss: 17.901633262634277,  L-Loss: 0.4481358528137207, C-Loss: 1.7677565813064575\n",
      "Epoch: 174, Loss: 17.9635066986084,  L-Loss: 0.4461849480867386, C-Loss: 1.7740413546562195\n",
      "Epoch: 175, Loss: 17.920750617980957,  L-Loss: 0.4398932009935379, C-Loss: 1.7700803875923157\n",
      "Epoch: 176, Loss: 17.939366340637207,  L-Loss: 0.4399636685848236, C-Loss: 1.771938443183899\n",
      "Epoch: 177, Loss: 17.936074256896973,  L-Loss: 0.42530328035354614, C-Loss: 1.7723422646522522\n",
      "Epoch: 178, Loss: 17.942874908447266,  L-Loss: 0.43059229850769043, C-Loss: 1.7727579474449158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 179, Loss: 17.88291072845459,  L-Loss: 0.42560234665870667, C-Loss: 1.7670109272003174\n",
      "Epoch: 180, Loss: 17.946958541870117,  L-Loss: 0.4331011325120926, C-Loss: 1.7730408310890198\n",
      "Epoch: 181, Loss: 17.946699142456055,  L-Loss: 0.40850964188575745, C-Loss: 1.7742444276809692\n",
      "Epoch: 182, Loss: 17.969584465026855,  L-Loss: 0.4081389755010605, C-Loss: 1.7765514254570007\n",
      "Epoch: 183, Loss: 17.94773292541504,  L-Loss: 0.427892729640007, C-Loss: 1.773378610610962\n",
      "Epoch: 184, Loss: 17.868247032165527,  L-Loss: 0.39892056584358215, C-Loss: 1.7668787240982056\n",
      "Epoch: 185, Loss: 17.927908897399902,  L-Loss: 0.3897229731082916, C-Loss: 1.7733046412467957\n",
      "Epoch: 186, Loss: 17.87748622894287,  L-Loss: 0.3872162103652954, C-Loss: 1.7683878540992737\n",
      "Epoch: 187, Loss: 17.868918418884277,  L-Loss: 0.3807680904865265, C-Loss: 1.7678533792495728\n",
      "Epoch: 188, Loss: 17.89543342590332,  L-Loss: 0.3828299939632416, C-Loss: 1.7704018354415894\n",
      "Epoch: 189, Loss: 17.88808822631836,  L-Loss: 0.3774785250425339, C-Loss: 1.7699349522590637\n",
      "Epoch: 190, Loss: 17.84420394897461,  L-Loss: 0.3756560981273651, C-Loss: 1.7656375765800476\n",
      "Epoch: 191, Loss: 17.898186683654785,  L-Loss: 0.3714548349380493, C-Loss: 1.7712459564208984\n",
      "Out of patience at epoch 191\n",
      "Guardando mejor modelo en  ../models/C2AE_alexnet_retrained/base/2/26L.pth\n",
      "HS fold 2: 0.1639\n",
      "Predicciones guardadas en ../outputs/C2AE_alexnet_retrained/base/26L/2/predictions.csv\n",
      "Usando top_labels previamente generados para 26 labels\n",
      "Starting training!\n",
      "Epoch: 0, Loss: 21.698585510253906,  L-Loss: 5.239078044891357, C-Loss: 1.9079046249389648\n",
      "Epoch: 1, Loss: 20.322093963623047,  L-Loss: 2.7276248931884766, C-Loss: 1.895828127861023\n",
      "Epoch: 2, Loss: 19.773472785949707,  L-Loss: 1.9054358005523682, C-Loss: 1.8820754885673523\n",
      "Epoch: 3, Loss: 19.4196720123291,  L-Loss: 1.5137971639633179, C-Loss: 1.8662773966789246\n",
      "Epoch: 4, Loss: 19.192858695983887,  L-Loss: 1.4475970268249512, C-Loss: 1.8469060063362122\n",
      "Epoch: 5, Loss: 19.06011199951172,  L-Loss: 1.557273805141449, C-Loss: 1.8281474709510803\n",
      "Epoch: 6, Loss: 18.916720390319824,  L-Loss: 1.6338396072387695, C-Loss: 1.809980034828186\n",
      "Epoch: 7, Loss: 18.783747673034668,  L-Loss: 1.6352005004882812, C-Loss: 1.7966147065162659\n",
      "Epoch: 8, Loss: 18.65927791595459,  L-Loss: 1.6155028343200684, C-Loss: 1.7851526737213135\n",
      "Epoch: 9, Loss: 18.599902153015137,  L-Loss: 1.6108649969100952, C-Loss: 1.7794470191001892\n",
      "Epoch: 10, Loss: 18.550963401794434,  L-Loss: 1.622273325920105, C-Loss: 1.7739826440811157\n",
      "Epoch: 11, Loss: 18.52589225769043,  L-Loss: 1.6362228393554688, C-Loss: 1.7707780599594116\n",
      "Epoch: 12, Loss: 18.565929412841797,  L-Loss: 1.7060738801956177, C-Loss: 1.7712892293930054\n",
      "Epoch: 13, Loss: 18.548983573913574,  L-Loss: 1.7145779132843018, C-Loss: 1.7691694498062134\n",
      "Epoch: 14, Loss: 18.57242774963379,  L-Loss: 1.7623910903930664, C-Loss: 1.769123136997223\n",
      "Epoch: 15, Loss: 18.6134614944458,  L-Loss: 1.7881788611412048, C-Loss: 1.7719372510910034\n",
      "Epoch: 16, Loss: 18.64198398590088,  L-Loss: 1.8155221939086914, C-Loss: 1.7734223008155823\n",
      "Epoch: 17, Loss: 18.680728912353516,  L-Loss: 1.833594799041748, C-Loss: 1.7763931155204773\n",
      "Epoch: 18, Loss: 18.689783096313477,  L-Loss: 1.8420889377593994, C-Loss: 1.7768738865852356\n",
      "Epoch: 19, Loss: 18.720128059387207,  L-Loss: 1.842570424079895, C-Loss: 1.7798842191696167\n",
      "Epoch: 20, Loss: 18.720457077026367,  L-Loss: 1.8238845467567444, C-Loss: 1.780851423740387\n",
      "Epoch: 21, Loss: 18.761180877685547,  L-Loss: 1.8312210440635681, C-Loss: 1.784557044506073\n",
      "Epoch: 22, Loss: 18.774245262145996,  L-Loss: 1.8422042727470398, C-Loss: 1.7853143215179443\n",
      "Epoch: 23, Loss: 18.78630542755127,  L-Loss: 1.8143603205680847, C-Loss: 1.7879124879837036\n",
      "Epoch: 24, Loss: 18.814193725585938,  L-Loss: 1.8303183913230896, C-Loss: 1.789903461933136\n",
      "Epoch: 25, Loss: 18.810648918151855,  L-Loss: 1.8257870078086853, C-Loss: 1.789775550365448\n",
      "Epoch: 26, Loss: 18.909440994262695,  L-Loss: 1.833683967590332, C-Loss: 1.799259901046753\n",
      "Epoch: 27, Loss: 18.830756187438965,  L-Loss: 1.8113548159599304, C-Loss: 1.792507827281952\n",
      "Epoch: 28, Loss: 18.880942344665527,  L-Loss: 1.8177551627159119, C-Loss: 1.797206461429596\n",
      "Epoch: 29, Loss: 18.83225440979004,  L-Loss: 1.808930516242981, C-Loss: 1.7927788496017456\n",
      "Epoch: 30, Loss: 18.84907054901123,  L-Loss: 1.7917881608009338, C-Loss: 1.7953175902366638\n",
      "Epoch: 31, Loss: 18.85158061981201,  L-Loss: 1.7898292541503906, C-Loss: 1.7956666350364685\n",
      "Epoch: 32, Loss: 18.773365020751953,  L-Loss: 1.779436707496643, C-Loss: 1.7883647084236145\n",
      "Epoch: 33, Loss: 18.955490112304688,  L-Loss: 1.786031186580658, C-Loss: 1.8062474727630615\n",
      "Epoch: 34, Loss: 18.87594223022461,  L-Loss: 1.7657644748687744, C-Loss: 1.799306035041809\n",
      "Epoch: 35, Loss: 18.97556972503662,  L-Loss: 1.7923704981803894, C-Loss: 1.8079383373260498\n",
      "Epoch: 36, Loss: 18.9545259475708,  L-Loss: 1.776808261871338, C-Loss: 1.8066121935844421\n",
      "Epoch: 37, Loss: 18.855037689208984,  L-Loss: 1.7565464973449707, C-Loss: 1.79767644405365\n",
      "Epoch: 38, Loss: 18.95023822784424,  L-Loss: 1.7587462067604065, C-Loss: 1.8070865869522095\n",
      "Epoch: 39, Loss: 18.933735847473145,  L-Loss: 1.749000608921051, C-Loss: 1.8059235215187073\n",
      "Epoch: 40, Loss: 18.92423915863037,  L-Loss: 1.727887511253357, C-Loss: 1.8060294389724731\n",
      "Epoch: 41, Loss: 18.85181999206543,  L-Loss: 1.7164449095726013, C-Loss: 1.7993597984313965\n",
      "Epoch: 42, Loss: 18.867316246032715,  L-Loss: 1.7017765045166016, C-Loss: 1.8016427159309387\n",
      "Epoch: 43, Loss: 18.892563819885254,  L-Loss: 1.6867057085037231, C-Loss: 1.8049210906028748\n",
      "Epoch: 44, Loss: 18.937984466552734,  L-Loss: 1.6838245391845703, C-Loss: 1.809607207775116\n",
      "Epoch: 45, Loss: 18.890862464904785,  L-Loss: 1.6788024306297302, C-Loss: 1.805146038532257\n",
      "Epoch: 46, Loss: 18.901519775390625,  L-Loss: 1.6752457022666931, C-Loss: 1.8063896298408508\n",
      "Epoch: 47, Loss: 18.848957061767578,  L-Loss: 1.6435710787773132, C-Loss: 1.80271714925766\n",
      "Epoch: 48, Loss: 18.85039520263672,  L-Loss: 1.6271072030067444, C-Loss: 1.803684115409851\n",
      "Epoch: 49, Loss: 18.94647979736328,  L-Loss: 1.6251591444015503, C-Loss: 1.8133900165557861\n",
      "Epoch: 50, Loss: 18.89716625213623,  L-Loss: 1.606052041053772, C-Loss: 1.8094140887260437\n",
      "Epoch: 51, Loss: 18.909116744995117,  L-Loss: 1.5963877439498901, C-Loss: 1.81109219789505\n",
      "Epoch: 52, Loss: 18.853389739990234,  L-Loss: 1.5862091183662415, C-Loss: 1.8060285449028015\n",
      "Epoch: 53, Loss: 18.773483276367188,  L-Loss: 1.560508906841278, C-Loss: 1.7993229627609253\n",
      "Epoch: 54, Loss: 18.71370506286621,  L-Loss: 1.5472792387008667, C-Loss: 1.7940065264701843\n",
      "Epoch: 55, Loss: 18.819530487060547,  L-Loss: 1.5442435145378113, C-Loss: 1.8047409057617188\n",
      "Epoch: 56, Loss: 18.936872482299805,  L-Loss: 1.5272334814071655, C-Loss: 1.8173255920410156\n",
      "Epoch: 57, Loss: 18.847421646118164,  L-Loss: 1.5073533058166504, C-Loss: 1.8093745112419128\n",
      "Epoch: 58, Loss: 18.792677879333496,  L-Loss: 1.4927461743354797, C-Loss: 1.80463045835495\n",
      "Epoch: 59, Loss: 18.76053237915039,  L-Loss: 1.4689412713050842, C-Loss: 1.8026062846183777\n",
      "Epoch: 60, Loss: 18.77003574371338,  L-Loss: 1.452278196811676, C-Loss: 1.8043895959854126\n",
      "Epoch: 61, Loss: 18.697218894958496,  L-Loss: 1.4426069855690002, C-Loss: 1.7975915670394897\n",
      "Epoch: 62, Loss: 18.738677978515625,  L-Loss: 1.4322112798690796, C-Loss: 1.802257239818573\n",
      "Epoch: 63, Loss: 18.722363471984863,  L-Loss: 1.4089961051940918, C-Loss: 1.8017866611480713\n",
      "Epoch: 64, Loss: 18.672240257263184,  L-Loss: 1.4002388715744019, C-Loss: 1.7972121834754944\n",
      "Epoch: 65, Loss: 18.646414756774902,  L-Loss: 1.3725880980491638, C-Loss: 1.796012043952942\n",
      "Epoch: 66, Loss: 18.6002254486084,  L-Loss: 1.360624074935913, C-Loss: 1.7919912934303284\n",
      "Epoch: 67, Loss: 18.623608589172363,  L-Loss: 1.3574272990226746, C-Loss: 1.7944895029067993\n",
      "Epoch: 68, Loss: 18.584186553955078,  L-Loss: 1.3365339040756226, C-Loss: 1.7915920615196228\n",
      "Epoch: 69, Loss: 18.52385425567627,  L-Loss: 1.3223066926002502, C-Loss: 1.786270022392273\n",
      "Epoch: 70, Loss: 18.58939552307129,  L-Loss: 1.3078811764717102, C-Loss: 1.793545424938202\n",
      "Epoch: 71, Loss: 18.615553855895996,  L-Loss: 1.2952725291252136, C-Loss: 1.7967917323112488\n",
      "Epoch: 72, Loss: 18.541173934936523,  L-Loss: 1.2707170248031616, C-Loss: 1.7905815839767456\n",
      "Epoch: 73, Loss: 18.46901226043701,  L-Loss: 1.2535246014595032, C-Loss: 1.7842249870300293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74, Loss: 18.70551872253418,  L-Loss: 1.249152421951294, C-Loss: 1.8080942034721375\n",
      "Epoch: 75, Loss: 18.48904514312744,  L-Loss: 1.2270167469978333, C-Loss: 1.7875536680221558\n",
      "Epoch: 76, Loss: 18.48096466064453,  L-Loss: 1.2206991910934448, C-Loss: 1.7870614528656006\n",
      "Epoch: 77, Loss: 18.517043113708496,  L-Loss: 1.206244707107544, C-Loss: 1.7913921475410461\n",
      "Epoch: 78, Loss: 18.46927261352539,  L-Loss: 1.1867029070854187, C-Loss: 1.7875921130180359\n",
      "Epoch: 79, Loss: 18.40500259399414,  L-Loss: 1.1755149364471436, C-Loss: 1.7817245721817017\n",
      "Epoch: 80, Loss: 18.407859802246094,  L-Loss: 1.1616113781929016, C-Loss: 1.7827053666114807\n",
      "Epoch: 81, Loss: 18.364352226257324,  L-Loss: 1.1440190076828003, C-Loss: 1.7792342901229858\n",
      "Epoch: 82, Loss: 18.350729942321777,  L-Loss: 1.1317089200019836, C-Loss: 1.778487503528595\n",
      "Epoch: 83, Loss: 18.334680557250977,  L-Loss: 1.1191979050636292, C-Loss: 1.7775080800056458\n",
      "Epoch: 84, Loss: 18.36689853668213,  L-Loss: 1.1098197102546692, C-Loss: 1.7811988592147827\n",
      "Epoch: 85, Loss: 18.36450958251953,  L-Loss: 1.1001383662223816, C-Loss: 1.781444013118744\n",
      "Epoch: 86, Loss: 18.29665184020996,  L-Loss: 1.0763235092163086, C-Loss: 1.775848925113678\n",
      "Epoch: 87, Loss: 18.319214820861816,  L-Loss: 1.067764401435852, C-Loss: 1.7785332202911377\n",
      "Epoch: 88, Loss: 18.451003074645996,  L-Loss: 1.0569232106208801, C-Loss: 1.7922541499137878\n",
      "Epoch: 89, Loss: 18.443981170654297,  L-Loss: 1.0499969124794006, C-Loss: 1.7918981909751892\n",
      "Epoch: 90, Loss: 18.33234405517578,  L-Loss: 1.0315983295440674, C-Loss: 1.7816545963287354\n",
      "Epoch: 91, Loss: 18.382156372070312,  L-Loss: 1.0222544372081757, C-Loss: 1.7871029376983643\n",
      "Epoch: 92, Loss: 18.272737503051758,  L-Loss: 1.0095644891262054, C-Loss: 1.776795506477356\n",
      "Epoch: 93, Loss: 18.32685661315918,  L-Loss: 0.9954956769943237, C-Loss: 1.7829108238220215\n",
      "Epoch: 94, Loss: 18.336585998535156,  L-Loss: 0.9894613027572632, C-Loss: 1.7841854691505432\n",
      "Epoch: 95, Loss: 18.28797149658203,  L-Loss: 0.9727058410644531, C-Loss: 1.7801618576049805\n",
      "Epoch: 96, Loss: 18.28830051422119,  L-Loss: 0.9643122553825378, C-Loss: 1.78061443567276\n",
      "Epoch: 97, Loss: 18.293745040893555,  L-Loss: 0.9580327570438385, C-Loss: 1.78147292137146\n",
      "Epoch: 98, Loss: 18.23328971862793,  L-Loss: 0.9384572505950928, C-Loss: 1.776406168937683\n",
      "Epoch: 99, Loss: 18.338329315185547,  L-Loss: 0.9381978511810303, C-Loss: 1.7869230508804321\n",
      "Epoch: 100, Loss: 18.256196975708008,  L-Loss: 0.9190644919872284, C-Loss: 1.7796664237976074\n",
      "Epoch: 101, Loss: 18.31696891784668,  L-Loss: 0.9137212038040161, C-Loss: 1.7860108613967896\n",
      "Epoch: 102, Loss: 18.198044776916504,  L-Loss: 0.8969666361808777, C-Loss: 1.7749561071395874\n",
      "Epoch: 103, Loss: 18.31788730621338,  L-Loss: 0.8930160999298096, C-Loss: 1.7871379256248474\n",
      "Epoch: 104, Loss: 18.15546703338623,  L-Loss: 0.8782082498073578, C-Loss: 1.7716363072395325\n",
      "Epoch: 105, Loss: 18.26133632659912,  L-Loss: 0.8727258145809174, C-Loss: 1.7824972867965698\n",
      "Epoch: 106, Loss: 18.228821754455566,  L-Loss: 0.8592745661735535, C-Loss: 1.7799184322357178\n",
      "Epoch: 107, Loss: 18.30989646911621,  L-Loss: 0.8547283411026001, C-Loss: 1.788253128528595\n",
      "Epoch: 108, Loss: 18.13645362854004,  L-Loss: 0.8320907950401306, C-Loss: 1.7720409035682678\n",
      "Epoch: 109, Loss: 18.244303703308105,  L-Loss: 0.8340151607990265, C-Loss: 1.78272944688797\n",
      "Epoch: 110, Loss: 18.14785099029541,  L-Loss: 0.8184305429458618, C-Loss: 1.7738636136054993\n",
      "Epoch: 111, Loss: 18.189330101013184,  L-Loss: 0.8141172528266907, C-Loss: 1.778227150440216\n",
      "Epoch: 112, Loss: 18.218183517456055,  L-Loss: 0.8021791875362396, C-Loss: 1.7817094326019287\n",
      "Epoch: 113, Loss: 18.16413688659668,  L-Loss: 0.7921025156974792, C-Loss: 1.7768085598945618\n",
      "Epoch: 114, Loss: 18.249696731567383,  L-Loss: 0.7846930921077728, C-Loss: 1.785735011100769\n",
      "Out of patience at epoch 114\n",
      "Guardando mejor modelo en  ../models/C2AE_alexnet_retrained/base/3/26L.pth\n",
      "HS fold 3: 0.1477\n",
      "Predicciones guardadas en ../outputs/C2AE_alexnet_retrained/base/26L/3/predictions.csv\n",
      "HS Final:  0.1491\n",
      "F1 Final:  0.2052\n",
      "F2 Final:  0.1648\n",
      "1MR Final:  0.3698\n",
      "5MR Final:  0.0541\n"
     ]
    }
   ],
   "source": [
    "sum_f1 = 0\n",
    "sum_f2 = 0\n",
    "sum_recall = 0\n",
    "sum_precision = 0\n",
    "sum_acc = 0\n",
    "sum_hl = 0\n",
    "sum_emr = 0\n",
    "sum_hs = 0\n",
    "sum_mr1 = 0\n",
    "sum_mr2 = 0\n",
    "sum_mr3 = 0\n",
    "sum_mr4 = 0\n",
    "sum_mr5 = 0\n",
    "\n",
    "for i in range(0, K):\n",
    "    fold = Kfolds[i]\n",
    "    labels_dir = fold['labels_dir']\n",
    "    output_dir = fold['output_dir']\n",
    "    model_path = fold['model_path']\n",
    "    features_dir = fold['features_dir']\n",
    "    # Carga de top labels\n",
    "    train_labels = pd.read_json(os.path.join(labels_dir, 'augmented_train_df.json'), orient='index')\n",
    "    \n",
    "    if not os.path.isfile(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle')):\n",
    "        print(f\"Creando top_labels para {NUM_LABELS} labels\")\n",
    "        top_labels = pruner.filter_labels(train_labels)\n",
    "        pruner.set_top_labels(top_labels)\n",
    "        \n",
    "        save = input(f\"Se creará un archivo nuevo para {len(top_labels)} labels. Desea continuar? (y/n)\")\n",
    "        if save == \"y\":\n",
    "            with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'wb') as f:\n",
    "                pickle.dump(top_labels, f)\n",
    "            print(\"Top labels creado con éxito\")\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"No se logró cargar top_labels\")\n",
    "            \n",
    "    else: \n",
    "        print(f\"Usando top_labels previamente generados para {NUM_LABELS} labels\")\n",
    "        with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'rb') as f:\n",
    "            top_labels = pickle.load(f)\n",
    "\n",
    "    pruner = KunischPruner(NUM_LABELS)\n",
    "    pruner.set_top_labels(top_labels)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "\n",
    "\n",
    "    train_labels = pd.read_json(os.path.join(labels_dir, 'augmented_train_df.json'), orient='index')\n",
    "    train_labels = pruner.filter_df(train_labels)\n",
    "\n",
    "    train_x = pd.read_json(os.path.join(features_dir, 'augmented_train_df.json'), orient='index').values\n",
    "    train_y = train_labels.values\n",
    "\n",
    "    test_labels = pd.read_json(os.path.join(labels_dir, 'test_df.json'), orient='index')\n",
    "    test_labels = pruner.filter_df(test_labels)\n",
    "\n",
    "    test_x = pd.read_json(os.path.join(features_dir, 'test_df.json'), orient='index').values\n",
    "    test_y = test_labels.values\n",
    "        \n",
    "    train_dataset = TensorDataset(torch.tensor(train_x, \n",
    "                                               device=device, \n",
    "                                               dtype=torch.float),\n",
    "                                  torch.tensor(train_y, \n",
    "                                               device=device,\n",
    "                                               dtype=torch.float))\n",
    "    test_dataset = TensorDataset(torch.tensor(test_x, \n",
    "                                              device=device, \n",
    "                                              dtype=torch.float), \n",
    "                                 torch.tensor(test_y, \n",
    "                                              device=device, \n",
    "                                              dtype=torch.float))\n",
    "\n",
    "    #display(train_dataset[:][0].shape, train_dataset[:][1].shape, test_dataset[:][0].shape, test_dataset[:][1].shape)\n",
    "\n",
    "    # Training configs.\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    batch_size = BATCH_SIZE\n",
    "    lr = 0.0001\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # # Scene config\n",
    "    feat_dim = FEATURES_DIM\n",
    "    num_labels = len(top_labels)\n",
    "\n",
    "    # Scene models.\n",
    "    Fx_scene = Fx(feat_dim, fx_h_dim, fx_h_dim, latent_dim)\n",
    "    Fe_scene = Fe(num_labels, fe_h_dim, latent_dim)\n",
    "    Fd_scene = Fd(latent_dim, fd_h_dim, num_labels, fin_act=torch.sigmoid)\n",
    "\n",
    "    # Initializing net.\n",
    "    net = C2AE(Fx_scene, Fe_scene, Fd_scene, beta=0.5, alpha=10, emb_lambda=0.01, latent_dim=latent_dim, device=device)\n",
    "    net = net.to(device)\n",
    "\n",
    "\n",
    "    # Doing weight_decay here is eqiv to adding the L2 norm.\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    print(\"Starting training!\")\n",
    "    best_weights = None\n",
    "    best_loss = math.inf\n",
    "    patience = PATIENCE\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs+1): \n",
    "        # Training.\n",
    "        net.train()\n",
    "        loss_tracker = 0.0\n",
    "        latent_loss_tracker = 0.0\n",
    "        cor_loss_tracker = 0.0\n",
    "        for x, y in train_dataloader:\n",
    "            optimizer.zero_grad()      \n",
    "\n",
    "            # Pass x, y to network. Retrieve both encodings, and decoding of ys encoding.\n",
    "            fx_x, fe_y, fd_z = net(x, y)\n",
    "            # Calc loss.\n",
    "            l_loss, c_loss = net.losses(fx_x, fe_y, fd_z, y)\n",
    "            # Normalize losses by batch.\n",
    "            l_loss /= x.shape[0]\n",
    "            c_loss /= x.shape[0]\n",
    "            loss = net.beta*l_loss + net.alpha*c_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_tracker+=loss.item()\n",
    "            latent_loss_tracker+=l_loss.item()\n",
    "            cor_loss_tracker+=c_loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        net.eval()\n",
    "        loss_tracker = 0.0\n",
    "        latent_loss_tracker = 0.0\n",
    "        cor_loss_tracker = 0.0\n",
    "        acc_track = 0.0\n",
    "        for x, y in test_dataloader:\n",
    "            # evaluation only requires x. As its just Fd(Fx(x))\n",
    "            fx_x, fe_y = net.Fx(x), net.Fe(y)\n",
    "            fd_z = net.Fd(fx_x)\n",
    "\n",
    "            l_loss, c_loss = net.losses(fx_x, fe_y, fd_z, y)\n",
    "            # Normalize losses by batch.\n",
    "            l_loss /= x.shape[0]\n",
    "            c_loss /= x.shape[0]\n",
    "            loss = net.beta*l_loss + net.alpha*c_loss\n",
    "\n",
    "            latent_loss_tracker += l_loss.item()\n",
    "            cor_loss_tracker += c_loss.item()\n",
    "            loss_tracker += loss.item()\n",
    "            lab_preds = torch.round(net.Fd(net.Fx(x))).cpu().detach().numpy()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss_tracker},  L-Loss: {latent_loss_tracker}, C-Loss: {cor_loss_tracker}\")\n",
    "        if cor_loss_tracker < best_loss:\n",
    "            best_loss = cor_loss_tracker\n",
    "            best_weights = net.state_dict()\n",
    "            bad_epochs = 0\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs == patience:\n",
    "                print(f\"Out of patience at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    if SAVE:\n",
    "        print(\"Guardando mejor modelo en \", model_path)\n",
    "        torch.save(best_weights, model_path)\n",
    "\n",
    "    eval_net = load_model(C2AE, model_path, \n",
    "                          Fx=Fx_scene, Fe=Fe_scene, Fd=Fd_scene, device=device).to(device)\n",
    "\n",
    "    y_pred, y_true = get_predictions(net, [test_dataset], device)\n",
    "\n",
    "    metrics = KunischMetrics(y_true, y_pred)\n",
    "    sum_f1 += metrics.f1()\n",
    "    sum_f2 += metrics.f2()\n",
    "    sum_recall += metrics.recall()\n",
    "    sum_precision += metrics.precision()\n",
    "    sum_acc += metrics.acc()\n",
    "    sum_hl += metrics.hl()\n",
    "    sum_emr += metrics.emr()\n",
    "    sum_hs += metrics.hs()\n",
    "    sum_mr1 += metrics.mr1()\n",
    "    sum_mr2 += metrics.mr2()\n",
    "    sum_mr3 += metrics.mr3()\n",
    "    sum_mr4 += metrics.mr4()\n",
    "    sum_mr5 += metrics.mr5()\n",
    "\n",
    "    print(f\"HS fold {i}: {metrics.hs()}\")\n",
    "\n",
    "    if SAVE:\n",
    "        save_df = pd.DataFrame(y_pred)\n",
    "        save_df.to_csv(os.path.join(output_dir, 'predictions.csv'))\n",
    "        print(f\"Predicciones guardadas en {os.path.join(output_dir, 'predictions.csv')}\")\n",
    "\n",
    "        \n",
    "avg_f1 = round(sum_f1/K, 4)\n",
    "avg_f2 = round(sum_f2/K, 4)\n",
    "avg_recall = round(sum_recall/K, 4)\n",
    "avg_precision = round(sum_precision/K, 4)\n",
    "avg_acc = round(sum_acc/K, 4)\n",
    "avg_hl = round(sum_hl/K, 4)\n",
    "avg_emr = round(sum_emr/K, 4)\n",
    "avg_hs = round(sum_hs/K, 4)\n",
    "avg_mr1 = round(sum_mr1/K, 4)\n",
    "avg_mr2 = round(sum_mr2/K, 4)\n",
    "avg_mr3 = round(sum_mr3/K, 4)\n",
    "avg_mr4 = round(sum_mr4/K, 4)\n",
    "avg_mr5 = round(sum_mr5/K, 4)\n",
    "\n",
    "metadata = {\n",
    "'data_flags': data_flags,\n",
    "'patience': PATIENCE,\n",
    "'batch_size': BATCH_SIZE,\n",
    "'optimizer': (type (optimizer).__name__),\n",
    "'epochs': num_epochs,\n",
    "'num_labels': NUM_LABELS,\n",
    "'f1': avg_f1,\n",
    "'f2': avg_f2,\n",
    "'recall': avg_recall,\n",
    "'precision': avg_precision,\n",
    "'acc': avg_acc,\n",
    "'hl': avg_hl,\n",
    "'emr': avg_emr,\n",
    "'hs': avg_hs,\n",
    "'mr1': avg_mr1,\n",
    "'mr2': avg_mr2,\n",
    "'mr3': avg_mr3,\n",
    "'mr4': avg_mr4,\n",
    "'mr5': avg_mr5\n",
    "}\n",
    "\n",
    "print(\"HS Final: \", avg_hs)\n",
    "print(\"F1 Final: \", avg_f1)\n",
    "print(\"F2 Final: \", avg_f2)\n",
    "print(\"1MR Final: \", avg_mr1)\n",
    "print(\"5MR Final: \", avg_mr5)\n",
    "\n",
    "if SAVE:\n",
    "    metadf = pd.DataFrame.from_dict(metadata, orient='index')\n",
    "    # output_dir pero sin numero de fold\n",
    "    os.makedirs(os.path.join(root_dir, 'outputs', 'C2AE_alexnet', data_flags, exp_name), exist_ok=True)\n",
    "    metadf.to_csv(os.path.join(root_dir, \"outputs\", \"C2AE_alexnet\", data_flags, exp_name, 'metadata.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
