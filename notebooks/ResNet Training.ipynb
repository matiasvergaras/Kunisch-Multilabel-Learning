{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpIfCtDGJ5q-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# Resnet Retraining\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "El objetivo de este notebook es entrenar arquitecturas de ResNet preentrenadas sobre la tarea de predecir el capítulo de cada patrón."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2Z2do8XKiAQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder_path = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IgrQgam9KgnN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np, scipy.io\n",
    "import argparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuración de dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CUDA_ID = 0\n",
    "\n",
    "device = torch.device(f'cuda:{CUDA_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuración de datos y modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Flags para los datos sintéticos\n",
    "# Cada flag está asociada a una o más funciones de data augmentation.\n",
    "# Los datos deben existir previamente \n",
    "# (se generan a partir del notebook split and augmentation)\n",
    "USE_RN50 = True\n",
    "DS_FLAGS = ['ref', 'rot', 'crop', 'elastic']\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "                \n",
    "# Las flags crop, randaug, elastic y gausblur \n",
    "# se pueden aplicar más de una vez c/u. \n",
    "# (si no están en DS_FLAGS, serán ignoradas).\n",
    "CROP_TIMES = 1\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1\n",
    "\n",
    "# Guardar el modelo cada SAVE_EACH épocas\n",
    "SAVE_EACH = -1 # -1 to save only the best model\n",
    "TRAINING_EPOCHS = 300\n",
    "K = 4\n",
    "k_model = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern set encontrado en ../patterns/ref_rot_crop1_elastic1/0\n",
      "Labels set encontrado en ../labels/ref_rot_crop1_elastic1/0\n"
     ]
    }
   ],
   "source": [
    "# Esta celda construye la variable data_flags, que lee DS_FLAGS de \n",
    "# la celda anterior y mapea su contenido a distintas rutas de \n",
    "# patrones, etiquetas y outputs\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "patterns_path = folder_path + 'patterns/' + data_flags + '/' + str(k_model)\n",
    "labels_path = folder_path + 'labels/' + data_flags + '/' + str(k_model)\n",
    "if not (os.path.isdir(patterns_path) and os.path.isdir(labels_path)):\n",
    "    raise FileNotFoundError(\"No existen directorios de datos para el conjunto de flags seleccionado. Verifique que el dataset exista y, de lo contrario, llame a Split and Augmentation\")\n",
    "print(\"Pattern set encontrado en {}\".format(patterns_path))\n",
    "print(\"Labels set encontrado en {}\".format(labels_path))\n",
    "OUTPUT_FILENAME = f'resnet50_K{k_model}.pth' if USE_RN50 else f'resnet18_K{k_model}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo resultante se guardará en ../models/resnet/ref_rot_crop1_elastic1/\n"
     ]
    }
   ],
   "source": [
    "model_output_dir = folder_path + 'models/resnet/{}'.format(data_flags)\n",
    "model_output_path = model_output_dir + OUTPUT_FILENAME\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "print(f\"El modelo resultante se guardará en {model_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['circular ornaments', 'lozenge', 'pictographics', 'rectangular ornaments', 'strokes and lines', 'triangular ornaments']\n"
     ]
    }
   ],
   "source": [
    "pathDataset = patterns_path + '/'\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(pathDataset + 'train', \n",
    "                                                    transform = transforms.Compose([\n",
    "                                                        transforms.RandomVerticalFlip(),\n",
    "                                                        transforms.RandomHorizontalFlip(),\n",
    "                                                        transforms.RandomResizedCrop(224),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder(pathDataset + 'val',\n",
    "                                                    transform = transforms.Compose([ transforms.Resize(256),\n",
    "                                                                    transforms.CenterCrop(224),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print(class_names)\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=30, output_path = 'model.pth', save_each = -1, patience=15):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    bad_epochs = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds ==  labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "        print('Train Loss: {:.4f}  Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(val_dataset)\n",
    "        epoch_acc = running_corrects / len(val_dataset)\n",
    "        print('Val Loss: {:.4f}  Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            bad_epochs = 0\n",
    "            best_epoch = epoch    \n",
    "\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs == patience:\n",
    "                print(f\"Se agotó la paciencia. Mejor época: {best_epoch}.\")\n",
    "                break\n",
    "                \n",
    "        if save_each > -1 and epoch%save_each == 0:\n",
    "            path = output_path.split(\"/\")\n",
    "            filename =  path[-1]\n",
    "            epoch_filename =filename.split(\".\")[0] + \"_e\" + str(epoch) + \".\" + filename.split(\".\")[1]\n",
    "            new_path = path[:-1]\n",
    "            new_path.append(epoch_filename)\n",
    "            new_path = '/'.join(new_path)\n",
    "            torch.save(model.state_dict(), new_path)\n",
    "            print(\"Saving model at epoch {} as {}\".format(epoch, new_path))\n",
    "\n",
    "            \n",
    "    print('Best accuracy: {:.4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "Train Loss: 1.6840  Acc: 0.3317\n",
      "Val Loss: 2.0425  Acc: 0.2564\n",
      "Epoch 1/299\n",
      "----------\n",
      "Train Loss: 1.4316  Acc: 0.4448\n",
      "Val Loss: 1.4869  Acc: 0.4744\n",
      "Epoch 2/299\n",
      "----------\n",
      "Train Loss: 1.2990  Acc: 0.5138\n",
      "Val Loss: 1.9419  Acc: 0.5256\n",
      "Epoch 3/299\n",
      "----------\n",
      "Train Loss: 1.2186  Acc: 0.5451\n",
      "Val Loss: 1.1046  Acc: 0.6410\n",
      "Epoch 4/299\n",
      "----------\n",
      "Train Loss: 1.1337  Acc: 0.5789\n",
      "Val Loss: 1.0303  Acc: 0.7051\n",
      "Epoch 5/299\n",
      "----------\n",
      "Train Loss: 1.1065  Acc: 0.5829\n",
      "Val Loss: 1.6207  Acc: 0.4872\n",
      "Epoch 6/299\n",
      "----------\n",
      "Train Loss: 1.0296  Acc: 0.6110\n",
      "Val Loss: 1.0530  Acc: 0.6795\n",
      "Epoch 7/299\n",
      "----------\n",
      "Train Loss: 0.9849  Acc: 0.6243\n",
      "Val Loss: 1.1282  Acc: 0.6154\n",
      "Epoch 8/299\n",
      "----------\n",
      "Train Loss: 0.9407  Acc: 0.6528\n",
      "Val Loss: 1.0943  Acc: 0.6795\n",
      "Epoch 9/299\n",
      "----------\n",
      "Train Loss: 0.9088  Acc: 0.6731\n",
      "Val Loss: 1.0666  Acc: 0.6923\n",
      "Epoch 10/299\n",
      "----------\n",
      "Train Loss: 0.8562  Acc: 0.6812\n",
      "Val Loss: 1.3402  Acc: 0.6410\n",
      "Epoch 11/299\n",
      "----------\n",
      "Train Loss: 0.8268  Acc: 0.7001\n",
      "Val Loss: 0.9585  Acc: 0.7308\n",
      "Epoch 12/299\n",
      "----------\n",
      "Train Loss: 0.8347  Acc: 0.6976\n",
      "Val Loss: 1.3892  Acc: 0.6026\n",
      "Epoch 13/299\n",
      "----------\n",
      "Train Loss: 0.7801  Acc: 0.7139\n",
      "Val Loss: 1.0817  Acc: 0.6538\n",
      "Epoch 14/299\n",
      "----------\n",
      "Train Loss: 0.7428  Acc: 0.7362\n",
      "Val Loss: 1.2489  Acc: 0.6282\n",
      "Epoch 15/299\n",
      "----------\n",
      "Train Loss: 0.6813  Acc: 0.7523\n",
      "Val Loss: 1.2928  Acc: 0.6538\n",
      "Epoch 16/299\n",
      "----------\n",
      "Train Loss: 0.6682  Acc: 0.7582\n",
      "Val Loss: 1.1209  Acc: 0.6538\n",
      "Epoch 17/299\n",
      "----------\n",
      "Train Loss: 0.6576  Acc: 0.7618\n",
      "Val Loss: 0.9551  Acc: 0.7179\n",
      "Epoch 18/299\n",
      "----------\n",
      "Train Loss: 0.6614  Acc: 0.7599\n",
      "Val Loss: 1.0821  Acc: 0.7179\n",
      "Epoch 19/299\n",
      "----------\n",
      "Train Loss: 0.6556  Acc: 0.7731\n",
      "Val Loss: 1.0388  Acc: 0.6026\n",
      "Epoch 20/299\n",
      "----------\n",
      "Train Loss: 0.5997  Acc: 0.7818\n",
      "Val Loss: 1.1910  Acc: 0.6795\n",
      "Epoch 21/299\n",
      "----------\n",
      "Train Loss: 0.5995  Acc: 0.7917\n",
      "Val Loss: 1.0414  Acc: 0.7051\n",
      "Epoch 22/299\n",
      "----------\n",
      "Train Loss: 0.6066  Acc: 0.7782\n",
      "Val Loss: 0.9842  Acc: 0.6667\n",
      "Epoch 23/299\n",
      "----------\n",
      "Train Loss: 0.5563  Acc: 0.8075\n",
      "Val Loss: 1.3455  Acc: 0.5641\n",
      "Epoch 24/299\n",
      "----------\n",
      "Train Loss: 0.5632  Acc: 0.7993\n",
      "Val Loss: 1.0201  Acc: 0.6795\n",
      "Epoch 25/299\n",
      "----------\n",
      "Train Loss: 0.5120  Acc: 0.8196\n",
      "Val Loss: 0.9769  Acc: 0.7692\n",
      "Epoch 26/299\n",
      "----------\n",
      "Train Loss: 0.5018  Acc: 0.8219\n",
      "Val Loss: 1.0247  Acc: 0.6795\n",
      "Epoch 27/299\n",
      "----------\n",
      "Train Loss: 0.5076  Acc: 0.8126\n",
      "Val Loss: 0.8579  Acc: 0.7692\n",
      "Epoch 28/299\n",
      "----------\n",
      "Train Loss: 0.4960  Acc: 0.8238\n",
      "Val Loss: 1.1439  Acc: 0.7179\n",
      "Epoch 29/299\n",
      "----------\n",
      "Train Loss: 0.4845  Acc: 0.8348\n",
      "Val Loss: 1.0048  Acc: 0.7564\n",
      "Epoch 30/299\n",
      "----------\n",
      "Train Loss: 0.4734  Acc: 0.8343\n",
      "Val Loss: 1.1074  Acc: 0.7051\n",
      "Epoch 31/299\n",
      "----------\n",
      "Train Loss: 0.4564  Acc: 0.8430\n",
      "Val Loss: 0.8732  Acc: 0.7436\n",
      "Epoch 32/299\n",
      "----------\n",
      "Train Loss: 0.4583  Acc: 0.8399\n",
      "Val Loss: 0.9588  Acc: 0.7436\n",
      "Epoch 33/299\n",
      "----------\n",
      "Train Loss: 0.4400  Acc: 0.8436\n",
      "Val Loss: 1.0817  Acc: 0.7308\n",
      "Epoch 34/299\n",
      "----------\n",
      "Train Loss: 0.4659  Acc: 0.8357\n",
      "Val Loss: 1.0322  Acc: 0.7436\n",
      "Epoch 35/299\n",
      "----------\n",
      "Train Loss: 0.4242  Acc: 0.8554\n",
      "Val Loss: 1.0756  Acc: 0.6538\n",
      "Epoch 36/299\n",
      "----------\n",
      "Train Loss: 0.4571  Acc: 0.8433\n",
      "Val Loss: 0.9485  Acc: 0.7436\n",
      "Epoch 37/299\n",
      "----------\n",
      "Train Loss: 0.4290  Acc: 0.8532\n",
      "Val Loss: 1.2390  Acc: 0.7308\n",
      "Epoch 38/299\n",
      "----------\n",
      "Train Loss: 0.4209  Acc: 0.8481\n",
      "Val Loss: 1.3487  Acc: 0.6410\n",
      "Epoch 39/299\n",
      "----------\n",
      "Train Loss: 0.3969  Acc: 0.8658\n",
      "Val Loss: 1.3919  Acc: 0.6923\n",
      "Epoch 40/299\n",
      "----------\n",
      "Train Loss: 0.4041  Acc: 0.8568\n",
      "Val Loss: 1.1105  Acc: 0.7051\n",
      "Se agotó la paciencia. Mejor época: 25.\n",
      "Best accuracy: 0.7692\n"
     ]
    }
   ],
   "source": [
    "if USE_RN50:\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "else:\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "num_ft = model_ft.fc.in_features\n",
    "\n",
    "output_dim = 6\n",
    "model_ft.fc = nn.Linear(num_ft, output_dim)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "groups = [{'params': model_ft.conv1.parameters(),'lr':learning_rate/4},\n",
    "            {'params': model_ft.bn1.parameters(),'lr':learning_rate/4},\n",
    "            {'params': model_ft.layer1.parameters(),'lr':learning_rate/4},\n",
    "            {'params': model_ft.layer2.parameters(),'lr':learning_rate/2},\n",
    "            {'params': model_ft.layer3.parameters(), 'lr':learning_rate/2},\n",
    "            {'params': model_ft.layer4.parameters(),'lr':learning_rate},\n",
    "            {'params': model_ft.fc.parameters(), 'lr':learning_rate}]\n",
    "\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr = 0.0015)\n",
    "\n",
    "output_path = model_output_path\n",
    "\n",
    "# change save_each and output_path to get partial outputs\n",
    "model_ft = train_model(model_ft, criterion, optimizer, num_epochs=TRAINING_EPOCHS,\n",
    "                       save_each=SAVE_EACH, output_path=output_path)\n",
    "\n",
    "# save best model\n",
    "torch.save(model_ft.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8068  Acc: 0.7577\n"
     ]
    }
   ],
   "source": [
    "model = model_output_path\n",
    "\n",
    "pathDataset = patterns_path + '/'\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(pathDataset + 'test',\n",
    "                                                    transform = transforms.Compose([ transforms.Resize(224),\n",
    "                                                                    #transforms.CenterCrop(224),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if USE_RN50:\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "else:\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "output_dim = 6\n",
    "model_ft.fc = nn.Linear(num_ft, output_dim)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "model_ft.load_state_dict(torch.load(model))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_ft.eval()\n",
    "running_loss = 0.0\n",
    "running_corrects = 0.0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model_ft(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_loss = running_loss / len(test_dataset)\n",
    "epoch_acc = running_corrects / len(test_dataset)\n",
    "print('Test Loss: {:.4f}  Acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet retraining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}