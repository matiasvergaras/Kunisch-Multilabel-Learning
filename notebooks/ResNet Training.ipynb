{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpIfCtDGJ5q-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# Resnet Retraining\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "El objetivo de este notebook es entrenar arquitecturas de ResNet preentrenadas sobre la tarea de predecir el capítulo de cada patrón."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2Z2do8XKiAQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IgrQgam9KgnN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np, scipy.io\n",
    "import argparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuración de dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando device: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "CUDA_ID = 0\n",
    "\n",
    "device = torch.device(f'cuda:{CUDA_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuración de datos y modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Flags para los datos sintéticos\n",
    "# Cada flag está asociada a una o más funciones de data augmentation.\n",
    "# Los datos deben existir previamente \n",
    "# (se generan a partir del notebook split and augmentation)\n",
    "USE_RN50 = False\n",
    "DS_FLAGS = []\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "              # DEJAR VACIO PARA TRABAJAR CONJUNTO BASE\n",
    "                \n",
    "# Las flags crop, randaug, elastic y gausblur \n",
    "# se pueden aplicar más de una vez c/u. \n",
    "# (si no están en DS_FLAGS, serán ignoradas).\n",
    "CROP_TIMES = 1\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1\n",
    "GAUSBLUR_TIMES = 1\n",
    "\n",
    "# Guardar el modelo cada SAVE_EACH épocas\n",
    "SAVE_EACH = -1 # -1 to save only the best model\n",
    "TRAINING_EPOCHS = 300\n",
    "K = 4\n",
    "k_model = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern set encontrado en ../patterns/base/0\n",
      "Labels set encontrado en ../labels/base/0\n"
     ]
    }
   ],
   "source": [
    "# Esta celda construye la variable data_flags, que lee DS_FLAGS de \n",
    "# la celda anterior y mapea su contenido a distintas rutas de \n",
    "# patrones, etiquetas y outputs\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "         'gausblur': GAUSBLUR_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic', 'gausblur']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "patterns_path = os.path.join(root_dir,'patterns', data_flags, str(k_model))\n",
    "labels_path = os.path.join(root_dir, 'labels', data_flags, str(k_model))\n",
    "\n",
    "if not (os.path.isdir(patterns_path) and os.path.isdir(labels_path)):\n",
    "    raise FileNotFoundError(\"No existen directorios de datos para el conjunto de flags seleccionado. Verifique que el dataset exista y, de lo contrario, llame a Split and Augmentation\")\n",
    "print(\"Pattern set encontrado en {}\".format(patterns_path))\n",
    "print(\"Labels set encontrado en {}\".format(labels_path))\n",
    "OUTPUT_FILENAME = f'resnet50_K{k_model}.pth' if USE_RN50 else f'resnet18_K{k_model}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo resultante se guardará en ../models/resnet/base/resnet18_K0.pth\n"
     ]
    }
   ],
   "source": [
    "model_output_dir = os.path.join(root_dir, 'models', 'resnet', data_flags)\n",
    "model_output_path = os.path.join(model_output_dir, OUTPUT_FILENAME)\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "print(f\"El modelo resultante se guardará en {model_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['circular ornaments', 'lozenge', 'pictographics', 'rectangular ornaments', 'strokes and lines', 'triangular ornaments']\n"
     ]
    }
   ],
   "source": [
    "pathDataset = patterns_path + '/'\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(pathDataset + 'train', \n",
    "                                                    transform = transforms.Compose([\n",
    "                                                        transforms.RandomVerticalFlip(),\n",
    "                                                        transforms.RandomHorizontalFlip(),\n",
    "                                                        transforms.RandomResizedCrop(224),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder(pathDataset + 'val',\n",
    "                                                    transform = transforms.Compose([ transforms.Resize(256),\n",
    "                                                                    transforms.CenterCrop(224),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print(class_names)\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=30, output_path = 'model.pth', save_each = -1, patience=15):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    bad_epochs = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds ==  labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "        print('Train Loss: {:.4f}  Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(val_dataset)\n",
    "        epoch_acc = running_corrects / len(val_dataset)\n",
    "        print('Val Loss: {:.4f}  Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            bad_epochs = 0\n",
    "            best_epoch = epoch    \n",
    "\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs == patience:\n",
    "                print(f\"Se agotó la paciencia. Mejor época: {best_epoch}.\")\n",
    "                break\n",
    "                \n",
    "        if save_each > -1 and epoch%save_each == 0:\n",
    "            path = output_path.split(\"/\")\n",
    "            filename =  path[-1]\n",
    "            epoch_filename =filename.split(\".\")[0] + \"_e\" + str(epoch) + \".\" + filename.split(\".\")[1]\n",
    "            new_path = path[:-1]\n",
    "            new_path.append(epoch_filename)\n",
    "            new_path = '/'.join(new_path)\n",
    "            torch.save(model.state_dict(), new_path)\n",
    "            print(\"Saving model at epoch {} as {}\".format(epoch, new_path))\n",
    "\n",
    "            \n",
    "    print('Best accuracy: {:.4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mavergar/anaconda3/envs/Kunisch_10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mavergar/anaconda3/envs/Kunisch_10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "Train Loss: 1.3708  Acc: 0.5049\n",
      "Val Loss: 1.4191  Acc: 0.5641\n",
      "Epoch 1/299\n",
      "----------\n",
      "Train Loss: 1.0735  Acc: 0.6156\n",
      "Val Loss: 1.2433  Acc: 0.6538\n",
      "Epoch 2/299\n",
      "----------\n",
      "Train Loss: 0.9378  Acc: 0.6590\n",
      "Val Loss: 0.8117  Acc: 0.7436\n",
      "Epoch 3/299\n",
      "----------\n",
      "Train Loss: 0.8345  Acc: 0.6991\n",
      "Val Loss: 1.0397  Acc: 0.7179\n",
      "Epoch 4/299\n",
      "----------\n",
      "Train Loss: 0.7389  Acc: 0.7327\n",
      "Val Loss: 0.7007  Acc: 0.7308\n",
      "Epoch 5/299\n",
      "----------\n",
      "Train Loss: 0.6951  Acc: 0.7491\n",
      "Val Loss: 0.5807  Acc: 0.7821\n",
      "Epoch 6/299\n",
      "----------\n",
      "Train Loss: 0.6400  Acc: 0.7742\n",
      "Val Loss: 0.6212  Acc: 0.8333\n",
      "Epoch 7/299\n",
      "----------\n",
      "Train Loss: 0.6007  Acc: 0.7793\n",
      "Val Loss: 0.7035  Acc: 0.8333\n",
      "Epoch 8/299\n",
      "----------\n",
      "Train Loss: 0.5598  Acc: 0.7924\n",
      "Val Loss: 0.7831  Acc: 0.7821\n",
      "Epoch 9/299\n",
      "----------\n",
      "Train Loss: 0.5446  Acc: 0.8051\n",
      "Val Loss: 0.6205  Acc: 0.8333\n",
      "Epoch 10/299\n",
      "----------\n",
      "Train Loss: 0.5426  Acc: 0.8062\n",
      "Val Loss: 0.6189  Acc: 0.7821\n",
      "Epoch 11/299\n",
      "----------\n",
      "Train Loss: 0.5047  Acc: 0.8224\n",
      "Val Loss: 0.6038  Acc: 0.8462\n",
      "Epoch 12/299\n",
      "----------\n",
      "Train Loss: 0.4843  Acc: 0.8264\n",
      "Val Loss: 0.6955  Acc: 0.7949\n",
      "Epoch 13/299\n",
      "----------\n",
      "Train Loss: 0.4938  Acc: 0.8302\n",
      "Val Loss: 0.5671  Acc: 0.7821\n",
      "Epoch 14/299\n",
      "----------\n",
      "Train Loss: 0.4525  Acc: 0.8380\n",
      "Val Loss: 0.5444  Acc: 0.8462\n",
      "Epoch 15/299\n",
      "----------\n",
      "Train Loss: 0.4519  Acc: 0.8421\n",
      "Val Loss: 0.4688  Acc: 0.8205\n",
      "Epoch 16/299\n",
      "----------\n",
      "Train Loss: 0.4293  Acc: 0.8488\n",
      "Val Loss: 0.6368  Acc: 0.8077\n",
      "Epoch 17/299\n",
      "----------\n",
      "Train Loss: 0.4387  Acc: 0.8466\n",
      "Val Loss: 0.5047  Acc: 0.8590\n",
      "Epoch 18/299\n",
      "----------\n",
      "Train Loss: 0.4438  Acc: 0.8454\n",
      "Val Loss: 0.7479  Acc: 0.8333\n",
      "Epoch 19/299\n",
      "----------\n",
      "Train Loss: 0.4204  Acc: 0.8486\n",
      "Val Loss: 0.5886  Acc: 0.8077\n",
      "Epoch 20/299\n",
      "----------\n",
      "Train Loss: 0.3845  Acc: 0.8596\n",
      "Val Loss: 0.5612  Acc: 0.8205\n",
      "Epoch 21/299\n",
      "----------\n",
      "Train Loss: 0.3847  Acc: 0.8604\n",
      "Val Loss: 0.4894  Acc: 0.8333\n",
      "Epoch 22/299\n",
      "----------\n",
      "Train Loss: 0.3724  Acc: 0.8705\n",
      "Val Loss: 0.5683  Acc: 0.8462\n",
      "Epoch 23/299\n",
      "----------\n",
      "Train Loss: 0.3665  Acc: 0.8698\n",
      "Val Loss: 0.7289  Acc: 0.8205\n",
      "Epoch 24/299\n",
      "----------\n",
      "Train Loss: 0.3550  Acc: 0.8728\n",
      "Val Loss: 0.7324  Acc: 0.7949\n",
      "Epoch 25/299\n",
      "----------\n",
      "Train Loss: 0.3589  Acc: 0.8723\n",
      "Val Loss: 0.8414  Acc: 0.7821\n",
      "Epoch 26/299\n",
      "----------\n",
      "Train Loss: 0.3641  Acc: 0.8667\n",
      "Val Loss: 0.6424  Acc: 0.8205\n",
      "Epoch 27/299\n",
      "----------\n",
      "Train Loss: 0.3580  Acc: 0.8698\n",
      "Val Loss: 0.8084  Acc: 0.8077\n",
      "Epoch 28/299\n",
      "----------\n",
      "Train Loss: 0.3557  Acc: 0.8727\n",
      "Val Loss: 0.7235  Acc: 0.8333\n",
      "Epoch 29/299\n",
      "----------\n",
      "Train Loss: 0.3482  Acc: 0.8779\n",
      "Val Loss: 0.4682  Acc: 0.8718\n",
      "Epoch 30/299\n",
      "----------\n",
      "Train Loss: 0.3213  Acc: 0.8866\n",
      "Val Loss: 0.6729  Acc: 0.8077\n",
      "Epoch 31/299\n",
      "----------\n",
      "Train Loss: 0.3365  Acc: 0.8777\n",
      "Val Loss: 0.6707  Acc: 0.8205\n",
      "Epoch 32/299\n",
      "----------\n",
      "Train Loss: 0.3333  Acc: 0.8783\n",
      "Val Loss: 0.7521  Acc: 0.8205\n",
      "Epoch 33/299\n",
      "----------\n",
      "Train Loss: 0.3135  Acc: 0.8882\n",
      "Val Loss: 0.6076  Acc: 0.8333\n",
      "Epoch 34/299\n",
      "----------\n",
      "Train Loss: 0.3434  Acc: 0.8849\n",
      "Val Loss: 0.6653  Acc: 0.8333\n",
      "Epoch 35/299\n",
      "----------\n",
      "Train Loss: 0.3023  Acc: 0.8952\n",
      "Val Loss: 0.5642  Acc: 0.8590\n",
      "Epoch 36/299\n",
      "----------\n",
      "Train Loss: 0.3134  Acc: 0.8913\n",
      "Val Loss: 0.6448  Acc: 0.8718\n",
      "Epoch 37/299\n",
      "----------\n",
      "Train Loss: 0.2910  Acc: 0.8960\n",
      "Val Loss: 0.7011  Acc: 0.8077\n",
      "Epoch 38/299\n",
      "----------\n",
      "Train Loss: 0.2967  Acc: 0.8932\n",
      "Val Loss: 0.5446  Acc: 0.8333\n",
      "Epoch 39/299\n",
      "----------\n",
      "Train Loss: 0.3136  Acc: 0.8869\n",
      "Val Loss: 0.7078  Acc: 0.7949\n",
      "Epoch 40/299\n",
      "----------\n",
      "Train Loss: 0.3224  Acc: 0.8867\n",
      "Val Loss: 0.6385  Acc: 0.8205\n",
      "Epoch 41/299\n",
      "----------\n",
      "Train Loss: 0.2850  Acc: 0.8987\n",
      "Val Loss: 0.8863  Acc: 0.7821\n",
      "Epoch 42/299\n",
      "----------\n",
      "Train Loss: 0.2809  Acc: 0.9017\n",
      "Val Loss: 0.6837  Acc: 0.7821\n",
      "Epoch 43/299\n",
      "----------\n",
      "Train Loss: 0.2898  Acc: 0.8972\n",
      "Val Loss: 0.4947  Acc: 0.8590\n",
      "Epoch 44/299\n",
      "----------\n",
      "Train Loss: 0.2534  Acc: 0.9113\n",
      "Val Loss: 0.5707  Acc: 0.8590\n",
      "Se agotó la paciencia. Mejor época: 29.\n",
      "Best accuracy: 0.8718\n"
     ]
    }
   ],
   "source": [
    "if USE_RN50:\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "else:\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "num_ft = model_ft.fc.in_features\n",
    "\n",
    "output_dim = 6\n",
    "model_ft.fc = nn.Linear(num_ft, output_dim)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "groups = [{'params': model_ft.conv1.parameters(),'lr':learning_rate/4},\n",
    "            {'params': model_ft.bn1.parameters(),'lr':learning_rate/4},\n",
    "            {'params': model_ft.layer1.parameters(),'lr':learning_rate/4},\n",
    "            {'params': model_ft.layer2.parameters(),'lr':learning_rate/2},\n",
    "            {'params': model_ft.layer3.parameters(), 'lr':learning_rate/2},\n",
    "            {'params': model_ft.layer4.parameters(),'lr':learning_rate},\n",
    "            {'params': model_ft.fc.parameters(), 'lr':learning_rate}]\n",
    "\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr = 0.0015)\n",
    "\n",
    "output_path = model_output_path\n",
    "\n",
    "# change save_each and output_path to get partial outputs\n",
    "model_ft = train_model(model_ft, criterion, optimizer, num_epochs=TRAINING_EPOCHS,\n",
    "                       save_each=SAVE_EACH, output_path=output_path)\n",
    "\n",
    "# save best model\n",
    "torch.save(model_ft.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4085  Acc: 0.8660\n"
     ]
    }
   ],
   "source": [
    "model = model_output_path\n",
    "\n",
    "pathDataset = patterns_path + '/'\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(pathDataset + 'test',\n",
    "                                                    transform = transforms.Compose([ transforms.Resize(224),\n",
    "                                                                    #transforms.CenterCrop(224),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if USE_RN50:\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "else:\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "output_dim = 6\n",
    "model_ft.fc = nn.Linear(num_ft, output_dim)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "model_ft.load_state_dict(torch.load(model))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_ft.eval()\n",
    "running_loss = 0.0\n",
    "running_corrects = 0.0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model_ft(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_loss = running_loss / len(test_dataset)\n",
    "epoch_acc = running_corrects / len(test_dataset)\n",
    "print('Test Loss: {:.4f}  Acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet retraining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
